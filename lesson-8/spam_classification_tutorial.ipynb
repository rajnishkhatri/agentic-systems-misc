{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification Tutorial: Model Cascade Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- ✅ Implement a 2-tier model cascade for SMS spam classification\n",
    "- ✅ Extract confidence scores from log probabilities (logprobs)\n",
    "- ✅ Optimize confidence thresholds to meet target accuracy\n",
    "- ✅ Simulate cascade routing and measure cost savings\n",
    "- ✅ Analyze cascade performance vs. baseline strategies\n",
    "- ✅ Understand when cascades provide value\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of [Model Cascade Concepts](model_cascade_concepts.md)\n",
    "- Python basics (pandas, numpy)\n",
    "- Familiarity with binary classification\n",
    "\n",
    "## Estimated Time\n",
    "\n",
    "**Execution Time:** <2 minutes (uses pre-computed predictions)\n",
    "**Learning Time:** 25-30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Target accuracy for cascade\n",
    "TARGET_ACCURACY = 0.99\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pre-computed Predictions\n",
    "\n",
    "We use pre-computed predictions from `gpt-4o-mini` (cheap proxy) and `gpt-4o` (expensive oracle) to avoid API costs during tutorial.\n",
    "\n",
    "**Dataset:**\n",
    "- SMS spam classification (binary: legitimate vs. financially risky)\n",
    "- Train set: 100 samples (for threshold optimization)\n",
    "- Test set: 400 samples (for cascade evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('sms_spam_predictions_train.csv')\n",
    "test_df = pd.read_csv('sms_spam_predictions_test.csv')\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "\n",
    "# Preview data\n",
    "print(\"\\nSample predictions:\")\n",
    "train_df[['text', 'proxy_prediction', 'proxy_confidence', 'oracle_prediction']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column Descriptions:**\n",
    "- `text`: SMS message content\n",
    "- `proxy_prediction`: Cheap model (gpt-4o-mini) prediction (1=legitimate, 0=risky)\n",
    "- `proxy_confidence`: Confidence score from logprobs (0-1)\n",
    "- `proxy_cost`: API cost for cheap model ($)\n",
    "- `oracle_prediction`: Expensive model (gpt-4o) prediction (ground truth)\n",
    "- `oracle_cost`: API cost for expensive model ($)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Performance\n",
    "\n",
    "Before implementing cascade, measure baseline accuracy and costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proxy accuracy (using oracle as ground truth)\n",
    "train_proxy_accuracy = (train_df['proxy_prediction'] == train_df['oracle_prediction']).mean()\n",
    "test_proxy_accuracy = (test_df['proxy_prediction'] == test_df['oracle_prediction']).mean()\n",
    "\n",
    "print(\"=== BASELINE PERFORMANCE ===\")\n",
    "print(f\"Train proxy accuracy: {train_proxy_accuracy:.4f}\")\n",
    "print(f\"Test proxy accuracy: {test_proxy_accuracy:.4f}\")\n",
    "print(f\"\")\n",
    "\n",
    "# Calculate baseline costs\n",
    "total_proxy_cost = test_df['proxy_cost'].sum()\n",
    "total_oracle_cost = test_df['oracle_cost'].sum()\n",
    "\n",
    "print(\"=== BASELINE COSTS (Test Set) ===\")\n",
    "print(f\"Proxy-only (gpt-4o-mini): ${total_proxy_cost:.4f}\")\n",
    "print(f\"Oracle-only (gpt-4o): ${total_oracle_cost:.4f}\")\n",
    "print(f\"Cost multiplier (oracle/proxy): {total_oracle_cost / total_proxy_cost:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- Proxy model is ~10-20x cheaper but slightly less accurate\n",
    "- Oracle model is expensive but highly accurate (our ground truth)\n",
    "- Goal: Use proxy for easy cases, oracle for hard cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Confidence Distribution\n",
    "\n",
    "Understanding proxy confidence helps identify easy vs. hard cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confidence histogram\n",
    "axes[0].hist(train_df['proxy_confidence'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Proxy Confidence')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Confidence Distribution (Train Set)')\n",
    "axes[0].axvline(0.9, color='red', linestyle='--', label='Potential threshold (0.9)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy by confidence bin\n",
    "train_df['confidence_bin'] = pd.cut(train_df['proxy_confidence'], bins=10)\n",
    "accuracy_by_bin = train_df.groupby('confidence_bin').apply(\n",
    "    lambda x: (x['proxy_prediction'] == x['oracle_prediction']).mean()\n",
    ")\n",
    "\n",
    "bin_centers = [interval.mid for interval in accuracy_by_bin.index]\n",
    "axes[1].plot(bin_centers, accuracy_by_bin.values, marker='o', linewidth=2)\n",
    "axes[1].axhline(TARGET_ACCURACY, color='red', linestyle='--', label=f'Target accuracy ({TARGET_ACCURACY})')\n",
    "axes[1].set_xlabel('Confidence')\n",
    "axes[1].set_ylabel('Proxy Accuracy')\n",
    "axes[1].set_title('Proxy Accuracy by Confidence Level')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"High confidence (≥0.9) samples: {(train_df['proxy_confidence'] >= 0.9).sum()} / {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:**\n",
    "- Most predictions have high confidence (>0.9)\n",
    "- Proxy accuracy improves with higher confidence\n",
    "- We can safely route high-confidence cases to proxy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Optimal Thresholds\n",
    "\n",
    "We optimize confidence thresholds to meet target accuracy (99%) while maximizing proxy usage.\n",
    "\n",
    "**Strategy:**\n",
    "1. For each predicted class (0 or 1), find the minimum confidence threshold\n",
    "2. Threshold must ensure proxy accuracy ≥ 99% for that class\n",
    "3. Lower threshold = more queries routed to proxy = more cost savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_thresholds(df: pd.DataFrame, target_accuracy: float = 0.99) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Find optimal confidence thresholds for each prediction class.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with proxy and oracle predictions\n",
    "        target_accuracy: Minimum accuracy required (default: 0.99)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping class (0 or 1) to threshold\n",
    "    \"\"\"\n",
    "    thresholds = {}\n",
    "    \n",
    "    for predicted_class in df['proxy_prediction'].unique():\n",
    "        # Get all predictions for this class\n",
    "        class_df = df[df['proxy_prediction'] == predicted_class]\n",
    "        \n",
    "        # Get unique confidence values (potential thresholds)\n",
    "        possible_thresholds = sorted(class_df['proxy_confidence'].unique())\n",
    "        \n",
    "        # Find lowest threshold that meets target accuracy\n",
    "        found = False\n",
    "        for threshold in possible_thresholds:\n",
    "            # Filter to predictions above threshold\n",
    "            above_threshold = class_df[class_df['proxy_confidence'] >= threshold]\n",
    "            \n",
    "            if len(above_threshold) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate accuracy at this threshold\n",
    "            accuracy = (above_threshold['proxy_prediction'] == above_threshold['oracle_prediction']).mean()\n",
    "            \n",
    "            if accuracy >= target_accuracy:\n",
    "                thresholds[predicted_class] = threshold\n",
    "                found = True\n",
    "                print(f\"Class {predicted_class}: threshold={threshold:.4f}, \"\n",
    "                      f\"accuracy={accuracy:.4f}, samples={len(above_threshold)}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"⚠️ Class {predicted_class}: No threshold achieves {target_accuracy:.2%} accuracy\")\n",
    "            thresholds[predicted_class] = float('inf')  # Route all to oracle\n",
    "    \n",
    "    return thresholds\n",
    "\n",
    "# Find thresholds on train set\n",
    "print(f\"=== THRESHOLD OPTIMIZATION (Target: {TARGET_ACCURACY:.2%}) ===\")\n",
    "thresholds = find_thresholds(train_df, TARGET_ACCURACY)\n",
    "print(f\"\\nOptimal thresholds: {thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Separate thresholds for each class (0 and 1) allow flexibility\n",
    "- Lower threshold = more aggressive proxy usage\n",
    "- If no threshold meets target, route all queries to oracle (threshold = ∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulate Cascade on Test Set\n",
    "\n",
    "Apply optimized thresholds to test data and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cascade(df: pd.DataFrame, thresholds: Dict[int, float]) -> Dict:\n",
    "    \"\"\"\n",
    "    Simulate cascade routing and calculate metrics.\n",
    "    \n",
    "    Args:\n",
    "        df: Test DataFrame with predictions\n",
    "        thresholds: Dictionary mapping class to confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with cascade metrics\n",
    "    \"\"\"\n",
    "    cascade_predictions = []\n",
    "    cascade_costs = []\n",
    "    uses_oracle = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        proxy_pred = row['proxy_prediction']\n",
    "        proxy_conf = row['proxy_confidence']\n",
    "        proxy_cost = row['proxy_cost']\n",
    "        oracle_pred = row['oracle_prediction']\n",
    "        oracle_cost = row['oracle_cost']\n",
    "        \n",
    "        # Get threshold for this prediction class\n",
    "        threshold = thresholds.get(proxy_pred, float('inf'))\n",
    "        \n",
    "        # Cascade logic\n",
    "        if proxy_conf >= threshold:\n",
    "            # Confident: use proxy prediction\n",
    "            cascade_predictions.append(proxy_pred)\n",
    "            cascade_costs.append(proxy_cost)  # Only pay proxy\n",
    "            uses_oracle.append(False)\n",
    "        else:\n",
    "            # Uncertain: escalate to oracle\n",
    "            cascade_predictions.append(oracle_pred)\n",
    "            cascade_costs.append(proxy_cost + oracle_cost)  # Pay both\n",
    "            uses_oracle.append(True)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_cascade_cost = sum(cascade_costs)\n",
    "    total_proxy_only_cost = df['proxy_cost'].sum()\n",
    "    total_oracle_only_cost = df['oracle_cost'].sum()\n",
    "    \n",
    "    oracle_usage_rate = sum(uses_oracle) / len(uses_oracle)\n",
    "    cascade_accuracy = sum(cp == op for cp, op in zip(cascade_predictions, df['oracle_prediction'])) / len(cascade_predictions)\n",
    "    \n",
    "    return {\n",
    "        'cascade_predictions': cascade_predictions,\n",
    "        'cascade_costs': cascade_costs,\n",
    "        'uses_oracle': uses_oracle,\n",
    "        'total_cascade_cost': total_cascade_cost,\n",
    "        'total_proxy_cost': total_proxy_only_cost,\n",
    "        'total_oracle_cost': total_oracle_only_cost,\n",
    "        'oracle_usage_rate': oracle_usage_rate,\n",
    "        'proxy_usage_rate': 1 - oracle_usage_rate,\n",
    "        'cascade_accuracy': cascade_accuracy,\n",
    "        'num_samples': len(df)\n",
    "    }\n",
    "\n",
    "# Run cascade simulation\n",
    "results = simulate_cascade(test_df, thresholds)\n",
    "\n",
    "print(\"=== CASCADE SIMULATION RESULTS ===\")\n",
    "print(f\"Total samples: {results['num_samples']}\")\n",
    "print(f\"Proxy usage rate: {results['proxy_usage_rate']:.2%}\")\n",
    "print(f\"Oracle usage rate: {results['oracle_usage_rate']:.2%}\")\n",
    "print(f\"Cascade accuracy: {results['cascade_accuracy']:.4f}\")\n",
    "print(f\"Target accuracy: {TARGET_ACCURACY}\")\n",
    "print(f\"✓ Met target: {results['cascade_accuracy'] >= TARGET_ACCURACY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost Analysis\n",
    "\n",
    "Compare cascade costs to baseline strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate savings\n",
    "savings_vs_oracle = results['total_oracle_cost'] - results['total_cascade_cost']\n",
    "savings_pct = (savings_vs_oracle / results['total_oracle_cost']) * 100\n",
    "\n",
    "print(\"=== COST ANALYSIS ===\")\n",
    "print(f\"Cascade cost:     ${results['total_cascade_cost']:.4f}\")\n",
    "print(f\"Proxy-only cost:  ${results['total_proxy_cost']:.4f}\")\n",
    "print(f\"Oracle-only cost: ${results['total_oracle_cost']:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Savings vs. oracle-only: ${savings_vs_oracle:.4f} ({savings_pct:.1f}%)\")\n",
    "print(f\"Cost ratio (cascade/oracle): {results['total_cascade_cost'] / results['total_oracle_cost']:.2f}x\")\n",
    "\n",
    "# Visualize costs\n",
    "strategies = ['Proxy-only\\n(cheap, less accurate)', 'Cascade\\n(optimized)', 'Oracle-only\\n(expensive, accurate)']\n",
    "costs = [results['total_proxy_cost'], results['total_cascade_cost'], results['total_oracle_cost']]\n",
    "colors = ['#4CAF50', '#2196F3', '#F44336']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(strategies, costs, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Total Cost ($)')\n",
    "plt.title('Cost Comparison: Model Cascade vs. Baselines')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, cost in zip(bars, costs):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'${cost:.4f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Results:**\n",
    "- Cascade achieves oracle-level accuracy at a fraction of the cost\n",
    "- Typical savings: 60-80% compared to oracle-only\n",
    "- Trade-off: Small increase vs. proxy-only, but much higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Sensitivity Analysis\n",
    "\n",
    "How do different thresholds affect performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep thresholds\n",
    "threshold_sweep = [0.5, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "sweep_results = []\n",
    "\n",
    "for t in threshold_sweep:\n",
    "    # Use same threshold for both classes for simplicity\n",
    "    test_thresholds = {0: t, 1: t}\n",
    "    result = simulate_cascade(test_df, test_thresholds)\n",
    "    sweep_results.append({\n",
    "        'threshold': t,\n",
    "        'accuracy': result['cascade_accuracy'],\n",
    "        'oracle_usage': result['oracle_usage_rate'],\n",
    "        'cost': result['total_cascade_cost']\n",
    "    })\n",
    "\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "\n",
    "# Plot trade-offs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs. threshold\n",
    "axes[0].plot(sweep_df['threshold'], sweep_df['accuracy'], marker='o', linewidth=2, label='Cascade accuracy')\n",
    "axes[0].axhline(TARGET_ACCURACY, color='red', linestyle='--', label=f'Target ({TARGET_ACCURACY})')\n",
    "axes[0].set_xlabel('Confidence Threshold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs. Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cost vs. oracle usage\n",
    "axes[1].scatter(sweep_df['oracle_usage'], sweep_df['cost'], s=100, alpha=0.6)\n",
    "for _, row in sweep_df.iterrows():\n",
    "    axes[1].annotate(f\"{row['threshold']:.2f}\", (row['oracle_usage'], row['cost']), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "axes[1].set_xlabel('Oracle Usage Rate')\n",
    "axes[1].set_ylabel('Total Cost ($)')\n",
    "axes[1].set_title('Cost vs. Oracle Usage')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Threshold sweep results:\")\n",
    "print(sweep_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-off Insights:**\n",
    "- Lower threshold → More proxy usage → Lower cost, but lower accuracy\n",
    "- Higher threshold → More oracle usage → Higher cost, higher accuracy\n",
    "- Optimal threshold balances cost and accuracy requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Cascade Decisions\n",
    "\n",
    "Examine specific cases where cascade routed to proxy vs. oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cascade decisions to test dataframe\n",
    "test_df['uses_oracle'] = results['uses_oracle']\n",
    "test_df['cascade_prediction'] = results['cascade_predictions']\n",
    "\n",
    "# Show proxy-routed examples (high confidence)\n",
    "print(\"=== PROXY-ROUTED EXAMPLES (High Confidence) ===\")\n",
    "proxy_examples = test_df[~test_df['uses_oracle']].head(3)\n",
    "for idx, row in proxy_examples.iterrows():\n",
    "    print(f\"\\nText: {row['text'][:80]}...\")\n",
    "    print(f\"Proxy prediction: {row['proxy_prediction']}, Confidence: {row['proxy_confidence']:.4f}\")\n",
    "    print(f\"Oracle prediction: {row['oracle_prediction']} (ground truth)\")\n",
    "    print(f\"✓ Correct: {row['proxy_prediction'] == row['oracle_prediction']}\")\n",
    "\n",
    "# Show oracle-routed examples (low confidence)\n",
    "print(\"\\n=== ORACLE-ROUTED EXAMPLES (Low Confidence) ===\")\n",
    "oracle_examples = test_df[test_df['uses_oracle']].head(3)\n",
    "for idx, row in oracle_examples.iterrows():\n",
    "    print(f\"\\nText: {row['text'][:80]}...\")\n",
    "    print(f\"Proxy prediction: {row['proxy_prediction']}, Confidence: {row['proxy_confidence']:.4f}\")\n",
    "    print(f\"Oracle prediction: {row['oracle_prediction']} (used)\")\n",
    "    print(f\"⚠️ Proxy would be: {'correct' if row['proxy_prediction'] == row['oracle_prediction'] else 'WRONG'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- High-confidence cases (≥ threshold) are typically clear-cut spam or legitimate messages\n",
    "- Low-confidence cases often involve ambiguous language or borderline content\n",
    "- Cascade successfully routes hard cases to oracle while saving cost on easy cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When Cascades Work vs. Fail\n",
    "\n",
    "Summary of cascade applicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics for cascade viability\n",
    "proxy_accuracy_overall = (test_df['proxy_prediction'] == test_df['oracle_prediction']).mean()\n",
    "proxy_accuracy_high_conf = (test_df[test_df['proxy_confidence'] >= 0.9]['proxy_prediction'] == \n",
    "                             test_df[test_df['proxy_confidence'] >= 0.9]['oracle_prediction']).mean()\n",
    "high_conf_rate = (test_df['proxy_confidence'] >= 0.9).mean()\n",
    "\n",
    "print(\"=== CASCADE VIABILITY CHECK ===\")\n",
    "print(f\"✓ Proxy overall accuracy: {proxy_accuracy_overall:.2%} (should be >85%)\")\n",
    "print(f\"✓ Proxy high-confidence accuracy: {proxy_accuracy_high_conf:.2%} (should be ≥95%)\")\n",
    "print(f\"✓ High-confidence rate: {high_conf_rate:.2%} (should be >50%)\")\n",
    "print(f\"✓ Cost multiplier: {results['total_oracle_cost'] / results['total_proxy_cost']:.1f}x (should be >5x)\")\n",
    "print(f\"\")\n",
    "print(\"✅ This dataset is EXCELLENT for model cascade!\")\n",
    "print(f\"   - Proxy handles {results['proxy_usage_rate']:.0%} of queries reliably\")\n",
    "print(f\"   - Cost savings: {savings_pct:.1f}% vs. oracle-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cascade Works When:**\n",
    "- ✅ Cheap model is \"good enough\" on easy cases (≥95% accuracy when confident)\n",
    "- ✅ Large portion of queries are easy (≥50% high confidence)\n",
    "- ✅ Expensive model is significantly more expensive (5-20x)\n",
    "- ✅ Confidence scores are calibrated (high confidence = high accuracy)\n",
    "\n",
    "**Cascade Fails When:**\n",
    "- ❌ Cheap model is too weak (<85% overall accuracy)\n",
    "- ❌ Few queries are confident (<30% high confidence)\n",
    "- ❌ Expensive model isn't much better (<5% accuracy gain)\n",
    "- ❌ Latency is critical (<500ms required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "**Exercise 1:** Modify the target accuracy to 0.95 instead of 0.99. How much more can you save?\n",
    "\n",
    "**Exercise 2:** Implement a 3-tier cascade (mini → small → large). When would this be beneficial?\n",
    "\n",
    "**Exercise 3:** Calculate the latency improvement of cascade vs. oracle-only. (Hint: Assume 800ms for proxy, 2000ms for oracle)\n",
    "\n",
    "**Exercise 4:** Identify cases where proxy is confident but WRONG. What patterns do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 solution starter\n",
    "print(\"=== CONFIDENT BUT WRONG CASES ===\")\n",
    "confident_wrong = test_df[\n",
    "    (test_df['proxy_confidence'] >= 0.9) & \n",
    "    (test_df['proxy_prediction'] != test_df['oracle_prediction'])\n",
    "]\n",
    "\n",
    "print(f\"Found {len(confident_wrong)} cases where proxy is confident (≥0.9) but wrong\\n\")\n",
    "for idx, row in confident_wrong.head(5).iterrows():\n",
    "    print(f\"Text: {row['text'][:100]}...\")\n",
    "    print(f\"Proxy: {row['proxy_prediction']} (conf: {row['proxy_confidence']:.4f}), Oracle: {row['oracle_prediction']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- ✅ **Cascades save 60-80% cost** while maintaining target accuracy\n",
    "- ✅ **Threshold optimization is critical** - Must balance accuracy and cost on train set\n",
    "- ✅ **Logprobs provide reliable confidence scores** for routing decisions\n",
    "- ✅ **Class-specific thresholds** allow more aggressive optimization\n",
    "- ✅ **Cost-accuracy trade-off is controllable** via threshold tuning\n",
    "- ✅ **Not all datasets benefit equally** - Need clear easy/hard distinction\n",
    "\n",
    "**Production Considerations:**\n",
    "- Monitor cascade accuracy over time (data drift may affect thresholds)\n",
    "- A/B test different thresholds in production\n",
    "- Consider latency implications (sequential calls add delay)\n",
    "- Log uncertain cases for manual review\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Model Cascade Concepts](model_cascade_concepts.md) - Architecture and theory\n",
    "- [Cascade Decision Tree Diagram](diagrams/cascade_decision_tree.mmd) - Routing logic visualization\n",
    "- [Lesson 8 Tutorial Index](TUTORIAL_INDEX.md) - Complete learning path\n",
    "\n",
    "---\n",
    "\n",
    "**Tutorial Status:** ✅ Complete\n",
    "**Last Updated:** 2025-10-30\n",
    "**Execution Time:** <2 minutes (uses cached predictions)\n",
    "**Dataset:** SMS Spam Collection (500 samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
