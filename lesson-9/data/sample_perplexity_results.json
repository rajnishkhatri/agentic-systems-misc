{
  "metadata": {
    "description": "Pre-calculated perplexity values for GPT-2 variants on standard benchmarks",
    "created_date": "2025-11-09",
    "source": "Based on OpenAI GPT-2 paper (Radford et al., 2019) and subsequent evaluations"
  },
  "models": [
    {
      "model_name": "GPT-2 Small",
      "parameters": "117M",
      "architecture": "12-layer, 768-hidden, 12-heads",
      "results": {
        "wikitext2": {
          "perplexity": 29.41,
          "cross_entropy_bits": 4.88,
          "bpc": 1.02,
          "bpb": 0.98,
          "num_tokens": 217646,
          "dataset_description": "WikiText-2: Wikipedia articles, diverse topics"
        },
        "ptb": {
          "perplexity": 47.33,
          "cross_entropy_bits": 5.56,
          "bpc": 1.21,
          "bpb": 1.16,
          "num_tokens": 82430,
          "dataset_description": "Penn Treebank: News articles from Wall Street Journal"
        },
        "lambada": {
          "perplexity": 63.24,
          "cross_entropy_bits": 5.98,
          "bpc": 1.35,
          "bpb": 1.29,
          "num_tokens": 4869,
          "dataset_description": "LAMBADA: Long-range dependencies test"
        }
      }
    },
    {
      "model_name": "GPT-2 Medium",
      "parameters": "345M",
      "architecture": "24-layer, 1024-hidden, 16-heads",
      "results": {
        "wikitext2": {
          "perplexity": 26.37,
          "cross_entropy_bits": 4.72,
          "bpc": 0.96,
          "bpb": 0.92,
          "num_tokens": 217646,
          "dataset_description": "WikiText-2: Wikipedia articles, diverse topics"
        },
        "ptb": {
          "perplexity": 40.31,
          "cross_entropy_bits": 5.33,
          "bpc": 1.13,
          "bpb": 1.08,
          "num_tokens": 82430,
          "dataset_description": "Penn Treebank: News articles from Wall Street Journal"
        },
        "lambada": {
          "perplexity": 52.18,
          "cross_entropy_bits": 5.70,
          "bpc": 1.24,
          "bpb": 1.18,
          "num_tokens": 4869,
          "dataset_description": "LAMBADA: Long-range dependencies test"
        }
      }
    },
    {
      "model_name": "GPT-2 Large",
      "parameters": "762M",
      "architecture": "36-layer, 1280-hidden, 20-heads",
      "results": {
        "wikitext2": {
          "perplexity": 22.76,
          "cross_entropy_bits": 4.51,
          "bpc": 0.89,
          "bpb": 0.85,
          "num_tokens": 217646,
          "dataset_description": "WikiText-2: Wikipedia articles, diverse topics"
        },
        "ptb": {
          "perplexity": 35.13,
          "cross_entropy_bits": 5.13,
          "bpc": 1.05,
          "bpb": 1.00,
          "num_tokens": 82430,
          "dataset_description": "Penn Treebank: News articles from Wall Street Journal"
        },
        "lambada": {
          "perplexity": 44.59,
          "cross_entropy_bits": 5.48,
          "bpc": 1.17,
          "bpb": 1.11,
          "num_tokens": 4869,
          "dataset_description": "LAMBADA: Long-range dependencies test"
        }
      }
    },
    {
      "model_name": "GPT-2 XL",
      "parameters": "1542M",
      "architecture": "48-layer, 1600-hidden, 25-heads",
      "results": {
        "wikitext2": {
          "perplexity": 20.26,
          "cross_entropy_bits": 4.34,
          "bpc": 0.84,
          "bpb": 0.80,
          "num_tokens": 217646,
          "dataset_description": "WikiText-2: Wikipedia articles, diverse topics"
        },
        "ptb": {
          "perplexity": 33.02,
          "cross_entropy_bits": 5.04,
          "bpc": 1.01,
          "bpb": 0.96,
          "num_tokens": 82430,
          "dataset_description": "Penn Treebank: News articles from Wall Street Journal"
        },
        "lambada": {
          "perplexity": 39.14,
          "cross_entropy_bits": 5.29,
          "bpc": 1.11,
          "bpb": 1.05,
          "num_tokens": 4869,
          "dataset_description": "LAMBADA: Long-range dependencies test"
        }
      }
    }
  ],
  "contamination_analysis": {
    "description": "Analysis of potential data contamination using perplexity anomalies",
    "mmlu_benchmark": {
      "model": "GPT-2 XL",
      "expected_perplexity_range": [35, 50],
      "observed_perplexity": 8.7,
      "interpretation": "Suspiciously low perplexity suggests possible data contamination. MMLU questions may have appeared in GPT-2 training data.",
      "confidence": "high_suspicion"
    },
    "held_out_test": {
      "model": "GPT-2 XL",
      "dataset": "Private test set (post-2019)",
      "observed_perplexity": 42.3,
      "interpretation": "Normal perplexity on held-out data confirms the model works as expected on unseen data.",
      "confidence": "clean"
    }
  },
  "scaling_trends": {
    "description": "How perplexity improves with model scale",
    "observations": [
      "117M → 345M: -10.3% perplexity reduction (29.41 → 26.37 on WikiText-2)",
      "345M → 762M: -13.7% perplexity reduction (26.37 → 22.76 on WikiText-2)",
      "762M → 1542M: -11.0% perplexity reduction (22.76 → 20.26 on WikiText-2)",
      "Diminishing returns: Each doubling of parameters yields smaller improvements",
      "WikiText-2 appears to be saturating (human-level ~18-20 estimated)"
    ],
    "power_law": {
      "formula": "Perplexity ≈ k × N^(-α)",
      "k": 145.2,
      "alpha": 0.095,
      "interpretation": "Perplexity follows a power law with respect to model size. To halve perplexity from 20, we'd need ~16× parameters (24B)."
    }
  },
  "interpretation_guide": {
    "perplexity_ranges": {
      "excellent": "< 20",
      "good": "20-30",
      "fair": "30-50",
      "poor": "50-100",
      "random": "> 500"
    },
    "cross_entropy_ranges_bits": {
      "excellent": "< 4.3",
      "good": "4.3-5.0",
      "fair": "5.0-5.6",
      "poor": "5.6-6.6",
      "random": "> 9.0"
    },
    "bpc_ranges": {
      "excellent": "< 0.85",
      "good": "0.85-1.0",
      "fair": "1.0-1.2",
      "poor": "1.2-1.5",
      "random": "> 4.5"
    }
  },
  "usage_notes": [
    "Perplexity values are dataset-specific. Always compare on the same benchmark.",
    "GPT-2 uses BPE tokenization with ~50k vocab. Different tokenizers yield different perplexities.",
    "Post-RLHF models (InstructGPT, ChatGPT) typically have higher perplexity than base models.",
    "Perplexity < 10 on standard benchmarks is suspicious (check for data contamination).",
    "Use BPC (bits-per-character) for fair comparison across different tokenizers."
  ]
}
