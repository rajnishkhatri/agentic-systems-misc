{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measurements Tutorial\n",
    "\n",
    "## Cost Warning\n",
    "\n",
    "- **DEMO MODE**: $0.20-0.50 (10 queries)\n",
    "- **FULL MODE**: $0.80-1.20 (50+ queries)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. Understand different similarity measurement techniques\n",
    "2. Implement exact match, fuzzy match, BLEU score, and semantic similarity\n",
    "3. Compare trade-offs between computational efficiency and semantic understanding\n",
    "4. Apply appropriate similarity metrics for different evaluation scenarios\n",
    "5. Analyze when to use each similarity measurement approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEMO_MODE = True  # Set to False for full dataset\n",
    "NUM_QUERIES = 10 if DEMO_MODE else 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Levenshtein import ratio\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We'll use recipe-related test examples with queries, reference answers, and candidate answers to evaluate different similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 recipe test examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"query\": \"How do I make chocolate chip cookies?\",\n",
    "        \"reference\": \"Mix butter, sugar, eggs, flour, and chocolate chips. Bake at 350F for 12 minutes.\",\n",
    "        \"candidate\": \"Combine butter, sugar, eggs, flour, and chocolate chips. Bake at 350 degrees for 12 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What ingredients do I need for pasta carbonara?\",\n",
    "        \"reference\": \"You need pasta, eggs, parmesan cheese, pancetta, and black pepper.\",\n",
    "        \"candidate\": \"The ingredients are spaghetti, eggs, parmesan, bacon, and pepper.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How long should I roast a chicken?\",\n",
    "        \"reference\": \"Roast at 425F for 20 minutes per pound, plus an additional 15 minutes.\",\n",
    "        \"candidate\": \"Cook at 425 degrees for approximately 20 minutes per pound.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the best way to cook rice?\",\n",
    "        \"reference\": \"Use a 2:1 water to rice ratio. Bring to boil, then simmer covered for 18 minutes.\",\n",
    "        \"candidate\": \"Use 2 cups water for 1 cup rice. Boil, then cover and simmer for 18 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I make scrambled eggs?\",\n",
    "        \"reference\": \"Beat eggs with milk, pour into hot buttered pan, and stir gently until set.\",\n",
    "        \"candidate\": \"Whisk eggs with a splash of milk, cook in butter over medium heat, stirring constantly.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What temperature should I bake bread?\",\n",
    "        \"reference\": \"Bake at 375F for 30-35 minutes until golden brown.\",\n",
    "        \"candidate\": \"Preheat oven to 375 degrees and bake for 30-35 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I make pizza dough?\",\n",
    "        \"reference\": \"Mix flour, yeast, water, salt, and olive oil. Knead for 10 minutes and let rise for 1 hour.\",\n",
    "        \"candidate\": \"Combine flour, yeast, warm water, salt, and oil. Knead well and allow to rise.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the secret to fluffy pancakes?\",\n",
    "        \"reference\": \"Don't overmix the batter and let it rest for 5 minutes before cooking.\",\n",
    "        \"candidate\": \"Mix the batter gently and let it sit briefly before making pancakes.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I properly season a steak?\",\n",
    "        \"reference\": \"Generously apply salt and pepper to both sides at least 40 minutes before cooking.\",\n",
    "        \"candidate\": \"Season liberally with salt and pepper on both sides, preferably 40+ minutes ahead.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the ideal temperature for brewing coffee?\",\n",
    "        \"reference\": \"Water should be between 195-205F for optimal extraction.\",\n",
    "        \"candidate\": \"The water temperature should range from 195 to 205 degrees Fahrenheit.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Limit to NUM_QUERIES\n",
    "test_examples = test_examples[:NUM_QUERIES]\n",
    "print(f\"Loaded {len(test_examples)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exact Match\n",
    "\n",
    "The simplest similarity metric: are the strings exactly the same after normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by lowercasing and removing extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to normalize\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"text must be a string\")\n",
    "    \n",
    "    # Lowercase and remove extra whitespace\n",
    "    normalized = re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def exact_match(reference: str, candidate: str) -> bool:\n",
    "    \"\"\"Check if two strings match exactly after normalization.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference string\n",
    "        candidate: Candidate string to compare\n",
    "        \n",
    "    Returns:\n",
    "        True if strings match exactly, False otherwise\n",
    "    \"\"\"\n",
    "    if not isinstance(reference, str) or not isinstance(candidate, str):\n",
    "        raise TypeError(\"Both reference and candidate must be strings\")\n",
    "    \n",
    "    return normalize_text(reference) == normalize_text(candidate)\n",
    "\n",
    "\n",
    "# Calculate exact match rate\n",
    "exact_matches = [exact_match(ex[\"reference\"], ex[\"candidate\"]) for ex in test_examples]\n",
    "exact_match_rate = sum(exact_matches) / len(exact_matches) * 100\n",
    "\n",
    "print(f\"Exact Match Rate: {exact_match_rate:.1f}%\")\n",
    "print(f\"Matches: {sum(exact_matches)}/{len(exact_matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fuzzy Match (Levenshtein Ratio)\n",
    "\n",
    "Fuzzy matching measures string similarity based on edit distance. The Levenshtein ratio ranges from 0 (completely different) to 1 (identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(reference: str, candidate: str, threshold: float = 0.8) -> tuple[bool, float]:\n",
    "    \"\"\"Calculate fuzzy match using Levenshtein ratio.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference string\n",
    "        candidate: Candidate string to compare\n",
    "        threshold: Minimum ratio to consider a match (0.0-1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_match, similarity_score)\n",
    "        \n",
    "    Raises:\n",
    "        TypeError: If inputs are not strings\n",
    "        ValueError: If threshold is not between 0 and 1\n",
    "    \"\"\"\n",
    "    if not isinstance(reference, str) or not isinstance(candidate, str):\n",
    "        raise TypeError(\"Both reference and candidate must be strings\")\n",
    "    \n",
    "    if not 0 <= threshold <= 1:\n",
    "        raise ValueError(\"threshold must be between 0 and 1\")\n",
    "    \n",
    "    # Calculate Levenshtein ratio\n",
    "    similarity = ratio(normalize_text(reference), normalize_text(candidate))\n",
    "    is_match = similarity >= threshold\n",
    "    \n",
    "    return is_match, similarity\n",
    "\n",
    "\n",
    "# Test with different thresholds\n",
    "thresholds = [0.7, 0.8, 0.9]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    results = [fuzzy_match(ex[\"reference\"], ex[\"candidate\"], thresh) for ex in test_examples]\n",
    "    matches = sum(match for match, _ in results)\n",
    "    avg_score = np.mean([score for _, score in results])\n",
    "    \n",
    "    print(f\"\\nThreshold {thresh}:\")\n",
    "    print(f\"  Match Rate: {matches/len(results)*100:.1f}%\")\n",
    "    print(f\"  Average Score: {avg_score:.3f}\")\n",
    "\n",
    "# Store scores for later comparison\n",
    "fuzzy_scores = [fuzzy_match(ex[\"reference\"], ex[\"candidate\"], 0.8)[1] for ex in test_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: BLEU Score\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) measures n-gram overlap between reference and candidate text. Originally designed for machine translation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference: str, candidate: str) -> float:\n",
    "    \"\"\"Calculate BLEU score between reference and candidate.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference string\n",
    "        candidate: Candidate string to compare\n",
    "        \n",
    "    Returns:\n",
    "        BLEU score (0.0-1.0)\n",
    "        \n",
    "    Raises:\n",
    "        TypeError: If inputs are not strings\n",
    "    \"\"\"\n",
    "    if not isinstance(reference, str) or not isinstance(candidate, str):\n",
    "        raise TypeError(\"Both reference and candidate must be strings\")\n",
    "    \n",
    "    # Tokenize by whitespace\n",
    "    reference_tokens = normalize_text(reference).split()\n",
    "    candidate_tokens = normalize_text(candidate).split()\n",
    "    \n",
    "    if not candidate_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    # Use smoothing to handle edge cases\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# Calculate BLEU scores for all examples\n",
    "bleu_scores = [calculate_bleu(ex[\"reference\"], ex[\"candidate\"]) for ex in test_examples]\n",
    "\n",
    "print(f\"Average BLEU Score: {np.mean(bleu_scores):.3f}\")\n",
    "print(f\"Min BLEU Score: {np.min(bleu_scores):.3f}\")\n",
    "print(f\"Max BLEU Score: {np.max(bleu_scores):.3f}\")\n",
    "\n",
    "# Show individual scores\n",
    "print(\"\\nIndividual BLEU Scores:\")\n",
    "for i, (ex, score) in enumerate(zip(test_examples, bleu_scores), 1):\n",
    "    print(f\"{i}. {score:.3f} - {ex['query'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Semantic Similarity\n",
    "\n",
    "Semantic similarity uses embeddings to capture meaning, not just surface-level text overlap. This can identify semantically equivalent text even with different wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> list[float]:\n",
    "    \"\"\"Get OpenAI embedding for text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model: OpenAI embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        Embedding vector as list of floats\n",
    "        \n",
    "    Raises:\n",
    "        TypeError: If text is not a string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"text must be a string\")\n",
    "    \n",
    "    if not text.strip():\n",
    "        raise ValueError(\"text cannot be empty\")\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vec1: First vector\n",
    "        vec2: Second vector\n",
    "        \n",
    "    Returns:\n",
    "        Cosine similarity score (-1.0 to 1.0)\n",
    "        \n",
    "    Raises:\n",
    "        TypeError: If inputs are not lists\n",
    "        ValueError: If vectors have different lengths\n",
    "    \"\"\"\n",
    "    if not isinstance(vec1, list) or not isinstance(vec2, list):\n",
    "        raise TypeError(\"Both vectors must be lists\")\n",
    "    \n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(\"Vectors must have the same length\")\n",
    "    \n",
    "    if not vec1 or not vec2:\n",
    "        raise ValueError(\"Vectors cannot be empty\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    \n",
    "    if norm_product == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / norm_product\n",
    "\n",
    "\n",
    "def semantic_similarity(reference: str, candidate: str) -> float:\n",
    "    \"\"\"Calculate semantic similarity using embeddings.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference string\n",
    "        candidate: Candidate string to compare\n",
    "        \n",
    "    Returns:\n",
    "        Semantic similarity score (0.0-1.0)\n",
    "    \"\"\"\n",
    "    if not isinstance(reference, str) or not isinstance(candidate, str):\n",
    "        raise TypeError(\"Both reference and candidate must be strings\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    ref_embedding = get_embedding(reference)\n",
    "    cand_embedding = get_embedding(candidate)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = cosine_similarity(ref_embedding, cand_embedding)\n",
    "    \n",
    "    # Normalize to 0-1 range (cosine similarity is -1 to 1)\n",
    "    return (similarity + 1) / 2\n",
    "\n",
    "\n",
    "# Calculate semantic similarity for all examples\n",
    "print(\"Calculating semantic similarities (this may take a moment)...\")\n",
    "semantic_scores = []\n",
    "\n",
    "for i, ex in enumerate(test_examples, 1):\n",
    "    score = semantic_similarity(ex[\"reference\"], ex[\"candidate\"])\n",
    "    semantic_scores.append(score)\n",
    "    print(f\"  {i}/{len(test_examples)} - Score: {score:.3f}\")\n",
    "\n",
    "print(f\"\\nAverage Semantic Similarity: {np.mean(semantic_scores):.3f}\")\n",
    "print(f\"Min Semantic Similarity: {np.min(semantic_scores):.3f}\")\n",
    "print(f\"Max Semantic Similarity: {np.max(semantic_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Comparison Analysis\n",
    "\n",
    "Let's compare all methods side-by-side to understand their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for i, ex in enumerate(test_examples):\n",
    "    comparison_data.append({\n",
    "        \"Query\": ex[\"query\"][:40] + \"...\",\n",
    "        \"Exact Match\": exact_matches[i],\n",
    "        \"Fuzzy (0.8)\": fuzzy_scores[i],\n",
    "        \"BLEU\": bleu_scores[i],\n",
    "        \"Semantic\": semantic_scores[i]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON OF SIMILARITY METHODS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\"Exact Match\", \"Fuzzy Match\", \"BLEU\", \"Semantic\"],\n",
    "    \"Mean\": [\n",
    "        sum(exact_matches) / len(exact_matches),\n",
    "        np.mean(fuzzy_scores),\n",
    "        np.mean(bleu_scores),\n",
    "        np.mean(semantic_scores)\n",
    "    ],\n",
    "    \"Std Dev\": [\n",
    "        np.std([float(x) for x in exact_matches]),\n",
    "        np.std(fuzzy_scores),\n",
    "        np.std(bleu_scores),\n",
    "        np.std(semantic_scores)\n",
    "    ],\n",
    "    \"Min\": [\n",
    "        min([float(x) for x in exact_matches]),\n",
    "        min(fuzzy_scores),\n",
    "        min(bleu_scores),\n",
    "        min(semantic_scores)\n",
    "    ],\n",
    "    \"Max\": [\n",
    "        max([float(x) for x in exact_matches]),\n",
    "        max(fuzzy_scores),\n",
    "        max(bleu_scores),\n",
    "        max(semantic_scores)\n",
    "    ]\n",
    "}).round(3)\n",
    "\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Method Comparison\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Exact Match** | Fast, deterministic, no API cost | Too strict, misses paraphrases | Classification tasks, code validation |\n",
    "| **Fuzzy Match** | Fast, handles typos, no API cost | Surface-level only, threshold tuning | Near-duplicates, typo detection |\n",
    "| **BLEU** | Proven in MT, n-gram overlap | Doesn't capture semantics, score interpretation | Translation, text generation |\n",
    "| **Semantic** | Captures meaning, robust to paraphrasing | API cost, slower, needs embedding model | QA evaluation, semantic search |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Exact match** is too strict for most NLP tasks - it rarely matches even when answers are semantically identical\n",
    "2. **Fuzzy match** is useful for handling minor variations but doesn't understand meaning\n",
    "3. **BLEU score** correlates with word overlap but can miss semantic equivalence\n",
    "4. **Semantic similarity** best captures meaning but requires API calls and is slower\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- **For evaluation pipelines**: Use semantic similarity as the primary metric\n",
    "- **For fast screening**: Start with fuzzy match, then apply semantic similarity to uncertain cases\n",
    "- **For retrieval**: Use semantic similarity for ranking\n",
    "- **For exact requirements**: Use exact match only when truly necessary (e.g., code, IDs)\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Speed vs. Accuracy**: Exact/Fuzzy are fast but less accurate; Semantic is slower but more accurate\n",
    "- **Cost vs. Quality**: Exact/Fuzzy/BLEU are free; Semantic requires API calls\n",
    "- **Simplicity vs. Sophistication**: Simpler methods are easier to debug; Semantic is more powerful but harder to interpret\n",
    "\n",
    "Choose the right metric based on your specific use case, balancing speed, cost, and accuracy requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}