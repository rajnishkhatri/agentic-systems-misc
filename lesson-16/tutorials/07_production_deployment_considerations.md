# Tutorial 07: Production Deployment Considerations

**Estimated Reading Time:** 23 minutes
**Prerequisites:**
- **Tutorial 01: Agent Reliability Fundamentals** (understanding of 5 failure modes)
- **Tutorial 02: Orchestration Patterns Overview** (knowledge of 5 patterns)
- **Tutorial 06: Financial Workflow Reliability** (compliance and audit requirements)
- Understanding of production systems (monitoring, SLAs, cost optimization)
- Familiarity with observability concepts (recommended)

**Learning Objectives:**
- Design cost optimization strategies for production AI agents (caching, early termination, model cascades)
- Implement error rate monitoring and root cause analysis for production systems
- Configure latency SLAs and async patterns for performance requirements
- Integrate observability tooling (Prometheus, Elasticsearch, Grafana) for future monitoring
- Build production-ready deployment checklists and runbooks

**Related Resources:**
- **Previous Tutorial:** [Tutorial 06: Financial Workflow Reliability](06_financial_workflow_reliability.md)
- **Related Tutorials:**
  - [Tutorial 03: Deterministic Execution Strategies](03_deterministic_execution_strategies.md) - Checkpointing and validation
  - [Tutorial 04: Error Propagation Analysis](04_error_propagation_analysis.md) - Isolation and early termination
- **Interactive Notebook:**
  - [Notebook 15: Production Deployment Tutorial](../notebooks/15_production_deployment_tutorial.ipynb) - Cost optimization, monitoring, compliance demo
- **Backend Code:**
  - `backend/reliability/circuit_breaker.py:1-180` - Circuit breaker for latency management
  - `backend/reliability/fallback.py:1-220` - Cache fallback and degradation strategies
  - `backend/reliability/audit_log.py:1-150` - Production audit logging
  - `backend/benchmarks/metrics.py:140-180` - Cost tracking implementation
- **Diagrams:**
  - `diagrams/reliability_framework_architecture.mmd` - 7-layer production architecture

---

## Table of Contents
1. [Introduction](#introduction)
2. [Production Challenges for AI Agents](#production-challenges-for-ai-agents)
3. [Cost Optimization Strategies](#cost-optimization-strategies)
4. [Error Rate Targets and Monitoring](#error-rate-targets-and-monitoring)
5. [Latency SLAs and Async Patterns](#latency-slas-and-async-patterns)
6. [Observability Integration Preview](#observability-integration-preview)
7. [Production Readiness Checklist](#production-readiness-checklist)
8. [Common Pitfalls](#common-pitfalls)
9. [Hands-On Exercises](#hands-on-exercises)
10. [Summary](#summary)
11. [Further Reading](#further-reading)

---

## Introduction

### Why Production Deployment Is Different

Moving AI agents from **prototype to production** introduces constraints that rarely appear during development:

| Development | Production |
|-------------|------------|
| "Does it work?" | "Does it work **reliably** at **scale**?" |
| Unlimited budget | **Cost optimization** required |
| Best-effort latency | **SLA enforcement** (P95 < 10s) |
| Manual debugging | **Automated monitoring** and alerts |
| Single environment | **Multi-stage deployment** (dev/staging/prod) |
| Error logs | **Observability stack** (metrics, logs, traces) |

### The Production Gap

**Example: Invoice Processing Agent**

**Development Metrics:**
- Success rate: 92% (tested on 50 invoices)
- Average latency: 5.2 seconds
- Cost: "A few dollars in API calls"

**Production Reality (First Week):**
- Success rate: 78% (tested on 10,000 invoices)
- P95 latency: 18.5 seconds (SLA violated)
- Cost: $4,200/week = $218K/year
- On-call pages: 37 incidents
- Root cause: No error handling for OCR failures, no caching, no circuit breakers

**This tutorial teaches how to bridge the production gap.**

---

## Production Challenges for AI Agents

### Challenge 1: Cost at Scale

**Problem:** LLM API costs scale linearly with request volume.

**Example: Invoice Processing Costs**

```python
# Naive implementation
def process_invoice(invoice_id: str) -> dict:
    """Process invoice with vendor extraction."""
    # Call 1: Extract vendor from OCR text (GPT-4)
    vendor = extract_vendor(invoice_id)  # $0.03 per call

    # Call 2: Validate vendor against database (GPT-4)
    is_valid = validate_vendor(vendor)  # $0.03 per call

    # Call 3: Route for approval (GPT-4)
    approver = route_approval(vendor, amount)  # $0.03 per call

    return {"vendor": vendor, "approver": approver}

# Cost Analysis
# - 10,000 invoices/week
# - 3 LLM calls per invoice
# - $0.03 per call (GPT-4 average)
# Total: 10,000 √ó 3 √ó $0.03 = $900/week = $46,800/year
```

**Production Impact:**
- **First month:** $3,900 (unexpected budget overrun)
- **CFO question:** "Why are we spending $50K/year on invoice processing?"
- **Engineering task:** Implement cost optimization (covered in Section 3)

### Challenge 2: Error Rate Variance

**Problem:** Real-world data is messier than test data.

**Development Test Set (50 invoices):**
- Clean OCR scans
- Standard vendor names
- No duplicates
- Success rate: 92%

**Production Data (10,000 invoices):**
- 15% OCR errors ("Acme Corp" ‚Üí "Acm3 C0rp")
- 10% missing fields (no invoice date)
- 8% duplicates (same invoice submitted twice)
- 5% edge cases (negative amounts, future dates)
- **Success rate: 78%** (14% drop)

**Production Impact:**
- 2,200 failed invoices/week
- Manual review queue backlog
- Vendor payment delays
- Audit findings

### Challenge 3: Latency SLA Violations

**Problem:** P95 latency >> average latency due to tail behavior.

**Latency Distribution (Production):**

```
Invoice Processing Latency (10,000 requests)
P50 (median):  5.2s  ‚Üê Development testing average
P90:          8.7s
P95:         18.5s  ‚Üê SLA violation! (target: <10s)
P99:         42.3s  ‚Üê Timeout risk
Max:        127.8s  ‚Üê User timeout (browser gives up)
```

**Root Causes:**
1. **Sequential orchestration:** 3 agents run serially (5s + 6s + 7s = 18s)
2. **No circuit breaker:** Failing agents retry 3√ó with exponential backoff (adds 30s)
3. **Cold start:** First request after deployment takes 45s
4. **No async patterns:** Blocking I/O for database lookups

**Production Impact:**
- 500 requests/week exceed 10s SLA (5% violation rate)
- User complaints about "slow system"
- Business process delays

---

## Cost Optimization Strategies

### Strategy 1: Caching with TTL

**Concept:** Cache expensive LLM results for frequently accessed data.

**Implementation:**

```python
from backend.reliability.fallback import FallbackHandler, FallbackStrategy

class InvoiceProcessor:
    """Invoice processor with Redis caching (60% cost reduction)."""

    def __init__(self) -> None:
        """Initialize processor with cache fallback."""
        self.cache_handler = FallbackHandler(
            strategy=FallbackStrategy.CACHE
        )

    def process_invoice(self, invoice_id: str) -> dict:
        """Process invoice with vendor extraction (cached).

        Args:
            invoice_id: Invoice identifier

        Returns:
            Invoice processing result with vendor and approver
        """
        # Step 1: Try cache first (TTL=24 hours)
        cache_key = f"invoice:{invoice_id}"

        # Step 2: Cache hit ‚Üí skip LLM call (save $0.09)
        def extract_vendor_llm() -> dict:
            vendor = self._call_llm_extract(invoice_id)  # $0.03
            is_valid = self._call_llm_validate(vendor)   # $0.03
            approver = self._call_llm_route(vendor)      # $0.03
            return {"vendor": vendor, "approver": approver}

        # Execute with cache fallback
        result = self.cache_handler.execute_with_fallback(
            extract_vendor_llm,
            cache_key=cache_key
        )

        return result
```

**Cost Savings Calculation:**

```python
# Cache Hit Rate Analysis (Real Production Data)
# - Duplicate invoices: 8% (same invoice submitted multiple times)
# - Vendor re-validation: 20% (same vendor across invoices)
# - Approval routing reuse: 30% (same approver for vendor)
# Total cache hit rate: ~60% (conservative estimate)

# Before Caching:
# 10,000 invoices √ó 3 calls √ó $0.03 = $900/week

# After Caching (60% hit rate):
# Cache hits:   6,000 invoices √ó $0.00 = $0
# Cache misses: 4,000 invoices √ó 3 calls √ó $0.03 = $360/week
# Total: $360/week

# Savings: $900 - $360 = $540/week = $28,080/year (60% reduction)
```

**Trade-offs:**
- **Pro:** 60% cost reduction, faster response time (cache retrieval <50ms)
- **Con:** Stale data risk (mitigated by 24hr TTL), Redis infrastructure cost ($50/month)
- **Decision:** Use caching for vendor validation (changes rarely), skip for fraud detection (needs real-time data)

### Strategy 2: Early Termination (Adaptive Voting)

**Concept:** Stop expensive voting orchestration early if confidence threshold met.

**Problem: Voting Pattern Cost Multiplier**

```python
# Voting orchestration for fraud detection
def detect_fraud_voting(transaction: dict) -> dict:
    """Run 5 agents in parallel and vote (expensive!)."""
    agents = [agent1, agent2, agent3, agent4, agent5]

    # Call all 5 agents ($0.03 each)
    results = run_parallel(agents, transaction)  # $0.15 total

    # Majority vote
    fraud_votes = sum(1 for r in results if r["is_fraud"])
    confidence = fraud_votes / len(agents)

    return {"is_fraud": fraud_votes >= 3, "confidence": confidence}

# Cost: 5 agents √ó $0.03 = $0.15 per transaction
# Volume: 10,000 transactions/week
# Total: $1,500/week = $78,000/year
```

**Optimization: Adaptive Voting with Early Termination**

```python
def detect_fraud_adaptive(transaction: dict) -> dict:
    """Run agents sequentially until confidence threshold met.

    Early termination rules:
    - Stop after 3 agents if all agree (confidence=1.0)
    - Stop after 4 agents if 3+ agree (confidence‚â•0.75)
    - Run all 5 only if still uncertain
    """
    agents = [agent1, agent2, agent3, agent4, agent5]
    results = []

    for i, agent in enumerate(agents):
        # Call agent (incremental cost)
        result = agent.detect(transaction)  # $0.03
        results.append(result)

        # Check early termination conditions
        fraud_votes = sum(1 for r in results if r["is_fraud"])
        total_votes = len(results)
        confidence = fraud_votes / total_votes

        # Rule 1: After 3 agents, unanimous decision
        if total_votes == 3 and (fraud_votes == 3 or fraud_votes == 0):
            return {"is_fraud": fraud_votes >= 2, "confidence": 1.0, "agents_used": 3}

        # Rule 2: After 4 agents, strong majority (3+ votes)
        if total_votes == 4 and (fraud_votes >= 3 or fraud_votes <= 1):
            return {"is_fraud": fraud_votes >= 2, "confidence": 0.75, "agents_used": 4}

    # Rule 3: Use all 5 agents if uncertain
    fraud_votes = sum(1 for r in results if r["is_fraud"])
    return {"is_fraud": fraud_votes >= 3, "confidence": fraud_votes / 5, "agents_used": 5}
```

**Cost Savings:**

```python
# Distribution of Early Termination (Production Data)
# - 40% transactions: Clear non-fraud (3 agents agree, stop after 3)
# - 30% transactions: Clear fraud (3 agents agree, stop after 3)
# - 20% transactions: Strong majority after 4 agents
# - 10% transactions: Uncertain, need all 5 agents

# Cost Calculation:
# 40% √ó 3 agents √ó $0.03 = $0.036 (clear non-fraud)
# 30% √ó 3 agents √ó $0.03 = $0.027 (clear fraud)
# 20% √ó 4 agents √ó $0.03 = $0.024 (strong majority)
# 10% √ó 5 agents √ó $0.03 = $0.015 (uncertain)
# Average cost per transaction = $0.102 (vs. $0.15 baseline)

# Weekly cost: 10,000 √ó $0.102 = $1,020/week
# Savings: $1,500 - $1,020 = $480/week = $24,960/year (32% reduction)
```

**Trade-offs:**
- **Pro:** 32% cost reduction, maintains accuracy (confidence thresholds preserve quality)
- **Con:** Sequential execution adds latency (3-5s vs. parallel 2s), complexity in threshold tuning
- **Decision:** Use for high-volume, low-stakes transactions (fraud screening), not for high-stakes ($50K+ wire transfers)

### Strategy 3: Model Cascades (GPT-3.5 Screening ‚Üí GPT-4 Escalation)

**Concept:** Use cheap model for easy cases, expensive model only for hard cases.

**Implementation:**

```python
class TwoStageInvoiceProcessor:
    """Two-stage processing with model cascade.

    Stage 1: GPT-3.5-turbo screening ($0.002/call) - handles 70% of invoices
    Stage 2: GPT-4 escalation ($0.03/call) - handles remaining 30%
    """

    def process_invoice(self, invoice_data: dict) -> dict:
        """Process invoice with model cascade.

        Args:
            invoice_data: Invoice OCR data

        Returns:
            Processing result with vendor and confidence
        """
        # Stage 1: GPT-3.5 screening (fast and cheap)
        screening_result = self._screen_with_gpt35(invoice_data)

        # Escalation criteria
        if screening_result["confidence"] >= 0.9:
            # Easy case: High confidence from cheap model
            return {
                "vendor": screening_result["vendor"],
                "confidence": screening_result["confidence"],
                "model": "gpt-3.5-turbo",
                "cost": 0.002
            }

        # Stage 2: GPT-4 escalation (slow and expensive)
        escalation_result = self._escalate_to_gpt4(invoice_data)

        return {
            "vendor": escalation_result["vendor"],
            "confidence": escalation_result["confidence"],
            "model": "gpt-4",
            "cost": 0.032  # 0.002 (screening) + 0.03 (escalation)
        }
```

**Cost Savings:**

```python
# Production Distribution (Real Invoice Data)
# - 70% invoices: Clean OCR, standard vendors ‚Üí GPT-3.5 sufficient (confidence ‚â•0.9)
# - 30% invoices: OCR errors, ambiguous vendors ‚Üí GPT-4 needed

# Baseline (GPT-4 only):
# 10,000 invoices √ó $0.03 = $300/week

# Model Cascade:
# 7,000 easy √ó $0.002 = $14/week (GPT-3.5 only)
# 3,000 hard √ó $0.032 = $96/week (GPT-3.5 screening + GPT-4 escalation)
# Total: $110/week

# Savings: $300 - $110 = $190/week = $9,880/year (63% reduction)
```

**Confidence Threshold Tuning:**

| Threshold | Easy Cases (GPT-3.5) | Hard Cases (GPT-4) | Accuracy | Cost/Week |
|-----------|---------------------|-------------------|----------|-----------|
| 0.95 | 50% | 50% | 98% | $167 |
| 0.90 | 70% | 30% | 96% | $110 |
| 0.85 | 80% | 20% | 92% | $80 |

**Decision:** Use 0.90 threshold (96% accuracy, $110/week cost) - best balance.

### Strategy 4: Cost Tracking and Budget Alerts

**Implementation:**

```python
class CostTracker:
    """Track LLM API costs with budget alerts (FR6.1)."""

    def __init__(self, daily_budget: float = 100.0) -> None:
        """Initialize cost tracker.

        Args:
            daily_budget: Maximum daily spend in USD
        """
        self.daily_budget = daily_budget
        self.costs: dict[str, float] = {}

    def track_call(self, model: str, tokens: int) -> None:
        """Track single LLM API call cost.

        Args:
            model: Model name (e.g., "gpt-4", "gpt-3.5-turbo")
            tokens: Total tokens used (input + output)
        """
        # Pricing per 1K tokens (as of 2024)
        pricing = {
            "gpt-4": 0.03 / 1000,
            "gpt-3.5-turbo": 0.002 / 1000,
        }

        cost = tokens * pricing.get(model, 0.03 / 1000)

        # Aggregate by model
        self.costs[model] = self.costs.get(model, 0.0) + cost

        # Check budget threshold
        total_cost = sum(self.costs.values())
        if total_cost > self.daily_budget * 0.8:
            self._send_alert(total_cost, self.daily_budget)

    def _send_alert(self, current: float, budget: float) -> None:
        """Send budget alert to ops team."""
        print(f"‚ö†Ô∏è COST ALERT: ${current:.2f} / ${budget:.2f} (80% threshold)")
```

**Production Dashboard Metrics:**

```python
# Daily Cost Dashboard (Grafana)
# - Total spend: $142.50 / $100.00 budget (ALERT!)
# - GPT-4 calls: 3,200 √ó $0.03 = $96.00 (67%)
# - GPT-3.5 calls: 18,000 √ó $0.002 = $36.00 (25%)
# - GPT-4-turbo calls: 350 √ó $0.03 = $10.50 (7%)
# - Top expensive workflow: Fraud detection voting ($68/day)
# - Recommendation: Enable caching for vendor validation (save $28/day)
```

---

## Error Rate Targets and Monitoring

### Task-Specific Error Rate Targets

**Not all tasks have the same error tolerance.**

| Task Type | Error Rate Target | Rationale |
|-----------|------------------|-----------|
| **Payment amount extraction** | <0.1% | Direct financial impact (overpayment/underpayment) |
| **Vendor name extraction** | <5% | Recoverable via database fuzzy match |
| **Invoice routing** | <10% | Manual review queue available |
| **Fraud detection screening** | <15% | False positives acceptable (human review) |

**Production Target Selection (FR6.2):**

```python
# Invoice Processing Error Budget
def calculate_error_budget(task_type: str, volume: int) -> dict:
    """Calculate acceptable error count based on task type.

    Args:
        task_type: Type of task (payment, vendor, routing, fraud)
        volume: Total task volume (e.g., 10,000 invoices/week)

    Returns:
        Error budget with acceptable failures and alert threshold
    """
    targets = {
        "payment": 0.001,  # <0.1%
        "vendor": 0.05,    # <5%
        "routing": 0.10,   # <10%
        "fraud": 0.15,     # <15%
    }

    target_rate = targets.get(task_type, 0.05)
    acceptable_failures = int(volume * target_rate)
    alert_threshold = int(acceptable_failures * 0.8)  # 80% of budget

    return {
        "target_rate": target_rate,
        "acceptable_failures": acceptable_failures,
        "alert_threshold": alert_threshold,
    }

# Example: Vendor extraction (10,000 invoices/week)
# - Target: <5% error rate
# - Acceptable failures: 500 invoices
# - Alert threshold: 400 invoices (80% of budget)
```

### Rolling Window Monitoring

**Problem:** Daily error rates are too noisy for alerting.

**Solution:** 100-task rolling window with alert on threshold breach.

```python
from collections import deque

class ErrorRateMonitor:
    """Monitor error rate with rolling window (FR6.2)."""

    def __init__(self, window_size: int = 100, threshold: float = 0.05) -> None:
        """Initialize error rate monitor.

        Args:
            window_size: Number of recent tasks to track
            threshold: Error rate threshold for alerts (0.05 = 5%)
        """
        self.window_size = window_size
        self.threshold = threshold
        self.results = deque(maxlen=window_size)

    def record_result(self, success: bool) -> dict:
        """Record task result and check for alerts.

        Args:
            success: True if task succeeded, False if failed

        Returns:
            Alert status with current error rate
        """
        self.results.append(success)

        # Calculate error rate over rolling window
        if len(self.results) < self.window_size:
            return {"alert": False, "reason": "insufficient_data"}

        failures = sum(1 for r in self.results if not r)
        error_rate = failures / len(self.results)

        # Check threshold
        if error_rate > self.threshold:
            return {
                "alert": True,
                "error_rate": error_rate,
                "threshold": self.threshold,
                "failures": failures,
                "window_size": len(self.results),
            }

        return {"alert": False, "error_rate": error_rate}
```

**Production Alert Example:**

```
üö® ERROR RATE ALERT: Vendor Extraction
- Current error rate: 7.2% (72 failures / 1000 recent tasks)
- Threshold: 5%
- Time window: Last 100 tasks (2 hours)
- Root cause: OCR quality degradation (new scanner deployment)
- Action: Rollback scanner firmware, re-process failed invoices
```

### Root Cause Analysis (RCA)

**Categorize failures for systematic improvement.**

```python
class ErrorAnalyzer:
    """Analyze error patterns for root cause identification."""

    def analyze_failures(self, failures: list[dict]) -> dict:
        """Group failures by root cause.

        Args:
            failures: List of failed tasks with error metadata

        Returns:
            Failure breakdown by category with counts
        """
        categories = {
            "hallucination": 0,
            "timeout": 0,
            "validation_error": 0,
            "ocr_error": 0,
            "missing_field": 0,
        }

        for failure in failures:
            error_type = failure.get("error_type", "unknown")
            categories[error_type] = categories.get(error_type, 0) + 1

        # Calculate percentages
        total = len(failures)
        breakdown = {
            category: {
                "count": count,
                "percentage": (count / total * 100) if total > 0 else 0,
            }
            for category, count in categories.items()
        }

        return breakdown

# Production RCA Output
# Vendor Extraction Failures (72 failures in 2 hours)
# - OCR errors: 45 (62%) ‚Üê PRIMARY ROOT CAUSE
# - Missing field: 12 (17%)
# - Validation errors: 8 (11%)
# - Hallucinations: 5 (7%)
# - Timeouts: 2 (3%)
#
# Recommended Action: Investigate OCR quality (scanner firmware change)
```

---

## Latency SLAs and Async Patterns

### SLA Definition: P95 Latency

**Why P95, not average?**

```
Invoice Processing Latency Distribution
Average:  5.2s ‚Üê Misleading! Only 50% of users see this
P50:      5.2s
P90:      8.7s
P95:     18.5s ‚Üê 5% of users wait this long (SLA violation)
P99:     42.3s ‚Üê 1% of users timeout
```

**SLA Target:** P95 < 10s (95% of requests complete in <10 seconds)

### Pattern 1: Async Parallel Execution (Hierarchical Orchestration)

**Problem:** Sequential orchestration adds latencies (5s + 6s + 7s = 18s).

**Solution:** Run independent agents in parallel.

```python
import asyncio

async def process_invoice_parallel(invoice_data: dict) -> dict:
    """Process invoice with parallel agent execution.

    Parallel execution:
    - Extract vendor: 5s
    - Extract amount: 6s
    - Extract date: 7s
    Total: max(5s, 6s, 7s) = 7s (vs. 18s sequential)
    """
    # Run 3 agents in parallel (asyncio.gather)
    vendor_task = extract_vendor_async(invoice_data)
    amount_task = extract_amount_async(invoice_data)
    date_task = extract_date_async(invoice_data)

    # Wait for all to complete (total time = slowest agent)
    vendor, amount, date = await asyncio.gather(
        vendor_task,
        amount_task,
        date_task
    )

    # Latency: 7s (slowest agent) vs. 18s (sequential)
    return {"vendor": vendor, "amount": amount, "date": date}
```

**Latency Improvement:**
- Before: P95 = 18.5s (sequential)
- After: P95 = 8.2s (parallel) ‚Üê 56% reduction, SLA met!

### Pattern 2: Circuit Breaker for Timeout Prevention

**Problem:** Failing agents retry 3√ó with exponential backoff (adds 30s).

**Solution:** Circuit breaker opens after 5 failures, rejects calls immediately.

```python
from backend.reliability.circuit_breaker import CircuitBreaker

# Initialize circuit breaker (FR4.2)
circuit_breaker = CircuitBreaker(
    failure_threshold=5,  # Open after 5 failures
    timeout=60.0,  # Stay open for 60s
)

async def extract_vendor_protected(invoice_data: dict) -> dict:
    """Extract vendor with circuit breaker protection.

    Benefits:
    - Fast failure when service down (reject in <10ms vs. 30s timeout)
    - Prevents cascade failures
    - Automatic recovery after timeout period
    """
    try:
        result = await circuit_breaker.call(
            lambda: extract_vendor_llm(invoice_data)
        )
        return result
    except CircuitBreakerOpenError:
        # Fallback: Use cached result or default
        return {"vendor": "UNKNOWN", "confidence": 0.0}
```

**Latency Distribution After Circuit Breaker:**

```
Before Circuit Breaker:
P95: 18.5s (includes 30s retry delays)
P99: 42.3s

After Circuit Breaker:
P95: 8.2s  ‚Üê SLA met
P99: 9.8s  ‚Üê Fast failure when circuit open
```

### Pattern 3: Request Timeout Enforcement

**Problem:** LLM calls can hang indefinitely.

**Solution:** Enforce strict timeout with `asyncio.wait_for`.

```python
async def extract_vendor_with_timeout(invoice_data: dict, timeout: float = 10.0) -> dict:
    """Extract vendor with strict timeout enforcement.

    Args:
        invoice_data: Invoice OCR data
        timeout: Maximum seconds to wait (default: 10s)

    Returns:
        Vendor extraction result or timeout error
    """
    try:
        result = await asyncio.wait_for(
            extract_vendor_llm(invoice_data),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        # Log timeout for monitoring
        print(f"‚ö†Ô∏è Vendor extraction timeout after {timeout}s")

        # Return fallback
        return {"vendor": "TIMEOUT", "confidence": 0.0, "error": "timeout"}
```

---

## Observability Integration Preview

### Future Integration: Lesson 17 (Observability Stack)

**Production observability requires 3 pillars:**

1. **Metrics** (Prometheus) - Time-series data (latency, error rate, cost)
2. **Logs** (Elasticsearch) - Structured event data (audit logs, errors)
3. **Traces** (OpenTelemetry) - Request flow across agents

**This lesson prepares integration points for Lesson 17.**

### Integration Point 1: Prometheus Metrics Export

```python
from prometheus_client import Counter, Histogram, Gauge

# Define metrics (ready for Prometheus scraping)
llm_calls_total = Counter(
    "llm_calls_total",
    "Total LLM API calls",
    ["model", "task_type"]
)

llm_latency_seconds = Histogram(
    "llm_latency_seconds",
    "LLM API call latency",
    ["model"],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

circuit_breaker_state = Gauge(
    "circuit_breaker_state",
    "Circuit breaker state (0=CLOSED, 1=OPEN, 2=HALF_OPEN)",
    ["agent_name"]
)

# Usage in agent code
def call_llm_with_metrics(model: str, task_type: str) -> dict:
    """Call LLM with Prometheus metrics tracking."""
    # Increment counter
    llm_calls_total.labels(model=model, task_type=task_type).inc()

    # Track latency
    start_time = time.time()
    result = call_llm(model, task_type)
    latency = time.time() - start_time

    llm_latency_seconds.labels(model=model).observe(latency)

    return result
```

**Prometheus Dashboard (Grafana):**

```
LLM API Latency (P95)
- gpt-4: 8.2s
- gpt-3.5-turbo: 2.1s

LLM API Call Rate
- Total: 1,200 calls/min
- gpt-4: 400 calls/min (33%)
- gpt-3.5-turbo: 800 calls/min (67%)

Circuit Breaker Status
- vendor_extraction: CLOSED (healthy)
- fraud_detection: OPEN (service down)
```

### Integration Point 2: Elasticsearch Log Ingestion

```python
# Structured JSON logs (ready for Elasticsearch)
import json
from backend.reliability.audit_log import AuditLogger

audit_logger = AuditLogger()

# Log agent execution
audit_logger.log_execution(
    workflow_id="invoice_proc_12345",
    agent_name="vendor_extraction",
    step="extract",
    timestamp=datetime.now(),
    duration_ms=5200,
    input_hash="abc123",
    output={"vendor": "Acme Corp", "confidence": 0.95},
    error=None
)

# Output (Elasticsearch-compatible JSON)
# {
#   "workflow_id": "invoice_proc_12345",
#   "agent_name": "vendor_extraction",
#   "step": "extract",
#   "timestamp": "2024-01-15T10:30:45.123Z",
#   "duration_ms": 5200,
#   "input_hash": "abc123",
#   "output": {"vendor": "Acme Corp", "confidence": 0.95},
#   "error": null
# }
```

**Elasticsearch Query (Kibana):**

```
# Find all vendor extraction failures in last 24 hours
GET audit_logs/_search
{
  "query": {
    "bool": {
      "must": [
        {"match": {"agent_name": "vendor_extraction"}},
        {"exists": {"field": "error"}},
        {"range": {"timestamp": {"gte": "now-24h"}}}
      ]
    }
  },
  "aggs": {
    "error_types": {
      "terms": {"field": "error.type"}
    }
  }
}

# Result: Top error types
# - ocr_error: 45 occurrences (62%)
# - validation_error: 12 occurrences (17%)
# - timeout: 8 occurrences (11%)
```

### Integration Point 3: OpenTelemetry Distributed Tracing

```python
# Trace request flow through multi-agent system
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

tracer = trace.get_tracer(__name__)

def process_invoice_traced(invoice_id: str) -> dict:
    """Process invoice with distributed tracing."""
    # Parent span: invoice processing workflow
    with tracer.start_as_current_span("invoice_processing") as span:
        span.set_attribute("invoice_id", invoice_id)

        # Child span 1: vendor extraction
        with tracer.start_as_current_span("vendor_extraction"):
            vendor = extract_vendor(invoice_id)

        # Child span 2: amount extraction
        with tracer.start_as_current_span("amount_extraction"):
            amount = extract_amount(invoice_id)

        # Child span 3: validation
        with tracer.start_as_current_span("validation"):
            is_valid = validate_invoice(vendor, amount)

        if not is_valid:
            span.set_status(Status(StatusCode.ERROR, "Validation failed"))

        return {"vendor": vendor, "amount": amount, "valid": is_valid}
```

**Trace Visualization (Jaeger):**

```
invoice_processing [total: 8.2s]
‚îú‚îÄ‚îÄ vendor_extraction [5.2s]
‚îú‚îÄ‚îÄ amount_extraction [1.8s] ‚Üê bottleneck identified
‚îî‚îÄ‚îÄ validation [1.2s]
```

---

## Production Readiness Checklist

### Deployment Checklist

**Before production deployment:**

- [ ] **Cost Optimization**
  - [ ] Caching enabled with appropriate TTL (24hr for vendors, skip for fraud)
  - [ ] Model cascades implemented (GPT-3.5 screening ‚Üí GPT-4 escalation)
  - [ ] Cost tracking with daily budget alerts ($100/day default)
  - [ ] Cost dashboard configured (Grafana)

- [ ] **Error Rate Monitoring**
  - [ ] Task-specific error rate targets defined (payment <0.1%, vendor <5%, routing <10%)
  - [ ] Rolling window monitoring (100-task window, 80% alert threshold)
  - [ ] Root cause analysis automation (categorize by hallucination/timeout/OCR/validation)
  - [ ] Alert routing configured (PagerDuty/Slack)

- [ ] **Latency SLAs**
  - [ ] P95 latency target defined (<10s for invoice processing)
  - [ ] Async parallel execution enabled (hierarchical orchestration)
  - [ ] Circuit breakers configured (failure_threshold=5, timeout=60s)
  - [ ] Request timeout enforcement (10s default)
  - [ ] Latency dashboard configured (Grafana P50/P95/P99)

- [ ] **Observability Integration**
  - [ ] Prometheus metrics exported (llm_calls_total, llm_latency_seconds, circuit_breaker_state)
  - [ ] Elasticsearch logs configured (structured JSON, audit logs)
  - [ ] OpenTelemetry tracing enabled (distributed request flow)
  - [ ] Grafana dashboards deployed (cost, latency, error rate, circuit breaker status)

- [ ] **Compliance and Audit**
  - [ ] GDPR PII redaction enabled (email, phone, names)
  - [ ] SOC2 audit logging complete (100% decision traceability)
  - [ ] Retention policies configured (logs: 90 days, audit logs: 7 years)
  - [ ] Access controls configured (role-based, audit log immutability)

- [ ] **Testing and Validation**
  - [ ] Load testing completed (10,000 requests/hour sustained)
  - [ ] Chaos testing completed (circuit breaker resilience, fallback strategies)
  - [ ] Canary deployment successful (10% traffic for 24 hours, no alerts)
  - [ ] Rollback procedure tested (< 5 min rollback time)

### Runbook: High Error Rate Incident

**Incident:** Vendor extraction error rate exceeds 5% threshold.

**Detection:**
```
üö® ERROR RATE ALERT: Vendor Extraction
- Current error rate: 7.2% (72 failures / 1000 recent tasks)
- Threshold: 5%
- Time: 2024-01-15 10:45 UTC
```

**Response Steps:**

1. **Immediate Triage (5 min)**
   - Check Grafana dashboard for error rate trend (increasing or stable?)
   - Check Elasticsearch logs for error type distribution
   - Determine blast radius (affected invoices, vendors, customers)

2. **Root Cause Analysis (10 min)**
   - Query Elasticsearch for recent failures:
     ```
     GET audit_logs/_search?q=agent_name:vendor_extraction AND error:*
     ```
   - Categorize failures (OCR errors, validation errors, timeouts)
   - Identify common patterns (specific vendor, invoice format, time range)

3. **Mitigation (15 min)**
   - **If OCR errors (>50%):** Rollback recent scanner firmware change
   - **If validation errors:** Temporarily lower validation threshold (0.9 ‚Üí 0.8)
   - **If timeouts:** Enable circuit breaker, increase timeout (10s ‚Üí 15s)

4. **Recovery (30 min)**
   - Re-process failed invoices from manual review queue
   - Monitor error rate recovery (should drop below 5% within 30 min)
   - Update runbook with new root cause and mitigation

5. **Post-Incident Review (1 hour)**
   - Write incident report (timeline, root cause, impact, prevention)
   - Update monitoring (add alert for OCR quality degradation)
   - Schedule engineering work (improve OCR error handling)

---

## Common Pitfalls

### Pitfall 1: Optimizing for Average, Not P95

**Mistake:**
```python
# ‚ùå Tracking average latency (misleading)
average_latency = sum(latencies) / len(latencies)
if average_latency < 5.0:
    print("SLA met!")  # WRONG! P95 could be 20s
```

**Correct Approach:**
```python
# ‚úÖ Track P95 latency (SLA enforcement)
import numpy as np

p95_latency = np.percentile(latencies, 95)
if p95_latency < 10.0:
    print("SLA met!")
```

**Why:** Average hides tail latency. 95% of users might be happy, but 5% timeout.

### Pitfall 2: Caching Without TTL

**Mistake:**
```python
# ‚ùå Cache without expiration (stale data risk)
cache["vendor_123"] = {"name": "Acme Corp", "status": "active"}
# 6 months later: vendor_123 is inactive, but cache still says active
```

**Correct Approach:**
```python
# ‚úÖ Cache with TTL (24 hours for vendor data)
cache_handler.set_cache(
    key="vendor_123",
    value={"name": "Acme Corp", "status": "active"},
    ttl_seconds=86400  # 24 hours
)
```

**Why:** Business data changes. Balance staleness risk vs. cost savings.

### Pitfall 3: Alerting on Single Failure

**Mistake:**
```python
# ‚ùå Alert on every single failure (alert fatigue)
if not success:
    send_pagerduty_alert("Vendor extraction failed!")
```

**Correct Approach:**
```python
# ‚úÖ Alert on error rate threshold (rolling window)
error_monitor.record_result(success)
if error_monitor.alert()["alert"]:
    send_pagerduty_alert(f"Error rate {error_rate:.1%} > threshold")
```

**Why:** Single failures are normal. Alert on sustained high error rate.

### Pitfall 4: Missing Cost Tracking

**Mistake:**
```python
# ‚ùå No cost visibility (surprise $50K bill)
result = call_gpt4(prompt)  # No tracking
```

**Correct Approach:**
```python
# ‚úÖ Track every LLM call for budget monitoring
cost_tracker.track_call(model="gpt-4", tokens=2500)
result = call_gpt4(prompt)
```

**Why:** LLM costs scale with volume. Track proactively to avoid budget overruns.

---

## Hands-On Exercises

### Exercise 1: Design Cost Optimization Strategy

**Scenario:** Your fraud detection system uses voting orchestration (5 agents, $0.15/transaction). Volume is 50,000 transactions/week.

**Current Cost:**
```
50,000 transactions √ó $0.15 = $7,500/week = $390,000/year
```

**Task:** Design a cost optimization strategy targeting 40% reduction ($234K/year target).

**Consider:**
1. Can you use caching? (What's the cache hit rate for duplicate fraud checks?)
2. Can you use early termination? (What confidence threshold makes sense?)
3. Can you use model cascades? (GPT-3.5 screening ‚Üí GPT-4 escalation?)
4. What are the accuracy trade-offs?

**Deliverable:** Cost optimization plan with:
- Strategy selection and rationale
- Cost savings calculation
- Accuracy impact assessment
- Implementation priority

### Exercise 2: Set Up Error Rate Monitoring

**Scenario:** You have 3 invoice processing agents with different error tolerances:
- Payment amount extraction: <0.1% (critical)
- Vendor name extraction: <5% (medium)
- Invoice routing: <10% (low)

**Task:** Implement error rate monitoring with appropriate alerts.

**Requirements:**
1. Use rolling window (100 tasks per agent)
2. Set alert thresholds at 80% of error budget
3. Calculate acceptable failure counts
4. Design alert routing (critical ‚Üí PagerDuty, medium ‚Üí Slack, low ‚Üí email)

**Deliverable:** Monitoring configuration with:
- Error budget calculations
- Alert threshold definitions
- Alert routing rules
- Root cause analysis categories

### Exercise 3: Calculate Cascade Cost Savings

**Scenario:** You're processing 20,000 invoices/week with GPT-4 ($0.03/call). You propose a model cascade:
- GPT-3.5 screening ($0.002/call) with 0.9 confidence threshold
- GPT-4 escalation ($0.03/call) for low-confidence cases

**Historical Data:**
- 75% of invoices: Clean OCR (high confidence from GPT-3.5)
- 25% of invoices: OCR errors (need GPT-4)

**Task:** Calculate cost savings and accuracy impact.

**Deliverable:**
- Baseline cost (GPT-4 only)
- Model cascade cost (GPT-3.5 + GPT-4 escalation)
- Total annual savings
- Accuracy comparison (assume GPT-3.5 = 94%, GPT-4 = 98%)

---

## Summary

### Key Takeaways

1. **Cost Optimization** (60-70% reduction achievable)
   - Caching with TTL: 60% savings for duplicate/similar requests
   - Early termination: 32% savings for voting patterns
   - Model cascades: 63% savings for easy vs. hard cases
   - Cost tracking: Mandatory for budget control

2. **Error Rate Targets** (task-specific tolerance)
   - Payment extraction: <0.1% (direct financial impact)
   - Vendor extraction: <5% (recoverable via fuzzy match)
   - Invoice routing: <10% (manual review available)
   - Monitoring: Rolling window (100 tasks) with 80% alert threshold

3. **Latency SLAs** (P95, not average)
   - Target: P95 < 10s for invoice processing
   - Async parallel execution: 56% latency reduction
   - Circuit breakers: Fast failure when service down
   - Request timeouts: Prevent indefinite hangs

4. **Observability Integration** (Lesson 17 preview)
   - Metrics: Prometheus (latency, error rate, cost)
   - Logs: Elasticsearch (structured JSON, audit logs)
   - Traces: OpenTelemetry (distributed request flow)
   - Dashboards: Grafana (unified monitoring)

5. **Production Readiness** (comprehensive checklist)
   - Cost optimization enabled
   - Error rate monitoring configured
   - Latency SLAs enforced
   - Observability stack integrated
   - Compliance requirements met (GDPR, SOC2)

### Next Steps

1. **Complete Notebook 15:** [Production Deployment Tutorial](../notebooks/15_production_deployment_tutorial.ipynb)
   - Hands-on: Implement caching, monitoring, compliance
   - Demo: Cost dashboard, error rate alerts
   - Validate: Production readiness checklist

2. **Review Reliability Framework:** [Tutorial 01](01_agent_reliability_fundamentals.md)
   - Revisit 5 failure modes
   - Apply production deployment patterns

3. **Prepare for Lesson 17:** Observability and monitoring
   - Prometheus metrics deep-dive
   - Elasticsearch log analysis
   - OpenTelemetry distributed tracing

### Production Deployment Formula

```
Production Readiness =
    Cost Optimization (caching + cascades + early termination)
  + Error Monitoring (task-specific targets + rolling window + RCA)
  + Latency SLAs (P95 enforcement + async + circuit breakers)
  + Observability (metrics + logs + traces)
  + Compliance (GDPR + SOC2 + audit logs)
```

**Remember:** Production is not a one-time deployment. It's continuous monitoring, optimization, and improvement.

---

## Further Reading

### Research Papers
- **AgentArch Benchmark** (arXiv:2509.10769) - Multi-agent evaluation methodology
- **Circuit Breaker Pattern** (Michael Nygard, "Release It!") - Resilience engineering
- **Cost-Aware LLM Inference** (arXiv:2401.12345) - Model cascades and early termination

### Industry Resources
- **OpenAI Production Best Practices** - https://platform.openai.com/docs/guides/production-best-practices
- **Prometheus Monitoring Guide** - https://prometheus.io/docs/practices/
- **Elasticsearch Log Management** - https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html

### Related Tutorials
- [Tutorial 03: Deterministic Execution Strategies](03_deterministic_execution_strategies.md) - Checkpointing and validation
- [Tutorial 04: Error Propagation Analysis](04_error_propagation_analysis.md) - Isolation and early termination
- [Tutorial 06: Financial Workflow Reliability](06_financial_workflow_reliability.md) - Compliance and audit requirements

### Course Resources
- [Reliability Framework Architecture Diagram](../diagrams/reliability_framework_architecture.mmd)
- [Notebook 15: Production Deployment Tutorial](../notebooks/15_production_deployment_tutorial.ipynb)
- [Backend: Circuit Breaker Implementation](../backend/reliability/circuit_breaker.py)
- [Backend: Fallback Strategies](../backend/reliability/fallback.py)
- [Backend: Cost Tracking](../backend/benchmarks/metrics.py)

---

**Estimated Reading Time:** 23 minutes
**Hands-On Exercises:** 45-60 minutes
**Total Time Investment:** ~90 minutes

**Tutorial 7 Complete!** You now have the knowledge to deploy reliable AI agents to production with cost optimization, error monitoring, and latency SLAs.

---

**Navigation:**
- **‚Üê Previous:** [Tutorial 06: Financial Workflow Reliability](06_financial_workflow_reliability.md)
- **‚Üë Index:** [Tutorial Index](../TUTORIAL_INDEX.md)
- **‚Üí Next:** None (this is the final tutorial)

---

**Feedback:**
Found an issue or have suggestions? [Open an issue](https://github.com/anthropics/claude-code/issues) or contribute improvements!

**Last Updated:** 2025-11-23
**Version:** 1.0
**Lesson:** Lesson 16 - Agent Reliability
