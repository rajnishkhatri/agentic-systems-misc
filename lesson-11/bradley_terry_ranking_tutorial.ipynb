{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Bradley-Terry Ranking Tutorial: Probabilistic Model Comparison\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- âœ… Understand the Bradley-Terry model and its probabilistic foundation\n",
    "- âœ… Learn maximum likelihood estimation (MLE) for skill parameters\n",
    "- âœ… Fit Bradley-Terry model to pairwise comparison data\n",
    "- âœ… Visualize skill estimates with uncertainty (standard errors)\n",
    "- âœ… Compare Bradley-Terry vs Elo rankings\n",
    "- âœ… Decide when to use Bradley-Terry vs Elo\n",
    "\n",
    "## Execution Details\n",
    "\n",
    "- **Execution Time:** <4 minutes\n",
    "- **Cost:** $0 (simulation-based, no API calls)\n",
    "- **Prerequisites:** Understanding of comparative evaluation and Elo ranking\n",
    "\n",
    "## Background\n",
    "\n",
    "The **Bradley-Terry model** (1952) is a probabilistic approach to ranking:\n",
    "- Each model has a latent \"skill\" parameter Î¸\n",
    "- Win probability follows logistic function\n",
    "- Uses **all comparison data simultaneously** (batch estimation)\n",
    "- Provides **uncertainty estimates** (standard errors)\n",
    "\n",
    "**Differences from Elo:**\n",
    "- Elo: Sequential updates (online), no uncertainty\n",
    "- Bradley-Terry: Batch estimation (offline), with uncertainty\n",
    "\n",
    "**Real-world uses:**\n",
    "- Sports rankings (NFL, NBA) - one-time seasonal rankings\n",
    "- Product preference studies (A/B/C testing)\n",
    "- LLM evaluation (AlpacaEval uses Bradley-Terry for final rankings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"backend\"))\n",
    "\n",
    "from comparative_evaluation import BradleyTerryRanking\n",
    "\n",
    "# Load pairwise comparisons dataset\n",
    "data_path = Path(\"data/pairwise_comparisons.json\")\n",
    "with open(data_path) as f:\n",
    "    comparisons_raw = json.load(f)\n",
    "\n",
    "# Convert to Bradley-Terry format: list of (model_a, model_b, result) tuples\n",
    "comparisons = []\n",
    "for comp in comparisons_raw:\n",
    "    model_a = \"Response_A\"\n",
    "    model_b = \"Response_B\"\n",
    "    # result: 1 if A wins, 0 if B wins, 0.5 if tie\n",
    "    if comp['winner'] == 'A':\n",
    "        result = 1\n",
    "    elif comp['winner'] == 'B':\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 0.5\n",
    "    \n",
    "    comparisons.append({\n",
    "        'model_a': model_a,\n",
    "        'model_b': model_b,\n",
    "        'winner': result\n",
    "    })\n",
    "\n",
    "print(f\"âœ… Loaded {len(comparisons)} pairwise comparisons\")\n",
    "print(\"âœ… BradleyTerryRanking class imported successfully\")\n",
    "print(\"\\nFirst comparison:\")\n",
    "print(f\"Model A: {comparisons[0]['model_a']}\")\n",
    "print(f\"Model B: {comparisons[0]['model_b']}\")\n",
    "print(f\"Result: {comparisons[0]['winner']} (1=A wins, 0=B wins, 0.5=tie)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bt-model",
   "metadata": {},
   "source": [
    "## Bradley-Terry Model Explanation\n",
    "\n",
    "### The Probabilistic Model\n",
    "\n",
    "**Core idea:** Each model i has a latent skill parameter Î¸áµ¢\n",
    "\n",
    "**Win probability:**\n",
    "```\n",
    "P(model i beats model j) = exp(Î¸áµ¢) / (exp(Î¸áµ¢) + exp(Î¸â±¼))\n",
    "                         = 1 / (1 + exp(Î¸â±¼ - Î¸áµ¢))\n",
    "```\n",
    "\n",
    "This is a **logistic function** - the same used in logistic regression!\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Likelihood function:**\n",
    "```\n",
    "L(Î¸) = âˆ P(observed comparisons | Î¸)\n",
    "```\n",
    "\n",
    "**Goal:** Find Î¸ that maximizes L(Î¸)\n",
    "\n",
    "**Method:** \n",
    "- Take log: `log L(Î¸)` (easier to optimize)\n",
    "- Use iterative solver (Newton-Raphson, IRLS)\n",
    "- Returns Î¸ and standard errors\n",
    "\n",
    "### Intuition with Examples\n",
    "\n",
    "**Example 1: Equal skill**\n",
    "- Î¸_A = 0.0, Î¸_B = 0.0\n",
    "- P(A beats B) = 1 / (1 + exp(0 - 0)) = 0.5 (50%)\n",
    "\n",
    "**Example 2: A slightly better**\n",
    "- Î¸_A = 0.5, Î¸_B = 0.0\n",
    "- P(A beats B) = 1 / (1 + exp(0 - 0.5)) = 0.62 (62%)\n",
    "\n",
    "**Example 3: A much better**\n",
    "- Î¸_A = 2.0, Î¸_B = 0.0\n",
    "- P(A beats B) = 1 / (1 + exp(0 - 2)) = 0.88 (88%)\n",
    "\n",
    "**Key insight:** Î¸ difference of 0.5 â‰ˆ 62% win rate, Î¸ difference of 2.0 â‰ˆ 88% win rate\n",
    "\n",
    "### Bradley-Terry vs Elo\n",
    "\n",
    "| Aspect | Bradley-Terry | Elo |\n",
    "|--------|---------------|-----|\n",
    "| **Estimation** | Batch (all data at once) | Online (sequential) |\n",
    "| **Uncertainty** | Yes (standard errors) | No |\n",
    "| **Order dependence** | No (order-independent) | Yes |\n",
    "| **New models** | Requires refitting | Easy (assign initial rating) |\n",
    "| **Complexity** | Moderate (MLE solver) | Simple (formula) |\n",
    "| **Best for** | Static analysis | Dynamic leaderboard |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Demonstrate Bradley-Terry formula with examples\n",
    "def bt_win_probability(theta_a, theta_b):\n",
    "    \"\"\"Calculate P(A beats B) using Bradley-Terry formula.\"\"\"\n",
    "    return 1 / (1 + np.exp(theta_b - theta_a))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Bradley-Terry Win Probability Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "examples = [\n",
    "    (0.0, 0.0, \"Equal skill\"),\n",
    "    (0.5, 0.0, \"A slightly better\"),\n",
    "    (1.0, 0.0, \"A moderately better\"),\n",
    "    (2.0, 0.0, \"A much better\"),\n",
    "    (0.0, -0.5, \"B slightly worse (same as A=0.5)\"),\n",
    "]\n",
    "\n",
    "for theta_a, theta_b, description in examples:\n",
    "    prob = bt_win_probability(theta_a, theta_b)\n",
    "    print(f\"\\nÎ¸_A={theta_a:4.1f}, Î¸_B={theta_b:5.1f} ({description})\")\n",
    "    print(f\"  â†’ P(A beats B) = {prob:.1%}\")\n",
    "\n",
    "# Visualize relationship\n",
    "theta_diff = np.linspace(-3, 3, 100)\n",
    "win_prob = 1 / (1 + np.exp(-theta_diff))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta_diff, win_prob, linewidth=3, color='steelblue')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Even match')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Skill Difference (Î¸_A - Î¸_B)', fontsize=12)\n",
    "plt.ylabel('P(A beats B)', fontsize=12)\n",
    "plt.title('Bradley-Terry Win Probability Curve', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Î¸_A - Î¸_B = 1\\nâ‰ˆ 73% win rate', xy=(1, 0.73), xytext=(1.5, 0.85),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "plt.annotate('Î¸_A - Î¸_B = -1\\nâ‰ˆ 27% win rate', xy=(-1, 0.27), xytext=(-2, 0.15),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/bradley_terry_curve.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n\\nâœ… Win probability curve saved to results/bradley_terry_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Fit Bradley-Terry model to comparison data\n",
    "print(\"Fitting Bradley-Terry model to pairwise comparisons...\\n\")\n",
    "\n",
    "# Initialize and fit model\n",
    "bt = BradleyTerryRanking()\n",
    "bt.fit(comparisons)\n",
    "\n",
    "# Get rankings\n",
    "rankings = bt.get_rankings()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Bradley-Terry Model Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Converged: {bt.converged}\")\n",
    "print(f\"Number of iterations: {bt.n_iterations}\")\n",
    "print(f\"Final log-likelihood: {bt.log_likelihood:.2f}\")\n",
    "print(\"\\nSkill Parameters (Î¸):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'Î¸ (skill)':<12} {'Std Error':<12} {'95% CI'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model, skill, std_err in rankings:\n",
    "    ci_lower = skill - 1.96 * std_err\n",
    "    ci_upper = skill + 1.96 * std_err\n",
    "    print(f\"{model:<20} {skill:>8.3f}     {std_err:>8.3f}     [{ci_lower:6.3f}, {ci_upper:6.3f}]\")\n",
    "\n",
    "print(\"\\nğŸ“Š Interpretation:\")\n",
    "print(\"- Î¸ > 0: Better than average\")\n",
    "print(\"- Î¸ < 0: Worse than average\")\n",
    "print(\"- Î¸ = 0: Average skill (reference point)\")\n",
    "print(\"- Smaller std error = more confident estimate\")\n",
    "\n",
    "print(\"\\nâœ… Model fitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-skills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize skill estimates with uncertainty\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Skill parameters with error bars\n",
    "ax1 = axes[0]\n",
    "models = [m for m, _, _ in rankings]\n",
    "skills = [s for _, s, _ in rankings]\n",
    "std_errs = [e for _, _, e in rankings]\n",
    "errors = [1.96 * e for e in std_errs]  # 95% CI\n",
    "\n",
    "y_pos = np.arange(len(models))\n",
    "colors = ['#2ecc71' if s > 0 else '#e74c3c' for s in skills]\n",
    "\n",
    "ax1.barh(y_pos, skills, xerr=errors, color=colors, alpha=0.7,\n",
    "         ecolor='black', capsize=5, error_kw={'linewidth': 2}, edgecolor='black')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(models)\n",
    "ax1.set_xlabel('Skill Parameter Î¸ (with 95% CI)', fontsize=11)\n",
    "ax1.set_title('Bradley-Terry Skill Estimates', fontsize=13, fontweight='bold')\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5, linewidth=2, label='Average skill')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add skill values on bars\n",
    "for i, (skill, error) in enumerate(zip(skills, errors)):\n",
    "    x_pos = skill + error + 0.05 if skill > 0 else skill - error - 0.05\n",
    "    ax1.text(x_pos, i, f'{skill:.3f}Â±{error:.3f}', va='center', \n",
    "             fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Win probabilities vs average opponent\n",
    "ax2 = axes[1]\n",
    "# Calculate win probability vs average opponent (Î¸=0)\n",
    "win_probs = [1 / (1 + np.exp(-s)) for s in skills]\n",
    "\n",
    "bars = ax2.barh(y_pos, win_probs, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(models)\n",
    "ax2.set_xlabel('Win Probability vs Average Opponent', fontsize=11)\n",
    "ax2.set_title('Expected Win Rates', fontsize=13, fontweight='bold')\n",
    "ax2.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, linewidth=2, label='50%')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add percentages on bars\n",
    "for i, (bar, prob) in enumerate(zip(bars, win_probs)):\n",
    "    ax2.text(prob + 0.02, i, f'{prob:.1%}', va='center', \n",
    "             fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/bradley_terry_skills.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nâœ… Skill visualization saved to results/bradley_terry_skills.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-elo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare Bradley-Terry vs Elo rankings\n",
    "from comparative_evaluation import EloRanking\n",
    "\n",
    "# Fit Elo model\n",
    "elo = EloRanking(initial_rating=1500, k_factor=32)\n",
    "for comp in comparisons:\n",
    "    elo.record_match(comp['model_a'], comp['model_b'], comp['winner'])\n",
    "\n",
    "elo_rankings = elo.get_leaderboard()\n",
    "\n",
    "# Compare rankings\n",
    "print(\"=\" * 80)\n",
    "print(\"Bradley-Terry vs Elo Rankings Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'BT Î¸':<12} {'BT Rank':<10} {'Elo Rating':<12} {'Elo Rank'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create rank mappings\n",
    "bt_ranks = {model: i+1 for i, (model, _, _) in enumerate(rankings)}\n",
    "elo_ranks = {model: i+1 for i, (model, _) in enumerate(elo_rankings)}\n",
    "bt_skills = {model: skill for model, skill, _ in rankings}\n",
    "elo_ratings = {model: rating for model, rating in elo_rankings}\n",
    "\n",
    "all_models = sorted(set(bt_ranks.keys()) | set(elo_ranks.keys()))\n",
    "\n",
    "for model in all_models:\n",
    "    bt_skill = bt_skills.get(model, 0.0)\n",
    "    bt_rank = bt_ranks.get(model, '-')\n",
    "    elo_rating = elo_ratings.get(model, 1500)\n",
    "    elo_rank = elo_ranks.get(model, '-')\n",
    "    \n",
    "    # Highlight rank differences\n",
    "    if bt_rank != '-' and elo_rank != '-':\n",
    "        rank_diff = abs(bt_rank - elo_rank)\n",
    "        marker = \" âš ï¸\" if rank_diff > 0 else \" âœ…\"\n",
    "    else:\n",
    "        marker = \"\"\n",
    "    \n",
    "    print(f\"{model:<20} {bt_skill:>8.3f}     {str(bt_rank):<9} {elo_rating:>9.1f}     {str(elo_rank):<8}{marker}\")\n",
    "\n",
    "# Calculate rank correlation (Spearman)\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "bt_rank_list = [bt_ranks[m] for m in all_models if m in bt_ranks]\n",
    "elo_rank_list = [elo_ranks[m] for m in all_models if m in elo_ranks]\n",
    "\n",
    "if len(bt_rank_list) > 1:\n",
    "    correlation, p_value = spearmanr(bt_rank_list, elo_rank_list)\n",
    "    print(f\"\\nğŸ“Š Spearman Rank Correlation: {correlation:.3f} (p={p_value:.4f})\")\n",
    "    \n",
    "    if correlation > 0.9:\n",
    "        print(\"âœ… Very high agreement between Bradley-Terry and Elo\")\n",
    "    elif correlation > 0.7:\n",
    "        print(\"âš ï¸  Moderate agreement - some rank differences\")\n",
    "    else:\n",
    "        print(\"âŒ Low agreement - significant ranking differences\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Differences:\")\n",
    "print(\"- Bradley-Terry: Order-independent, provides uncertainty\")\n",
    "print(\"- Elo: Order-dependent (early matches have more impact)\")\n",
    "print(\"- High correlation suggests both methods agree on relative skill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "when-to-use",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Decision guide - when to use Bradley-Terry\n",
    "print(\"=\" * 70)\n",
    "print(\"When to Use Bradley-Terry vs Elo: Decision Guide\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "decision_tree = \"\"\"\n",
    "START: Do you need to rank models from pairwise comparisons?\n",
    "â”‚\n",
    "â”œâ”€ YES â†’ Are you building a LIVE leaderboard with continuous updates?\n",
    "â”‚        â”‚\n",
    "â”‚        â”œâ”€ YES â†’ Do new models get added frequently?\n",
    "â”‚        â”‚        â”‚\n",
    "â”‚        â”‚        â”œâ”€ YES â†’ Use **Elo**\n",
    "â”‚        â”‚        â”‚        (Easy to add new models, online updates)\n",
    "â”‚        â”‚        â”‚\n",
    "â”‚        â”‚        â””â”€ NO â†’ Do you need uncertainty estimates?\n",
    "â”‚        â”‚                 â”‚\n",
    "â”‚        â”‚                 â”œâ”€ YES â†’ Use **Bradley-Terry** (batch refit periodically)\n",
    "â”‚        â”‚                 â”‚\n",
    "â”‚        â”‚                 â””â”€ NO â†’ Use **Elo** (simpler)\n",
    "â”‚        â”‚\n",
    "â”‚        â””â”€ NO â†’ Is this a ONE-TIME ranking of a fixed set of models?\n",
    "â”‚                 â”‚\n",
    "â”‚                 â”œâ”€ YES â†’ Do you need uncertainty / statistical inference?\n",
    "â”‚                 â”‚        â”‚\n",
    "â”‚                 â”‚        â”œâ”€ YES â†’ Use **Bradley-Terry**\n",
    "â”‚                 â”‚        â”‚        (Uncertainty, order-independent)\n",
    "â”‚                 â”‚        â”‚\n",
    "â”‚                 â”‚        â””â”€ NO â†’ Either works, **Elo** is simpler\n",
    "â”‚                 â”‚\n",
    "â”‚                 â””â”€ MAYBE â†’ Use **Elo** for exploration, **Bradley-Terry** for final report\n",
    "â”‚\n",
    "â””â”€ NO â†’ Use different evaluation method (pointwise scoring, A/B testing, etc.)\n",
    "\"\"\"\n",
    "\n",
    "print(decision_tree)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Summary Table\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = [\n",
    "    (\"Criterion\", \"Bradley-Terry\", \"Elo\"),\n",
    "    (\"-\" * 30, \"-\" * 20, \"-\" * 15),\n",
    "    (\"Update type\", \"Batch (offline)\", \"Online (sequential)\"),\n",
    "    (\"Uncertainty estimates\", \"Yes (std errors)\", \"No\"),\n",
    "    (\"Order dependence\", \"No\", \"Yes\"),\n",
    "    (\"Adding new models\", \"Requires refit\", \"Easy\"),\n",
    "    (\"Computational cost\", \"Moderate (MLE)\", \"Low (formula)\"),\n",
    "    (\"Statistical rigor\", \"High (MLE)\", \"Heuristic\"),\n",
    "    (\"Best use case\", \"Static analysis\", \"Live leaderboard\"),\n",
    "    (\"Example\", \"AlpacaEval final\", \"Chatbot Arena\"),\n",
    "]\n",
    "\n",
    "for row in comparison:\n",
    "    print(f\"{row[0]:<30} {row[1]:<20} {row[2]:<15}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Practical Recommendation:\")\n",
    "print(\"   - Use **Elo** for exploration and live leaderboards\")\n",
    "print(\"   - Use **Bradley-Terry** for final analysis and publication\")\n",
    "print(\"   - If they disagree significantly (correlation < 0.7):\")\n",
    "print(\"     â†’ Investigate judge inconsistency or transitivity violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Bradley-Terry Model**:\n",
    "   - Probabilistic foundation: `P(A beats B) = exp(Î¸_A) / (exp(Î¸_A) + exp(Î¸_B))`\n",
    "   - Maximum likelihood estimation for skill parameters\n",
    "   - Provides uncertainty via standard errors\n",
    "\n",
    "2. **Batch vs Online**:\n",
    "   - Bradley-Terry: Uses all data simultaneously (order-independent)\n",
    "   - Elo: Sequential updates (order-dependent)\n",
    "   - Both converge to similar rankings with enough data\n",
    "\n",
    "3. **Uncertainty Quantification**:\n",
    "   - Standard errors from MLE\n",
    "   - 95% confidence intervals for skill estimates\n",
    "   - Enables statistical hypothesis testing\n",
    "\n",
    "4. **When to Use Each**:\n",
    "   - Bradley-Terry: Static analysis, need uncertainty, publication\n",
    "   - Elo: Live leaderboard, frequent model additions, simplicity\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "**Bradley-Terry strengths:**\n",
    "- âœ… Order-independent (fair to all models)\n",
    "- âœ… Provides uncertainty (can test \"is A significantly better than B?\")\n",
    "- âœ… Statistically principled (MLE)\n",
    "- âœ… Works with incomplete comparison data (not all pairs compared)\n",
    "\n",
    "**Bradley-Terry limitations:**\n",
    "- âŒ Requires batch refitting for new models\n",
    "- âŒ More complex to implement\n",
    "- âŒ Assumes transitivity (fit degrades with many violations)\n",
    "- âŒ Can be slow for large datasets (O(nÂ²) comparisons)\n",
    "\n",
    "**Elo strengths:**\n",
    "- âœ… Simple and fast\n",
    "- âœ… Easy to add new models\n",
    "- âœ… Well-understood (70+ years of use)\n",
    "- âœ… Robust to transitivity violations\n",
    "\n",
    "**Elo limitations:**\n",
    "- âŒ Order-dependent (early matches have outsized impact)\n",
    "- âŒ No uncertainty estimates\n",
    "- âŒ Requires K-factor tuning\n",
    "- âŒ Cold start problem for new models\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Bradley-Terry:**\n",
    "- AlpacaEval 2.0 (final win rates)\n",
    "- MT-Bench (model comparison)\n",
    "- Academic research (statistical testing)\n",
    "\n",
    "**Elo:**\n",
    "- Chatbot Arena (live leaderboard)\n",
    "- Game matchmaking (LoL, Overwatch)\n",
    "- Chess ratings (FIDE)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- ğŸ§ª [A/B Testing vs Comparative Eval](ab_testing_vs_comparative_eval.ipynb) - Sample size comparison\n",
    "- ğŸ“– [Comparative Evaluation Guide](comparative_evaluation_guide.md) - Full methodology\n",
    "- ğŸ¯ [Elo Ranking Tutorial](elo_ranking_tutorial.ipynb) - Dynamic leaderboards\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Bradley-Terry Model (Wikipedia)](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)\n",
    "- [AlpacaEval 2.0 Paper](https://arxiv.org/abs/2404.04475) - Uses Bradley-Terry\n",
    "- [Backend Implementation](../backend/comparative_evaluation.py) - BradleyTerryRanking source\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations!** You now understand probabilistic ranking with Bradley-Terry and can choose the right algorithm for your use case.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}