{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Elo Ranking Tutorial: Dynamic Leaderboards for LLM Evaluation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Understand the Elo rating formula and its intuition\n",
    "- ‚úÖ Implement Elo ranking from scratch\n",
    "- ‚úÖ Record pairwise matches and update rankings dynamically\n",
    "- ‚úÖ Visualize leaderboard evolution over time\n",
    "- ‚úÖ Calculate confidence intervals for rankings\n",
    "- ‚úÖ Detect transitivity violations in comparisons\n",
    "\n",
    "## Execution Details\n",
    "\n",
    "- **Execution Time:** <5 minutes\n",
    "- **Cost:** $0 (simulation-based, no API calls)\n",
    "- **Prerequisites:** Understanding of comparative evaluation (see `comparative_evaluation_guide.md`)\n",
    "\n",
    "## Background\n",
    "\n",
    "The **Elo rating system** was developed by Arpad Elo for chess in the 1960s. It's now used for:\n",
    "- Chess rankings (FIDE)\n",
    "- Game matchmaking (League of Legends, Overwatch)\n",
    "- **LLM evaluation** (Chatbot Arena, AlpacaEval 2.0)\n",
    "\n",
    "**Why Elo for LLMs?**\n",
    "- Simple and interpretable\n",
    "- Online updates (process comparisons one at a time)\n",
    "- Self-correcting (ratings converge to true skill)\n",
    "- Handles new models easily (just assign initial rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 pairwise comparisons\n",
      "‚úÖ EloRanking class imported successfully\n",
      "\n",
      "First comparison example:\n",
      "Query: How do I make gluten-free pasta from scratch?...\n",
      "Winner: A\n",
      "Dimension: helpfulness\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and imports\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"backend\"))\n",
    "\n",
    "from comparative_evaluation import EloRanking\n",
    "\n",
    "# Load pairwise comparisons dataset\n",
    "data_path = Path(\"data/pairwise_comparisons.json\")\n",
    "with open(data_path) as f:\n",
    "    comparisons = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(comparisons)} pairwise comparisons\")\n",
    "print(\"‚úÖ EloRanking class imported successfully\")\n",
    "print(\"\\nFirst comparison example:\")\n",
    "print(f\"Query: {comparisons[0]['query'][:80]}...\")\n",
    "print(f\"Winner: {comparisons[0]['winner']}\")\n",
    "print(f\"Dimension: {comparisons[0]['dimension']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elo-formula",
   "metadata": {},
   "source": [
    "## Elo Formula and Intuition\n",
    "\n",
    "### The Math\n",
    "\n",
    "**Step 1: Calculate expected score (win probability)**\n",
    "```\n",
    "E_A = 1 / (1 + 10^((R_B - R_A) / 400))\n",
    "```\n",
    "- `R_A`, `R_B`: Current ratings for models A and B\n",
    "- `E_A`: Expected probability that A beats B\n",
    "- The 400 constant comes from chess (10% rating difference ‚âà 64% win probability)\n",
    "\n",
    "**Step 2: Update rating based on actual outcome**\n",
    "```\n",
    "R_A' = R_A + K * (S_A - E_A)\n",
    "```\n",
    "- `S_A`: Actual score (1 if A wins, 0 if B wins, 0.5 if tie)\n",
    "- `K`: Learning rate / K-factor (how much ratings can change)\n",
    "- `(S_A - E_A)`: Prediction error (surprise!)\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "**Example 1: Evenly matched models**\n",
    "- Model A: 1500 Elo, Model B: 1500 Elo\n",
    "- Expected: `E_A = 0.5` (50% chance A wins)\n",
    "- A wins: `R_A' = 1500 + 32 * (1 - 0.5) = 1516`\n",
    "- B loses: `R_B' = 1500 + 32 * (0 - 0.5) = 1484`\n",
    "- **Rating change: ¬±16 points**\n",
    "\n",
    "**Example 2: Underdog wins (upset!)**\n",
    "- Model A: 1300 Elo, Model B: 1600 Elo\n",
    "- Expected: `E_A = 0.09` (only 9% chance A wins)\n",
    "- A wins: `R_A' = 1300 + 32 * (1 - 0.09) = 1329`\n",
    "- B loses: `R_B' = 1600 + 32 * (0 - 0.91) = 1571`\n",
    "- **Rating change: +29 for A, -29 for B (big surprise!)**\n",
    "\n",
    "**Example 3: Favorite wins (expected)**\n",
    "- Model A: 1600 Elo, Model B: 1300 Elo\n",
    "- Expected: `E_A = 0.91` (91% chance A wins)\n",
    "- A wins: `R_A' = 1600 + 32 * (1 - 0.91) = 1603`\n",
    "- B loses: `R_B' = 1300 + 32 * (0 - 0.09) = 1297`\n",
    "- **Rating change: ¬±3 points (no surprise)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Implement and demonstrate Elo ranking\n",
    "# Initialize Elo ranking system\n",
    "elo = EloRanking(initial_rating=1500, k_factor=32)\n",
    "\n",
    "# Demonstrate the three examples from above\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Evenly matched models\")\n",
    "print(\"=\" * 60)\n",
    "elo_demo1 = EloRanking(initial_rating=1500, k_factor=32)\n",
    "elo_demo1.ratings = {\"Model_A\": 1500, \"Model_B\": 1500}\n",
    "print(f\"Before: A={elo_demo1.ratings['Model_A']}, B={elo_demo1.ratings['Model_B']}\")\n",
    "elo_demo1.record_match(\"Model_A\", \"Model_B\", result=1.0)\n",
    "print(f\"After A wins: A={elo_demo1.ratings['Model_A']:.0f}, B={elo_demo1.ratings['Model_B']:.0f}\")\n",
    "print(f\"Rating change: ¬±{abs(elo_demo1.ratings['Model_A'] - 1500):.0f} points\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Underdog wins (upset!)\")\n",
    "print(\"=\" * 60)\n",
    "elo_demo2 = EloRanking(initial_rating=1500, k_factor=32)\n",
    "elo_demo2.ratings = {\"Model_A\": 1300, \"Model_B\": 1600}\n",
    "expected_a = elo_demo2._calculate_expected_score(1300, 1600)\n",
    "print(f\"Before: A={elo_demo2.ratings['Model_A']}, B={elo_demo2.ratings['Model_B']}\")\n",
    "print(f\"Expected win probability for A: {expected_a:.1%}\")\n",
    "elo_demo2.record_match(\"Model_A\", \"Model_B\", result=1.0)\n",
    "print(f\"After A wins: A={elo_demo2.ratings['Model_A']:.0f}, B={elo_demo2.ratings['Model_B']:.0f}\")\n",
    "print(f\"Rating change: +{elo_demo2.ratings['Model_A'] - 1300:.0f} for A, {elo_demo2.ratings['Model_B'] - 1600:.0f} for B\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Favorite wins (expected)\")\n",
    "print(\"=\" * 60)\n",
    "elo_demo3 = EloRanking(initial_rating=1500, k_factor=32)\n",
    "elo_demo3.ratings = {\"Model_A\": 1600, \"Model_B\": 1300}\n",
    "expected_a = elo_demo3._calculate_expected_score(1600, 1300)\n",
    "print(f\"Before: A={elo_demo3.ratings['Model_A']}, B={elo_demo3.ratings['Model_B']}\")\n",
    "print(f\"Expected win probability for A: {expected_a:.1%}\")\n",
    "elo_demo3.record_match(\"Model_A\", \"Model_B\", result=1.0)\n",
    "print(f\"After A wins: A={elo_demo3.ratings['Model_A']:.0f}, B={elo_demo3.ratings['Model_B']:.0f}\")\n",
    "print(f\"Rating change: +{elo_demo3.ratings['Model_A'] - 1600:.0f} for A, {elo_demo3.ratings['Model_B'] - 1300:.0f} for B\")\n",
    "\n",
    "print(\"\\n‚úÖ Elo formula demonstrated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "record-matches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Record all pairwise matches from dataset\n",
    "# Reset Elo ranking system\n",
    "elo = EloRanking(initial_rating=1500, k_factor=32)\n",
    "\n",
    "# Track rating evolution\n",
    "rating_history = defaultdict(list)\n",
    "match_count = defaultdict(int)\n",
    "\n",
    "# Process each comparison\n",
    "for i, comp in enumerate(comparisons):\n",
    "    # Extract winner and loser from comparison\n",
    "    # In our dataset, responses are labeled as response_a and response_b\n",
    "    # We'll treat them as different \"models\" for this tutorial\n",
    "    model_a = \"Response_A\"\n",
    "    model_b = \"Response_B\"\n",
    "    \n",
    "    # Determine result (1.0 if A wins, 0.0 if B wins, 0.5 if tie)\n",
    "    if comp['winner'] == 'A':\n",
    "        result = 1.0\n",
    "    elif comp['winner'] == 'B':\n",
    "        result = 0.0\n",
    "    else:\n",
    "        result = 0.5  # Tie\n",
    "    \n",
    "    # Record match\n",
    "    elo.record_match(model_a, model_b, result)\n",
    "    \n",
    "    # Track history\n",
    "    for model in elo.ratings:\n",
    "        rating_history[model].append(elo.ratings[model])\n",
    "        match_count[model] += 1\n",
    "\n",
    "# Get final leaderboard\n",
    "leaderboard = elo.get_leaderboard()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Final Elo Leaderboard\")\n",
    "print(\"=\" * 60)\n",
    "for i, (model, rating) in enumerate(leaderboard, 1):\n",
    "    print(f\"{i}. {model:20s} {rating:7.1f} Elo ({match_count[model]} matches)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(comparisons)} pairwise comparisons\")\n",
    "print(f\"‚úÖ {len(leaderboard)} models ranked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize leaderboard evolution over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Rating evolution over matches\n",
    "ax1 = axes[0]\n",
    "for model, history in rating_history.items():\n",
    "    ax1.plot(history, label=model, linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
    "\n",
    "ax1.axhline(y=1500, color='gray', linestyle='--', alpha=0.5, label='Initial rating')\n",
    "ax1.set_xlabel('Match Number', fontsize=11)\n",
    "ax1.set_ylabel('Elo Rating', fontsize=11)\n",
    "ax1.set_title('Elo Rating Evolution Over Time', fontsize=13, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final ratings bar chart\n",
    "ax2 = axes[1]\n",
    "models = [m for m, _ in leaderboard]\n",
    "ratings = [r for _, r in leaderboard]\n",
    "colors = ['#2ecc71' if r > 1500 else '#e74c3c' for r in ratings]\n",
    "\n",
    "bars = ax2.barh(models, ratings, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=1500, color='gray', linestyle='--', alpha=0.5, label='Initial rating')\n",
    "ax2.set_xlabel('Elo Rating', fontsize=11)\n",
    "ax2.set_title('Final Elo Rankings', fontsize=13, fontweight='bold')\n",
    "ax2.legend(loc='best', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add rating values on bars\n",
    "for i, (bar, rating) in enumerate(zip(bars, ratings)):\n",
    "    ax2.text(rating + 5, i, f'{rating:.0f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/elo_evolution.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Visualization saved to results/elo_evolution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence-intervals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Calculate confidence intervals for rankings\n",
    "# Elo doesn't natively provide uncertainty, so we'll estimate it using rating volatility\n",
    "# Method: Calculate standard deviation of rating over last N matches\n",
    "\n",
    "def calculate_rating_uncertainty(rating_history, window=10):\n",
    "    \"\"\"Estimate rating uncertainty from recent volatility.\"\"\"\n",
    "    if len(rating_history) < window:\n",
    "        window = len(rating_history)\n",
    "    recent = rating_history[-window:]\n",
    "    return np.std(recent) if len(recent) > 1 else 50.0  # Default uncertainty\n",
    "\n",
    "# Calculate 95% confidence intervals (¬±1.96 * std)\n",
    "print(\"=\" * 70)\n",
    "print(\"Elo Rankings with 95% Confidence Intervals\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rank':<6} {'Model':<20} {'Rating':<10} {'95% CI':<20} {'Uncertainty'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "uncertainties = {}\n",
    "for i, (model, rating) in enumerate(leaderboard, 1):\n",
    "    uncertainty = calculate_rating_uncertainty(rating_history[model])\n",
    "    uncertainties[model] = uncertainty\n",
    "    ci_lower = rating - 1.96 * uncertainty\n",
    "    ci_upper = rating + 1.96 * uncertainty\n",
    "    print(f\"{i:<6} {model:<20} {rating:7.1f}    [{ci_lower:6.1f}, {ci_upper:6.1f}]    ¬±{uncertainty:.1f}\")\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"- Lower uncertainty = more stable/confident ranking\")\n",
    "print(\"- Higher uncertainty = volatile ranking (needs more comparisons)\")\n",
    "print(\"- Overlapping CIs = rankings not statistically different\")\n",
    "\n",
    "# Visualize uncertainty\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models = [m for m, _ in leaderboard]\n",
    "ratings = [r for _, r in leaderboard]\n",
    "errors = [1.96 * uncertainties[m] for m in models]\n",
    "\n",
    "y_pos = np.arange(len(models))\n",
    "ax.barh(y_pos, ratings, xerr=errors, color='steelblue', alpha=0.7, \n",
    "        ecolor='black', capsize=5, error_kw={'linewidth': 2})\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(models)\n",
    "ax.set_xlabel('Elo Rating (with 95% CI)', fontsize=11)\n",
    "ax.set_title('Elo Rankings with Uncertainty', fontsize=13, fontweight='bold')\n",
    "ax.axvline(x=1500, color='gray', linestyle='--', alpha=0.5, label='Initial rating')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/elo_uncertainty.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Uncertainty visualization saved to results/elo_uncertainty.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transitivity-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analyze transitivity violations\n",
    "# Transitivity: If A > B and B > C, then A > C should hold\n",
    "# Violations indicate inconsistency in judgments\n",
    "\n",
    "def find_transitivity_violations(comparisons):\n",
    "    \"\"\"Find cycles in pairwise comparisons (A > B > C > A).\"\"\"\n",
    "    # Build win graph\n",
    "    wins = defaultdict(set)  # wins[A] = set of models that A beat\n",
    "    \n",
    "    for comp in comparisons:\n",
    "        if comp['winner'] == 'A':\n",
    "            wins['Response_A'].add('Response_B')\n",
    "        elif comp['winner'] == 'B':\n",
    "            wins['Response_B'].add('Response_A')\n",
    "    \n",
    "    # Find simple cycles (A > B > C > A)\n",
    "    violations = []\n",
    "    models = list(wins.keys())\n",
    "    \n",
    "    for a in models:\n",
    "        for b in wins[a]:  # A beat B\n",
    "            if b in wins:\n",
    "                for c in wins[b]:  # B beat C\n",
    "                    if c in wins and a in wins[c]:  # C beat A (cycle!)\n",
    "                        violations.append((a, b, c))\n",
    "    \n",
    "    return violations\n",
    "\n",
    "violations = find_transitivity_violations(comparisons)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Transitivity Violation Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if violations:\n",
    "    print(f\"‚ö†Ô∏è  Found {len(violations)} transitivity violations (cycles):\\n\")\n",
    "    for i, (a, b, c) in enumerate(violations[:5], 1):  # Show first 5\n",
    "        print(f\"{i}. {a} > {b} > {c} > {a}\")\n",
    "    if len(violations) > 5:\n",
    "        print(f\"   ... and {len(violations) - 5} more\")\n",
    "    \n",
    "    violation_rate = len(violations) / len(comparisons)\n",
    "    print(f\"\\nViolation rate: {violation_rate:.1%}\")\n",
    "    \n",
    "    if violation_rate > 0.1:\n",
    "        print(\"\\n‚ö†Ô∏è  High violation rate (>10%) suggests:\")\n",
    "        print(\"   - Inconsistent judge behavior\")\n",
    "        print(\"   - Query-dependent preferences\")\n",
    "        print(\"   - Forced wins (should have been ties)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Acceptable violation rate (<10%)\")\n",
    "        print(\"   Some cycles are expected due to measurement noise\")\n",
    "else:\n",
    "    print(\"‚úÖ No transitivity violations found!\")\n",
    "    print(\"   All comparisons are consistent (A > B > C implies A > C)\")\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"- Elo rankings are robust to some violations (averages out noise)\")\n",
    "print(\"- Bradley-Terry model assumes transitivity (may fit poorly if many violations)\")\n",
    "print(\"- Investigate violations to improve judge consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Elo Formula**: \n",
    "   - Expected score: `E_A = 1 / (1 + 10^((R_B - R_A) / 400))`\n",
    "   - Rating update: `R_A' = R_A + K * (S_A - E_A)`\n",
    "   - Larger updates for surprising results (upsets)\n",
    "\n",
    "2. **Dynamic Updates**:\n",
    "   - Process comparisons incrementally (online learning)\n",
    "   - Ratings converge to true skill over time\n",
    "   - Easy to add new models (just assign initial rating)\n",
    "\n",
    "3. **Uncertainty Estimation**:\n",
    "   - Elo doesn't provide native uncertainty\n",
    "   - Can estimate from rating volatility\n",
    "   - More matches = lower uncertainty\n",
    "\n",
    "4. **Transitivity Violations**:\n",
    "   - Some cycles expected due to noise\n",
    "   - >10% violation rate indicates judge inconsistency\n",
    "   - Elo is robust to violations (Bradley-Terry is not)\n",
    "\n",
    "### When to Use Elo\n",
    "\n",
    "‚úÖ **Use Elo when:**\n",
    "- Building live leaderboard with continuous updates (e.g., Chatbot Arena)\n",
    "- New models added frequently\n",
    "- Want simple, interpretable rankings\n",
    "- Order of comparisons doesn't matter for final use\n",
    "\n",
    "‚ùå **Don't use Elo when:**\n",
    "- Need uncertainty estimates (use Bradley-Terry)\n",
    "- Want batch analysis of fixed model set (use Bradley-Terry)\n",
    "- Order-independence is critical for fairness\n",
    "\n",
    "### Practical Tips\n",
    "\n",
    "1. **K-factor tuning:**\n",
    "   - K=32: Standard (balanced convergence speed)\n",
    "   - K=64: New/volatile models (faster updates)\n",
    "   - K=16: Mature/stable models (slower updates)\n",
    "\n",
    "2. **Cold start problem:**\n",
    "   - New models start at initial rating (e.g., 1500)\n",
    "   - First ~20 matches have high volatility\n",
    "   - Consider higher K-factor for new models\n",
    "\n",
    "3. **Rating interpretation:**\n",
    "   - 100 Elo difference ‚âà 64% win probability\n",
    "   - 200 Elo difference ‚âà 76% win probability\n",
    "   - 400 Elo difference ‚âà 91% win probability\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- üìä [Bradley-Terry Ranking Tutorial](bradley_terry_ranking_tutorial.ipynb) - Probabilistic alternative with uncertainty\n",
    "- üî¨ [A/B Testing vs Comparative Eval](ab_testing_vs_comparative_eval.ipynb) - When to use comparative evaluation\n",
    "- üìñ [Comparative Evaluation Guide](comparative_evaluation_guide.md) - Comprehensive methodology overview\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Elo Rating System (Wikipedia)](https://en.wikipedia.org/wiki/Elo_rating_system)\n",
    "- [Chatbot Arena Leaderboard](https://chat.lmsys.org/?leaderboard) - Live Elo rankings\n",
    "- [Backend Implementation](../backend/comparative_evaluation.py) - EloRanking class source code\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've mastered Elo ranking for LLM evaluation. You can now build dynamic leaderboards and understand rating evolution over time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
