{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Utilization Measurement in RAG Systems\n",
    "\n",
    "**Lesson 13 - Interactive Notebook**\n",
    "\n",
    "This notebook demonstrates how to measure which retrieved context documents the LLM actually uses in its responses.\n",
    "\n",
    "## Learning Objectives\n",
    "- Measure semantic similarity between response and each context document\n",
    "- Classify documents as USED (>0.7), PARTIAL (0.4-0.7), or IGNORED (<0.4)\n",
    "- Identify which retrieved documents contribute to the response\n",
    "- Detect retrieval waste (unused relevant documents)\n",
    "\n",
    "## Execution Time & Cost\n",
    "- **DEMO Mode**: ~2 minutes, $0.03 (20 test cases)\n",
    "- **FULL Mode**: ~6 minutes, $1.50-2.50 (200 test cases)\n",
    "\n",
    "**âš ï¸ COST WARNING**: Running FULL mode will use OpenAI API credits for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from backend.rag_generation_eval import ContextUtilizationScorer\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: DEMO or FULL mode\n",
    "MODE = \"DEMO\"  # Change to \"FULL\" for comprehensive evaluation\n",
    "\n",
    "N_SAMPLES = 20 if MODE == \"DEMO\" else 200\n",
    "ESTIMATED_COST = \"$0.03\" if MODE == \"DEMO\" else \"$1.50-2.50\"\n",
    "ESTIMATED_TIME = \"2 minutes\" if MODE == \"DEMO\" else \"6 minutes\"\n",
    "\n",
    "print(f\"ðŸ”§ Mode: {MODE}\")\n",
    "print(f\"ðŸ“Š Samples: {N_SAMPLES}\")\n",
    "print(f\"ðŸ’° Estimated Cost: {ESTIMATED_COST}\")\n",
    "print(f\"â±ï¸  Estimated Time: {ESTIMATED_TIME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load RAG Evaluation Test Suite\n",
    "\n",
    "We'll use the same 500-case test suite, but focus on multi-document context cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test suite\n",
    "test_suite_path = Path(\"data/rag_evaluation_suite.json\")\n",
    "\n",
    "if not test_suite_path.exists():\n",
    "    raise FileNotFoundError(f\"Test suite not found at {test_suite_path}\")\n",
    "\n",
    "with open(test_suite_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_suite = json.load(f)\n",
    "\n",
    "all_test_cases = test_suite[\"test_cases\"]\n",
    "\n",
    "# Sample based on mode\n",
    "import random\n",
    "random.seed(42)  # Reproducibility\n",
    "test_cases = random.sample(all_test_cases, min(N_SAMPLES, len(all_test_cases)))\n",
    "\n",
    "print(f\"âœ… Loaded {len(all_test_cases)} test cases from suite\")\n",
    "print(f\"ðŸ“‹ Selected {len(test_cases)} test cases for {MODE} mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Context Utilization Scorer\n",
    "\n",
    "The `ContextUtilizationScorer` measures semantic similarity between response and each context document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scorer\n",
    "scorer = ContextUtilizationScorer()\n",
    "\n",
    "print(\"âœ… ContextUtilizationScorer initialized\")\n",
    "print(\"\\nScorer Methods:\")\n",
    "print(\"  - measure_utilization(): Calculate similarity for each context doc\")\n",
    "print(\"  - classify_usage(): Classify as USED/PARTIAL/IGNORED\")\n",
    "print(\"\\nThresholds:\")\n",
    "print(\"  - USED: Similarity > 0.7\")\n",
    "print(\"  - PARTIAL: Similarity 0.4 - 0.7\")\n",
    "print(\"  - IGNORED: Similarity < 0.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Example - Single Case Utilization\n",
    "\n",
    "Let's see how context utilization works on a single example with multiple context documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with multiple context documents\n",
    "example_case = {\n",
    "    \"query\": \"What is Krishna's teaching to Arjuna?\",\n",
    "    \"context\": [\n",
    "        \"Krishna teaches Arjuna about duty and dharma in the Bhagavad Gita.\",  # USED\n",
    "        \"The Mahabharata is an ancient Indian epic about a great war.\",  # PARTIAL\n",
    "        \"Chocolate chip cookies require flour, sugar, butter, and chocolate chips.\",  # IGNORED\n",
    "    ],\n",
    "    \"response\": \"Krishna teaches Arjuna about his duty and the importance of dharma.\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ“– Example Case: Context Utilization\\n\")\n",
    "print(f\"Query: {example_case['query']}\")\n",
    "print(f\"\\nResponse: {example_case['response']}\")\n",
    "print(f\"\\nContext Documents ({len(example_case['context'])}):\\n\")\n",
    "for i, doc in enumerate(example_case['context']):\n",
    "    print(f\"{i + 1}. {doc}\")\n",
    "\n",
    "# Measure utilization\n",
    "utilization = scorer.measure_utilization(example_case['response'], example_case['context'])\n",
    "\n",
    "print(f\"\\nðŸ“Š Utilization Scores:\")\n",
    "for doc_idx, similarity in utilization.items():\n",
    "    usage = scorer.classify_usage(similarity)\n",
    "    emoji = \"ðŸŸ©\" if usage == \"USED\" else \"ðŸŸ¨\" if usage == \"PARTIAL\" else \"ðŸŸ¥\"\n",
    "    print(f\"   Doc {doc_idx + 1}: {similarity:.3f} {emoji} {usage}\")\n",
    "\n",
    "print(\"\\nâœ… As expected:\")\n",
    "print(\"   - Doc 1 (Krishna teaching) is USED\")\n",
    "print(\"   - Doc 2 (Mahabharata context) is PARTIAL\")\n",
    "print(\"   - Doc 3 (cookie recipe) is IGNORED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Evaluation on Test Suite\n",
    "\n",
    "Evaluate context utilization across all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluation\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "errors = 0\n",
    "\n",
    "print(f\"ðŸ”„ Measuring context utilization for {len(test_cases)} test cases...\\n\")\n",
    "\n",
    "for test_case in tqdm(test_cases, desc=\"Context Utilization\"):\n",
    "    try:\n",
    "        # Extract fields\n",
    "        answer = test_case.get(\"answer\", \"\")\n",
    "        context = test_case.get(\"context\", [])\n",
    "        ground_truth_usage = test_case.get(\"labels\", {}).get(\"context_utilization\", None)\n",
    "        \n",
    "        # Measure utilization\n",
    "        utilization = scorer.measure_utilization(answer, context)\n",
    "        \n",
    "        # Classify each document\n",
    "        doc_classifications = {}\n",
    "        for doc_idx, similarity in utilization.items():\n",
    "            usage = scorer.classify_usage(similarity)\n",
    "            doc_classifications[doc_idx] = {\n",
    "                \"similarity\": similarity,\n",
    "                \"usage\": usage,\n",
    "            }\n",
    "        \n",
    "        # Aggregate: what's the overall utilization?\n",
    "        # Classify as USED if at least 1 doc is USED, PARTIAL if at least 1 PARTIAL, else IGNORED\n",
    "        usages = [doc[\"usage\"] for doc in doc_classifications.values()]\n",
    "        if \"USED\" in usages:\n",
    "            overall_usage = \"USED\"\n",
    "        elif \"PARTIAL\" in usages:\n",
    "            overall_usage = \"PARTIAL\"\n",
    "        else:\n",
    "            overall_usage = \"IGNORED\"\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"source\": test_case.get(\"source\"),\n",
    "            \"query\": test_case.get(\"query\"),\n",
    "            \"answer\": answer,\n",
    "            \"context_count\": len(context),\n",
    "            \"utilization\": utilization,\n",
    "            \"doc_classifications\": doc_classifications,\n",
    "            \"overall_usage\": overall_usage,\n",
    "            \"ground_truth_usage\": ground_truth_usage,\n",
    "            \"correct_classification\": overall_usage == ground_truth_usage if ground_truth_usage else None,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(f\"âš ï¸  Error on test case {test_case.get('id')}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete!\")\n",
    "print(f\"   Total cases: {len(results)}\")\n",
    "print(f\"   Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Compute metrics and analyze context utilization patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "total_cases = len(results)\n",
    "\n",
    "# Overall utilization distribution\n",
    "usage_counts = {\"USED\": 0, \"PARTIAL\": 0, \"IGNORED\": 0}\n",
    "for result in results:\n",
    "    usage_counts[result[\"overall_usage\"]] += 1\n",
    "\n",
    "# Accuracy (if ground truth available)\n",
    "cases_with_ground_truth = [r for r in results if r[\"correct_classification\"] is not None]\n",
    "correct_classifications = sum(1 for r in cases_with_ground_truth if r[\"correct_classification\"])\n",
    "accuracy = (\n",
    "    correct_classifications / len(cases_with_ground_truth)\n",
    "    if cases_with_ground_truth\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "# Average utilization per document\n",
    "all_similarities = []\n",
    "for result in results:\n",
    "    for doc_idx, doc_info in result[\"doc_classifications\"].items():\n",
    "        all_similarities.append(doc_info[\"similarity\"])\n",
    "\n",
    "avg_similarity = sum(all_similarities) / len(all_similarities) if all_similarities else 0.0\n",
    "\n",
    "# By source\n",
    "source_breakdown = {}\n",
    "for result in results:\n",
    "    source = result[\"source\"]\n",
    "    if source not in source_breakdown:\n",
    "        source_breakdown[source] = {\"USED\": 0, \"PARTIAL\": 0, \"IGNORED\": 0, \"total\": 0}\n",
    "    source_breakdown[source][result[\"overall_usage\"]] += 1\n",
    "    source_breakdown[source][\"total\"] += 1\n",
    "\n",
    "# Print metrics\n",
    "print(\"ðŸ“Š CONTEXT UTILIZATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nðŸ“ˆ Overall Metrics:\")\n",
    "print(f\"   Total test cases: {total_cases}\")\n",
    "print(f\"   USED: {usage_counts['USED']} ({usage_counts['USED'] / total_cases * 100:.1f}%)\")\n",
    "print(f\"   PARTIAL: {usage_counts['PARTIAL']} ({usage_counts['PARTIAL'] / total_cases * 100:.1f}%)\")\n",
    "print(f\"   IGNORED: {usage_counts['IGNORED']} ({usage_counts['IGNORED'] / total_cases * 100:.1f}%)\")\n",
    "print(f\"   Avg Similarity: {avg_similarity:.3f}\")\n",
    "print(f\"   Classification Accuracy: {accuracy * 100:.1f}% (vs ground truth)\")\n",
    "\n",
    "print(f\"\\nðŸ“‚ Breakdown by Source:\")\n",
    "for source, stats in source_breakdown.items():\n",
    "    print(f\"\\n   {source}:\")\n",
    "    print(f\"      USED: {stats['USED']}/{stats['total']} ({stats['USED'] / stats['total'] * 100:.1f}%)\")\n",
    "    print(f\"      PARTIAL: {stats['PARTIAL']}/{stats['total']} ({stats['PARTIAL'] / stats['total'] * 100:.1f}%)\")\n",
    "    print(f\"      IGNORED: {stats['IGNORED']}/{stats['total']} ({stats['IGNORED'] / stats['total'] * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Utilization Heatmap\n",
    "\n",
    "Create a heatmap showing which documents are used across test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prepare heatmap data (first 20 cases for readability)\n",
    "n_display = min(20, len(results))\n",
    "max_docs = max(r[\"context_count\"] for r in results[:n_display])\n",
    "\n",
    "heatmap_data = np.zeros((n_display, max_docs))\n",
    "for i, result in enumerate(results[:n_display]):\n",
    "    for doc_idx, doc_info in result[\"doc_classifications\"].items():\n",
    "        heatmap_data[i][doc_idx] = doc_info[\"similarity\"]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"RdYlGn\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"label\": \"Similarity Score\"},\n",
    "    xticklabels=[f\"Doc {i+1}\" for i in range(max_docs)],\n",
    "    yticklabels=[f\"Case {i+1}\" for i in range(n_display)],\n",
    ")\n",
    "\n",
    "plt.title(\"Context Utilization Heatmap\\n(Green = USED, Yellow = PARTIAL, Red = IGNORED)\", fontsize=14)\n",
    "plt.xlabel(\"Context Document Index\")\n",
    "plt.ylabel(\"Test Case Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Heatmap Legend:\")\n",
    "print(\"   ðŸŸ© Green (>0.7): USED - LLM actively referenced document\")\n",
    "print(\"   ðŸŸ¨ Yellow (0.4-0.7): PARTIAL - LLM partially used document\")\n",
    "print(\"   ðŸŸ¥ Red (<0.4): IGNORED - LLM did not use document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Identify Retrieval Waste\n",
    "\n",
    "Find cases where relevant documents were retrieved but not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases with IGNORED documents\n",
    "wasted_retrievals = []\n",
    "for result in results:\n",
    "    ignored_count = sum(1 for doc in result[\"doc_classifications\"].values() if doc[\"usage\"] == \"IGNORED\")\n",
    "    if ignored_count > 0:\n",
    "        wasted_retrievals.append({\n",
    "            \"id\": result[\"id\"],\n",
    "            \"source\": result[\"source\"],\n",
    "            \"ignored_count\": ignored_count,\n",
    "            \"total_docs\": result[\"context_count\"],\n",
    "            \"waste_rate\": ignored_count / result[\"context_count\"],\n",
    "        })\n",
    "\n",
    "# Sort by waste rate\n",
    "wasted_retrievals.sort(key=lambda x: x[\"waste_rate\"], reverse=True)\n",
    "\n",
    "print(f\"âš ï¸  Retrieval Waste Analysis: {len(wasted_retrievals)} cases with ignored documents\\n\")\n",
    "\n",
    "# Show top 5 wasted cases\n",
    "for i, case in enumerate(wasted_retrievals[:5]):\n",
    "    print(f\"{i + 1}. {case['id']} ({case['source']})\")\n",
    "    print(f\"   Ignored: {case['ignored_count']}/{case['total_docs']} ({case['waste_rate'] * 100:.1f}%)\\n\")\n",
    "\n",
    "# Overall waste rate\n",
    "total_docs = sum(r[\"context_count\"] for r in results)\n",
    "total_ignored = sum(\n",
    "    sum(1 for doc in r[\"doc_classifications\"].values() if doc[\"usage\"] == \"IGNORED\")\n",
    "    for r in results\n",
    ")\n",
    "overall_waste_rate = total_ignored / total_docs if total_docs > 0 else 0.0\n",
    "\n",
    "print(f\"ðŸ“Š Overall Retrieval Waste Rate: {overall_waste_rate * 100:.1f}%\")\n",
    "print(f\"   (Out of {total_docs} retrieved documents, {total_ignored} were ignored)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Export Results for Dashboard\n",
    "\n",
    "Save results to JSON for integration with evaluation dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = {\n",
    "    \"metadata\": {\n",
    "        \"lesson\": \"Lesson 13 - RAG Generation & Attribution\",\n",
    "        \"notebook\": \"context_utilization.ipynb\",\n",
    "        \"mode\": MODE,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"test_cases\": len(results),\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"usage_distribution\": usage_counts,\n",
    "        \"avg_similarity\": avg_similarity,\n",
    "        \"classification_accuracy\": accuracy,\n",
    "        \"retrieval_waste_rate\": overall_waste_rate,\n",
    "        \"source_breakdown\": source_breakdown,\n",
    "    },\n",
    "    \"results\": results,\n",
    "}\n",
    "\n",
    "# Save to results directory\n",
    "output_path = Path(\"results/context_utilization_results.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Results exported to {output_path}\")\n",
    "print(f\"ðŸ“Š File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nðŸŽ¯ Ready for dashboard integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. How to measure semantic similarity between response and context\n",
    "2. How to classify documents as USED/PARTIAL/IGNORED\n",
    "3. How to identify retrieval waste (unused documents)\n",
    "4. How to visualize context utilization patterns\n",
    "\n",
    "**Key Insights:**\n",
    "- Not all retrieved documents are used by the LLM\n",
    "- Gita Q&A tends to have higher context utilization (focused queries)\n",
    "- Recipe responses may partially use multiple documents\n",
    "- Retrieval waste indicates opportunity for reranking\n",
    "- Low utilization may signal irrelevant retrieval or LLM ignoring context\n",
    "\n",
    "**Next Steps:**\n",
    "- Read [End-to-End RAG Evaluation Tutorial](./end_to_end_rag_eval.md)\n",
    "- Try [Attribution Detection Notebook](./attribution_detection.ipynb)\n",
    "- Explore [RAG Failure Taxonomy Diagram](./diagrams/rag_failure_taxonomy.mmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
