{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Detection in RAG Systems\n",
    "\n",
    "**Lesson 13 - Interactive Notebook**\n",
    "\n",
    "This notebook demonstrates how to detect whether LLM responses properly attribute claims to source documents.\n",
    "\n",
    "## Learning Objectives\n",
    "- Extract atomic claims from LLM responses\n",
    "- Verify claims against retrieved context\n",
    "- Measure attribution rate across test cases\n",
    "- Identify attribution failures\n",
    "\n",
    "## Execution Time & Cost\n",
    "- **DEMO Mode**: ~2 minutes, $0.05 (20 test cases)\n",
    "- **FULL Mode**: ~7 minutes, $2.00-3.00 (200 test cases)\n",
    "\n",
    "**‚ö†Ô∏è COST WARNING**: Running FULL mode will use OpenAI API credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from backend.rag_generation_eval import AttributionDetector\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Mode: DEMO\n",
      "üìä Samples: 20\n",
      "üí∞ Estimated Cost: $0.05\n",
      "‚è±Ô∏è  Estimated Time: 2 minutes\n"
     ]
    }
   ],
   "source": [
    "# Configuration: DEMO or FULL mode\n",
    "MODE = \"DEMO\"  # Change to \"FULL\" for comprehensive evaluation\n",
    "\n",
    "N_SAMPLES = 20 if MODE == \"DEMO\" else 200\n",
    "ESTIMATED_COST = \"$0.05\" if MODE == \"DEMO\" else \"$2.00-3.00\"\n",
    "ESTIMATED_TIME = \"2 minutes\" if MODE == \"DEMO\" else \"7 minutes\"\n",
    "\n",
    "print(f\"üîß Mode: {MODE}\")\n",
    "print(f\"üìä Samples: {N_SAMPLES}\")\n",
    "print(f\"üí∞ Estimated Cost: {ESTIMATED_COST}\")\n",
    "print(f\"‚è±Ô∏è  Estimated Time: {ESTIMATED_TIME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load RAG Evaluation Test Suite\n",
    "\n",
    "We'll use the 500-case test suite generated in `lesson-13/data/rag_evaluation_suite.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 500 test cases from suite\n",
      "üìã Selected 20 test cases for DEMO mode\n",
      "\n",
      "üìä Test Suite Statistics:\n",
      "   total_cases: 500\n",
      "   gita_samples: 200\n",
      "   recipe_samples: 200\n",
      "   adversarial_samples: 100\n",
      "   attribution_pass: 400\n",
      "   attribution_fail: 100\n",
      "   hallucination_none: 400\n",
      "   hallucination_intrinsic: 40\n",
      "   hallucination_extrinsic: 60\n"
     ]
    }
   ],
   "source": [
    "# Load test suite\n",
    "test_suite_path = Path(\"data/rag_evaluation_suite.json\")\n",
    "\n",
    "if not test_suite_path.exists():\n",
    "    raise FileNotFoundError(f\"Test suite not found at {test_suite_path}\")\n",
    "\n",
    "with open(test_suite_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_suite = json.load(f)\n",
    "\n",
    "all_test_cases = test_suite[\"test_cases\"]\n",
    "\n",
    "# Sample based on mode\n",
    "import random\n",
    "\n",
    "random.seed(42)  # Reproducibility\n",
    "test_cases = random.sample(all_test_cases, min(N_SAMPLES, len(all_test_cases)))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(all_test_cases)} test cases from suite\")\n",
    "print(f\"üìã Selected {len(test_cases)} test cases for {MODE} mode\")\n",
    "print(\"\\nüìä Test Suite Statistics:\")\n",
    "for key, value in test_suite[\"statistics\"].items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Attribution Detector\n",
    "\n",
    "The `AttributionDetector` extracts claims from responses and verifies them against context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AttributionDetector initialized\n",
      "\n",
      "Detector Methods:\n",
      "  - extract_claims(): Extract atomic claims from response\n",
      "  - verify_attribution(): Check claims against context\n",
      "  - calculate_attribution_rate(): Calculate overall attribution rate\n"
     ]
    }
   ],
   "source": [
    "# Initialize detector\n",
    "detector = AttributionDetector()\n",
    "\n",
    "print(\"‚úÖ AttributionDetector initialized\")\n",
    "print(\"\\nDetector Methods:\")\n",
    "print(\"  - extract_claims(): Extract atomic claims from response\")\n",
    "print(\"  - verify_attribution(): Check claims against context\")\n",
    "print(\"  - calculate_attribution_rate(): Calculate overall attribution rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Example - Single Case Attribution\n",
    "\n",
    "Let's see how attribution detection works on a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Example Case: GOOD Attribution\n",
      "\n",
      "Query: What is the main teaching of the Bhagavad Gita?\n",
      "\n",
      "Context: The Bhagavad Gita teaches dharma (duty), karma (action), and moksha (liberation).\n",
      "\n",
      "Answer: The main teaching is dharma, karma, and moksha.\n",
      "\n",
      "‚úÖ Extracted Claims: ['The main teaching is dharma, karma, and moksha']\n",
      "\n",
      "üìä Attribution Scores: [False]\n",
      "üìà Attribution Rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Example: GOOD attribution (from Gita dataset)\n",
    "example_case = {\n",
    "    \"query\": \"What is the main teaching of the Bhagavad Gita?\",\n",
    "    \"context\": [\"The Bhagavad Gita teaches dharma (duty), karma (action), and moksha (liberation).\"],\n",
    "    \"answer\": \"The main teaching is dharma, karma, and moksha.\"\n",
    "}\n",
    "\n",
    "print(\"üìñ Example Case: GOOD Attribution\\n\")\n",
    "print(f\"Query: {example_case['query']}\")\n",
    "print(f\"\\nContext: {example_case['context'][0]}\")\n",
    "print(f\"\\nAnswer: {example_case['answer']}\")\n",
    "\n",
    "# Extract claims\n",
    "claims = detector.extract_claims(example_case['answer'])\n",
    "print(f\"\\n‚úÖ Extracted Claims: {claims}\")\n",
    "\n",
    "# Verify attribution\n",
    "result = detector.verify_attribution(claims, example_case['context'])\n",
    "print(f\"\\nüìä Attribution Scores: {result['attribution_scores']}\")\n",
    "\n",
    "# Calculate rate\n",
    "rate = sum(result['attribution_scores']) / len(result['attribution_scores']) if result['attribution_scores'] else 0.0\n",
    "print(f\"üìà Attribution Rate: {rate * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Example Case: BAD Attribution (Hallucination)\n",
      "\n",
      "Query: What is the Bhagavad Gita?\n",
      "\n",
      "Context: The Bhagavad Gita is a Hindu scripture.\n",
      "\n",
      "Answer: The Bhagavad Gita is a Hindu scripture written in 300 BC by sage Vyasa in the Himalayas.\n",
      "\n",
      "‚úÖ Extracted Claims: ['The Bhagavad Gita is a Hindu scripture written in 300 BC by sage Vyasa in the Himalayas']\n",
      "\n",
      "üìä Attribution Scores: [False]\n",
      "\n",
      "‚ùå Issues: '300 BC', 'Vyasa', 'Himalayas' are NOT in context!\n",
      "üìà Attribution Rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Example: BAD attribution (unattributed claim)\n",
    "bad_example = {\n",
    "    \"query\": \"What is the Bhagavad Gita?\",\n",
    "    \"context\": [\"The Bhagavad Gita is a Hindu scripture.\"],\n",
    "    \"answer\": \"The Bhagavad Gita is a Hindu scripture written in 300 BC by sage Vyasa in the Himalayas.\"\n",
    "}\n",
    "\n",
    "print(\"‚ö†Ô∏è  Example Case: BAD Attribution (Hallucination)\\n\")\n",
    "print(f\"Query: {bad_example['query']}\")\n",
    "print(f\"\\nContext: {bad_example['context'][0]}\")\n",
    "print(f\"\\nAnswer: {bad_example['answer']}\")\n",
    "\n",
    "# Extract claims\n",
    "claims = detector.extract_claims(bad_example['answer'])\n",
    "print(f\"\\n‚úÖ Extracted Claims: {claims}\")\n",
    "\n",
    "# Verify attribution\n",
    "result = detector.verify_attribution(claims, bad_example['context'])\n",
    "print(f\"\\nüìä Attribution Scores: {result['attribution_scores']}\")\n",
    "print(\"\\n‚ùå Issues: '300 BC', 'Vyasa', 'Himalayas' are NOT in context!\")\n",
    "\n",
    "# Calculate rate\n",
    "rate = sum(result['attribution_scores']) / len(result['attribution_scores']) if result['attribution_scores'] else 0.0\n",
    "print(f\"üìà Attribution Rate: {rate * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Evaluation on Test Suite\n",
    "\n",
    "Now we'll evaluate attribution across all test cases in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Evaluating attribution for 20 test cases...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attribution Detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 60393.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete!\n",
      "   Total cases: 20\n",
      "   Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch evaluation\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "errors = 0\n",
    "\n",
    "print(f\"üîÑ Evaluating attribution for {len(test_cases)} test cases...\\n\")\n",
    "\n",
    "for test_case in tqdm(test_cases, desc=\"Attribution Detection\"):\n",
    "    try:\n",
    "        # Extract fields\n",
    "        answer = test_case.get(\"answer\", \"\")\n",
    "        context = test_case.get(\"context\", [])\n",
    "        ground_truth = test_case.get(\"labels\", {}).get(\"is_attributed\", None)\n",
    "        \n",
    "        # Evaluate attribution\n",
    "        claims = detector.extract_claims(answer)\n",
    "        attribution_result = detector.verify_attribution(claims, context)\n",
    "        \n",
    "        # Calculate attribution rate for this case\n",
    "        attribution_scores = attribution_result[\"attribution_scores\"]\n",
    "        attribution_rate = (\n",
    "            sum(attribution_scores) / len(attribution_scores)\n",
    "            if attribution_scores\n",
    "            else 0.0\n",
    "        )\n",
    "        \n",
    "        # Classify as PASS/FAIL (>0.7 = attributed)\n",
    "        is_attributed = attribution_rate > 0.7\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"source\": test_case.get(\"source\"),\n",
    "            \"query\": test_case.get(\"query\"),\n",
    "            \"answer\": answer,\n",
    "            \"claims\": claims,\n",
    "            \"attribution_rate\": attribution_rate,\n",
    "            \"is_attributed\": is_attributed,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"correct_classification\": is_attributed == ground_truth if ground_truth is not None else None,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(f\"‚ö†Ô∏è  Error on test case {test_case.get('id')}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"   Total cases: {len(results)}\")\n",
    "print(f\"   Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Let's compute metrics and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ATTRIBUTION DETECTION RESULTS\n",
      "==================================================\n",
      "\n",
      "üìà Overall Metrics:\n",
      "   Total test cases: 20\n",
      "   Attributed: 12 (60.0%)\n",
      "   Unattributed: 8 (40.0%)\n",
      "   Avg Attribution Rate: 60.0%\n",
      "   Detection Accuracy: 65.0% (vs ground truth)\n",
      "\n",
      "üìÇ Breakdown by Source:\n",
      "   recipes: 0/7 (0.0%)\n",
      "   bhagavad_gita: 12/12 (100.0%)\n",
      "   adversarial: 0/1 (0.0%)\n",
      "\n",
      "‚úÖ SUCCESS CRITERIA:\n",
      "   Target: 80%+ attribution detection accuracy\n",
      "   Achieved: 65.0%\n",
      "   ‚ö†Ô∏è  NEEDS IMPROVEMENT: Below 80% threshold\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "total_cases = len(results)\n",
    "attributed_cases = sum(1 for r in results if r[\"is_attributed\"])\n",
    "unattributed_cases = total_cases - attributed_cases\n",
    "\n",
    "# Overall attribution rate\n",
    "overall_attribution_rate = sum(r[\"attribution_rate\"] for r in results) / total_cases if total_cases > 0 else 0.0\n",
    "\n",
    "# Accuracy (if ground truth available)\n",
    "cases_with_ground_truth = [r for r in results if r[\"correct_classification\"] is not None]\n",
    "correct_classifications = sum(1 for r in cases_with_ground_truth if r[\"correct_classification\"])\n",
    "accuracy = (\n",
    "    correct_classifications / len(cases_with_ground_truth)\n",
    "    if cases_with_ground_truth\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "# By source (Gita vs recipes vs adversarial)\n",
    "source_breakdown = {}\n",
    "for result in results:\n",
    "    source = result[\"source\"]\n",
    "    if source not in source_breakdown:\n",
    "        source_breakdown[source] = {\"total\": 0, \"attributed\": 0}\n",
    "    source_breakdown[source][\"total\"] += 1\n",
    "    if result[\"is_attributed\"]:\n",
    "        source_breakdown[source][\"attributed\"] += 1\n",
    "\n",
    "# Print metrics\n",
    "print(\"üìä ATTRIBUTION DETECTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nüìà Overall Metrics:\")\n",
    "print(f\"   Total test cases: {total_cases}\")\n",
    "print(f\"   Attributed: {attributed_cases} ({attributed_cases / total_cases * 100:.1f}%)\")\n",
    "print(f\"   Unattributed: {unattributed_cases} ({unattributed_cases / total_cases * 100:.1f}%)\")\n",
    "print(f\"   Avg Attribution Rate: {overall_attribution_rate * 100:.1f}%\")\n",
    "print(f\"   Detection Accuracy: {accuracy * 100:.1f}% (vs ground truth)\")\n",
    "\n",
    "print(\"\\nüìÇ Breakdown by Source:\")\n",
    "for source, stats in source_breakdown.items():\n",
    "    rate = stats[\"attributed\"] / stats[\"total\"] * 100 if stats[\"total\"] > 0 else 0\n",
    "    print(f\"   {source}: {stats['attributed']}/{stats['total']} ({rate:.1f}%)\")\n",
    "\n",
    "# Success criteria check\n",
    "print(\"\\n‚úÖ SUCCESS CRITERIA:\")\n",
    "print(\"   Target: 80%+ attribution detection accuracy\")\n",
    "print(f\"   Achieved: {accuracy * 100:.1f}%\")\n",
    "if accuracy >= 0.80:\n",
    "    print(\"   ‚úÖ PASS: Meets success criteria!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  NEEDS IMPROVEMENT: Below 80% threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inspect Failure Cases\n",
    "\n",
    "Let's examine cases where attribution detection failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Attribution Detection Failures: 7\n",
      "\n",
      "\n",
      "============================================================\n",
      "Failure #1: recipe_127\n",
      "Source: recipes\n",
      "\n",
      "Query: How do I make this dish?\n",
      "\n",
      "Answer: To make this dish, you'll need: acorn squash, fresh ground pepper, nutmeg. Follow the recipe instructions.\n",
      "\n",
      "Claims: [\"To make this dish, you'll need: acorn squash, fresh ground pepper, nutmeg\", 'Follow the recipe instructions']\n",
      "\n",
      "Attribution Rate: 0.0%\n",
      "Predicted: UNATTRIBUTED\n",
      "Ground Truth: ATTRIBUTED\n",
      "\n",
      "============================================================\n",
      "Failure #2: recipe_179\n",
      "Source: recipes\n",
      "\n",
      "Query: What is the recipe for this dish?\n",
      "\n",
      "Answer: To make this dish, you'll need: whole turkey, extra virgin olive oil, salt. Follow the recipe instructions.\n",
      "\n",
      "Claims: [\"To make this dish, you'll need: whole turkey, extra virgin olive oil, salt\", 'Follow the recipe instructions']\n",
      "\n",
      "Attribution Rate: 0.0%\n",
      "Predicted: UNATTRIBUTED\n",
      "Ground Truth: ATTRIBUTED\n",
      "\n",
      "============================================================\n",
      "Failure #3: recipe_177\n",
      "Source: recipes\n",
      "\n",
      "Query: What is the recipe for this dish?\n",
      "\n",
      "Answer: To make this dish, you'll need: ground beef, bacon, cheese. Follow the recipe instructions.\n",
      "\n",
      "Claims: [\"To make this dish, you'll need: ground beef, bacon, cheese\", 'Follow the recipe instructions']\n",
      "\n",
      "Attribution Rate: 0.0%\n",
      "Predicted: UNATTRIBUTED\n",
      "Ground Truth: ATTRIBUTED\n",
      "\n",
      "============================================================\n",
      "Failure #4: recipe_146\n",
      "Source: recipes\n",
      "\n",
      "Query: What are the ingredients for this recipe?\n",
      "\n",
      "Answer: To make this dish, you'll need: salmon fillets, salt, pepper. Follow the recipe instructions.\n",
      "\n",
      "Claims: [\"To make this dish, you'll need: salmon fillets, salt, pepper\", 'Follow the recipe instructions']\n",
      "\n",
      "Attribution Rate: 0.0%\n",
      "Predicted: UNATTRIBUTED\n",
      "Ground Truth: ATTRIBUTED\n",
      "\n",
      "============================================================\n",
      "Failure #5: recipe_79\n",
      "Source: recipes\n",
      "\n",
      "Query: How do I make this dish?\n",
      "\n",
      "Answer: To make this dish, you'll need: cocoa powder, boiling water, unsweetened chocolate. Follow the recipe instructions.\n",
      "\n",
      "Claims: [\"To make this dish, you'll need: cocoa powder, boiling water, unsweetened chocolate\", 'Follow the recipe instructions']\n",
      "\n",
      "Attribution Rate: 0.0%\n",
      "Predicted: UNATTRIBUTED\n",
      "Ground Truth: ATTRIBUTED\n"
     ]
    }
   ],
   "source": [
    "# Find failure cases (incorrect classification vs ground truth)\n",
    "failures = [r for r in results if r[\"correct_classification\"] is False]\n",
    "\n",
    "print(f\"‚ùå Attribution Detection Failures: {len(failures)}\\n\")\n",
    "\n",
    "# Show top 5 failures\n",
    "for i, failure in enumerate(failures[:5]):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Failure #{i + 1}: {failure['id']}\")\n",
    "    print(f\"Source: {failure['source']}\")\n",
    "    print(f\"\\nQuery: {failure['query']}\")\n",
    "    print(f\"\\nAnswer: {failure['answer'][:200]}...\" if len(failure['answer']) > 200 else f\"\\nAnswer: {failure['answer']}\")\n",
    "    print(f\"\\nClaims: {failure['claims']}\")\n",
    "    print(f\"\\nAttribution Rate: {failure['attribution_rate'] * 100:.1f}%\")\n",
    "    print(f\"Predicted: {'ATTRIBUTED' if failure['is_attributed'] else 'UNATTRIBUTED'}\")\n",
    "    print(f\"Ground Truth: {'ATTRIBUTED' if failure['ground_truth'] else 'UNATTRIBUTED'}\")\n",
    "\n",
    "if len(failures) == 0:\n",
    "    print(\"‚úÖ No failures detected! Perfect attribution detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results for Dashboard\n",
    "\n",
    "Save results to JSON for integration with evaluation dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = {\n",
    "    \"metadata\": {\n",
    "        \"lesson\": \"Lesson 13 - RAG Generation & Attribution\",\n",
    "        \"notebook\": \"attribution_detection.ipynb\",\n",
    "        \"mode\": MODE,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"test_cases\": len(results),\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"overall_attribution_rate\": overall_attribution_rate,\n",
    "        \"detection_accuracy\": accuracy,\n",
    "        \"attributed_cases\": attributed_cases,\n",
    "        \"unattributed_cases\": unattributed_cases,\n",
    "        \"source_breakdown\": source_breakdown,\n",
    "    },\n",
    "    \"results\": results,\n",
    "}\n",
    "\n",
    "# Save to results directory\n",
    "output_path = Path(\"results/attribution_results.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Results exported to {output_path}\")\n",
    "print(f\"üìä File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nüéØ Ready for dashboard integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. How to extract atomic claims from LLM responses\n",
    "2. How to verify claims against retrieved context\n",
    "3. How to measure attribution rate across test cases\n",
    "4. How to identify attribution failures and hallucinations\n",
    "\n",
    "**Key Insights:**\n",
    "- Attribution detection accuracy depends on claim extraction quality\n",
    "- Simple substring matching works for exact attribution\n",
    "- Semantic similarity needed for paraphrased claims\n",
    "- Gita Q&A has higher attribution rates (structured answers)\n",
    "- Recipe responses may have lower attribution (LLM adds instructions)\n",
    "\n",
    "**Next Steps:**\n",
    "- Try [Context Utilization Notebook](./context_utilization.ipynb)\n",
    "- Read [Hallucination Detection Tutorial](./hallucination_detection_rag.md)\n",
    "- Explore [End-to-End RAG Evaluation](./end_to_end_rag_eval.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
