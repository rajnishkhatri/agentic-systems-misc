{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Detection in RAG Systems\n",
    "\n",
    "**Lesson 13 - Interactive Notebook**\n",
    "\n",
    "This notebook demonstrates how to detect whether LLM responses properly attribute claims to source documents.\n",
    "\n",
    "## Learning Objectives\n",
    "- Extract atomic claims from LLM responses\n",
    "- Verify claims against retrieved context\n",
    "- Measure attribution rate across test cases\n",
    "- Identify attribution failures\n",
    "\n",
    "## Execution Time & Cost\n",
    "- **DEMO Mode**: ~2 minutes, $0.05 (20 test cases)\n",
    "- **FULL Mode**: ~7 minutes, $2.00-3.00 (200 test cases)\n",
    "\n",
    "**âš ï¸ COST WARNING**: Running FULL mode will use OpenAI API credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from backend.rag_generation_eval import AttributionDetector\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: DEMO or FULL mode\n",
    "MODE = \"DEMO\"  # Change to \"FULL\" for comprehensive evaluation\n",
    "\n",
    "N_SAMPLES = 20 if MODE == \"DEMO\" else 200\n",
    "ESTIMATED_COST = \"$0.05\" if MODE == \"DEMO\" else \"$2.00-3.00\"\n",
    "ESTIMATED_TIME = \"2 minutes\" if MODE == \"DEMO\" else \"7 minutes\"\n",
    "\n",
    "print(f\"ðŸ”§ Mode: {MODE}\")\n",
    "print(f\"ðŸ“Š Samples: {N_SAMPLES}\")\n",
    "print(f\"ðŸ’° Estimated Cost: {ESTIMATED_COST}\")\n",
    "print(f\"â±ï¸  Estimated Time: {ESTIMATED_TIME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load RAG Evaluation Test Suite\n",
    "\n",
    "We'll use the 500-case test suite generated in `lesson-13/data/rag_evaluation_suite.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test suite\n",
    "test_suite_path = Path(\"data/rag_evaluation_suite.json\")\n",
    "\n",
    "if not test_suite_path.exists():\n",
    "    raise FileNotFoundError(f\"Test suite not found at {test_suite_path}\")\n",
    "\n",
    "with open(test_suite_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_suite = json.load(f)\n",
    "\n",
    "all_test_cases = test_suite[\"test_cases\"]\n",
    "\n",
    "# Sample based on mode\n",
    "import random\n",
    "random.seed(42)  # Reproducibility\n",
    "test_cases = random.sample(all_test_cases, min(N_SAMPLES, len(all_test_cases)))\n",
    "\n",
    "print(f\"âœ… Loaded {len(all_test_cases)} test cases from suite\")\n",
    "print(f\"ðŸ“‹ Selected {len(test_cases)} test cases for {MODE} mode\")\n",
    "print(f\"\\nðŸ“Š Test Suite Statistics:\")\n",
    "for key, value in test_suite[\"statistics\"].items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Attribution Detector\n",
    "\n",
    "The `AttributionDetector` extracts claims from responses and verifies them against context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "detector = AttributionDetector()\n",
    "\n",
    "print(\"âœ… AttributionDetector initialized\")\n",
    "print(\"\\nDetector Methods:\")\n",
    "print(\"  - extract_claims(): Extract atomic claims from response\")\n",
    "print(\"  - verify_attribution(): Check claims against context\")\n",
    "print(\"  - calculate_attribution_rate(): Calculate overall attribution rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Example - Single Case Attribution\n",
    "\n",
    "Let's see how attribution detection works on a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: GOOD attribution (from Gita dataset)\n",
    "example_case = {\n",
    "    \"query\": \"What is the main teaching of the Bhagavad Gita?\",\n",
    "    \"context\": [\"The Bhagavad Gita teaches dharma (duty), karma (action), and moksha (liberation).\"],\n",
    "    \"answer\": \"The main teaching is dharma, karma, and moksha.\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ“– Example Case: GOOD Attribution\\n\")\n",
    "print(f\"Query: {example_case['query']}\")\n",
    "print(f\"\\nContext: {example_case['context'][0]}\")\n",
    "print(f\"\\nAnswer: {example_case['answer']}\")\n",
    "\n",
    "# Extract claims\n",
    "claims = detector.extract_claims(example_case['answer'])\n",
    "print(f\"\\nâœ… Extracted Claims: {claims}\")\n",
    "\n",
    "# Verify attribution\n",
    "result = detector.verify_attribution(claims, example_case['context'])\n",
    "print(f\"\\nðŸ“Š Attribution Scores: {result['attribution_scores']}\")\n",
    "\n",
    "# Calculate rate\n",
    "rate = sum(result['attribution_scores']) / len(result['attribution_scores']) if result['attribution_scores'] else 0.0\n",
    "print(f\"ðŸ“ˆ Attribution Rate: {rate * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: BAD attribution (unattributed claim)\n",
    "bad_example = {\n",
    "    \"query\": \"What is the Bhagavad Gita?\",\n",
    "    \"context\": [\"The Bhagavad Gita is a Hindu scripture.\"],\n",
    "    \"answer\": \"The Bhagavad Gita is a Hindu scripture written in 300 BC by sage Vyasa in the Himalayas.\"\n",
    "}\n",
    "\n",
    "print(\"âš ï¸  Example Case: BAD Attribution (Hallucination)\\n\")\n",
    "print(f\"Query: {bad_example['query']}\")\n",
    "print(f\"\\nContext: {bad_example['context'][0]}\")\n",
    "print(f\"\\nAnswer: {bad_example['answer']}\")\n",
    "\n",
    "# Extract claims\n",
    "claims = detector.extract_claims(bad_example['answer'])\n",
    "print(f\"\\nâœ… Extracted Claims: {claims}\")\n",
    "\n",
    "# Verify attribution\n",
    "result = detector.verify_attribution(claims, bad_example['context'])\n",
    "print(f\"\\nðŸ“Š Attribution Scores: {result['attribution_scores']}\")\n",
    "print(\"\\nâŒ Issues: '300 BC', 'Vyasa', 'Himalayas' are NOT in context!\")\n",
    "\n",
    "# Calculate rate\n",
    "rate = sum(result['attribution_scores']) / len(result['attribution_scores']) if result['attribution_scores'] else 0.0\n",
    "print(f\"ðŸ“ˆ Attribution Rate: {rate * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Evaluation on Test Suite\n",
    "\n",
    "Now we'll evaluate attribution across all test cases in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluation\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "errors = 0\n",
    "\n",
    "print(f\"ðŸ”„ Evaluating attribution for {len(test_cases)} test cases...\\n\")\n",
    "\n",
    "for test_case in tqdm(test_cases, desc=\"Attribution Detection\"):\n",
    "    try:\n",
    "        # Extract fields\n",
    "        answer = test_case.get(\"answer\", \"\")\n",
    "        context = test_case.get(\"context\", [])\n",
    "        ground_truth = test_case.get(\"labels\", {}).get(\"is_attributed\", None)\n",
    "        \n",
    "        # Evaluate attribution\n",
    "        claims = detector.extract_claims(answer)\n",
    "        attribution_result = detector.verify_attribution(claims, context)\n",
    "        \n",
    "        # Calculate attribution rate for this case\n",
    "        attribution_scores = attribution_result[\"attribution_scores\"]\n",
    "        attribution_rate = (\n",
    "            sum(attribution_scores) / len(attribution_scores)\n",
    "            if attribution_scores\n",
    "            else 0.0\n",
    "        )\n",
    "        \n",
    "        # Classify as PASS/FAIL (>0.7 = attributed)\n",
    "        is_attributed = attribution_rate > 0.7\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"source\": test_case.get(\"source\"),\n",
    "            \"query\": test_case.get(\"query\"),\n",
    "            \"answer\": answer,\n",
    "            \"claims\": claims,\n",
    "            \"attribution_rate\": attribution_rate,\n",
    "            \"is_attributed\": is_attributed,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"correct_classification\": is_attributed == ground_truth if ground_truth is not None else None,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(f\"âš ï¸  Error on test case {test_case.get('id')}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete!\")\n",
    "print(f\"   Total cases: {len(results)}\")\n",
    "print(f\"   Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Let's compute metrics and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "total_cases = len(results)\n",
    "attributed_cases = sum(1 for r in results if r[\"is_attributed\"])\n",
    "unattributed_cases = total_cases - attributed_cases\n",
    "\n",
    "# Overall attribution rate\n",
    "overall_attribution_rate = sum(r[\"attribution_rate\"] for r in results) / total_cases if total_cases > 0 else 0.0\n",
    "\n",
    "# Accuracy (if ground truth available)\n",
    "cases_with_ground_truth = [r for r in results if r[\"correct_classification\"] is not None]\n",
    "correct_classifications = sum(1 for r in cases_with_ground_truth if r[\"correct_classification\"])\n",
    "accuracy = (\n",
    "    correct_classifications / len(cases_with_ground_truth)\n",
    "    if cases_with_ground_truth\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "# By source (Gita vs recipes vs adversarial)\n",
    "source_breakdown = {}\n",
    "for result in results:\n",
    "    source = result[\"source\"]\n",
    "    if source not in source_breakdown:\n",
    "        source_breakdown[source] = {\"total\": 0, \"attributed\": 0}\n",
    "    source_breakdown[source][\"total\"] += 1\n",
    "    if result[\"is_attributed\"]:\n",
    "        source_breakdown[source][\"attributed\"] += 1\n",
    "\n",
    "# Print metrics\n",
    "print(\"ðŸ“Š ATTRIBUTION DETECTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nðŸ“ˆ Overall Metrics:\")\n",
    "print(f\"   Total test cases: {total_cases}\")\n",
    "print(f\"   Attributed: {attributed_cases} ({attributed_cases / total_cases * 100:.1f}%)\")\n",
    "print(f\"   Unattributed: {unattributed_cases} ({unattributed_cases / total_cases * 100:.1f}%)\")\n",
    "print(f\"   Avg Attribution Rate: {overall_attribution_rate * 100:.1f}%\")\n",
    "print(f\"   Detection Accuracy: {accuracy * 100:.1f}% (vs ground truth)\")\n",
    "\n",
    "print(f\"\\nðŸ“‚ Breakdown by Source:\")\n",
    "for source, stats in source_breakdown.items():\n",
    "    rate = stats[\"attributed\"] / stats[\"total\"] * 100 if stats[\"total\"] > 0 else 0\n",
    "    print(f\"   {source}: {stats['attributed']}/{stats['total']} ({rate:.1f}%)\")\n",
    "\n",
    "# Success criteria check\n",
    "print(\"\\nâœ… SUCCESS CRITERIA:\")\n",
    "print(f\"   Target: 80%+ attribution detection accuracy\")\n",
    "print(f\"   Achieved: {accuracy * 100:.1f}%\")\n",
    "if accuracy >= 0.80:\n",
    "    print(\"   âœ… PASS: Meets success criteria!\")\n",
    "else:\n",
    "    print(\"   âš ï¸  NEEDS IMPROVEMENT: Below 80% threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inspect Failure Cases\n",
    "\n",
    "Let's examine cases where attribution detection failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find failure cases (incorrect classification vs ground truth)\n",
    "failures = [r for r in results if r[\"correct_classification\"] is False]\n",
    "\n",
    "print(f\"âŒ Attribution Detection Failures: {len(failures)}\\n\")\n",
    "\n",
    "# Show top 5 failures\n",
    "for i, failure in enumerate(failures[:5]):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Failure #{i + 1}: {failure['id']}\")\n",
    "    print(f\"Source: {failure['source']}\")\n",
    "    print(f\"\\nQuery: {failure['query']}\")\n",
    "    print(f\"\\nAnswer: {failure['answer'][:200]}...\" if len(failure['answer']) > 200 else f\"\\nAnswer: {failure['answer']}\")\n",
    "    print(f\"\\nClaims: {failure['claims']}\")\n",
    "    print(f\"\\nAttribution Rate: {failure['attribution_rate'] * 100:.1f}%\")\n",
    "    print(f\"Predicted: {'ATTRIBUTED' if failure['is_attributed'] else 'UNATTRIBUTED'}\")\n",
    "    print(f\"Ground Truth: {'ATTRIBUTED' if failure['ground_truth'] else 'UNATTRIBUTED'}\")\n",
    "\n",
    "if len(failures) == 0:\n",
    "    print(\"âœ… No failures detected! Perfect attribution detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results for Dashboard\n",
    "\n",
    "Save results to JSON for integration with evaluation dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = {\n",
    "    \"metadata\": {\n",
    "        \"lesson\": \"Lesson 13 - RAG Generation & Attribution\",\n",
    "        \"notebook\": \"attribution_detection.ipynb\",\n",
    "        \"mode\": MODE,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"test_cases\": len(results),\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"overall_attribution_rate\": overall_attribution_rate,\n",
    "        \"detection_accuracy\": accuracy,\n",
    "        \"attributed_cases\": attributed_cases,\n",
    "        \"unattributed_cases\": unattributed_cases,\n",
    "        \"source_breakdown\": source_breakdown,\n",
    "    },\n",
    "    \"results\": results,\n",
    "}\n",
    "\n",
    "# Save to results directory\n",
    "output_path = Path(\"results/attribution_results.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Results exported to {output_path}\")\n",
    "print(f\"ðŸ“Š File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nðŸŽ¯ Ready for dashboard integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. How to extract atomic claims from LLM responses\n",
    "2. How to verify claims against retrieved context\n",
    "3. How to measure attribution rate across test cases\n",
    "4. How to identify attribution failures and hallucinations\n",
    "\n",
    "**Key Insights:**\n",
    "- Attribution detection accuracy depends on claim extraction quality\n",
    "- Simple substring matching works for exact attribution\n",
    "- Semantic similarity needed for paraphrased claims\n",
    "- Gita Q&A has higher attribution rates (structured answers)\n",
    "- Recipe responses may have lower attribution (LLM adds instructions)\n",
    "\n",
    "**Next Steps:**\n",
    "- Try [Context Utilization Notebook](./context_utilization.ipynb)\n",
    "- Read [Hallucination Detection Tutorial](./hallucination_detection_rag.md)\n",
    "- Explore [End-to-End RAG Evaluation](./end_to_end_rag_eval.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
