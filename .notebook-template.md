# Notebook Template Guide

## Standard Notebook Structure for Lessons 9-11

All interactive notebooks in this tutorial system should follow this standard structure for consistency and best practices.

---

## Cell 0: Title and Learning Objectives (Markdown)

```markdown
# [Tutorial Title]: [Specific Topic]

## Learning Objectives

By completing this tutorial, you will be able to:
- ‚úÖ [Learning objective 1]
- ‚úÖ [Learning objective 2]
- ‚úÖ [Learning objective 3]
- ‚úÖ [Learning objective 4]

## Prerequisites

- [Prerequisite 1]
- [Prerequisite 2]
- [Link to related tutorial if applicable]

## Estimated Time

**Execution Time:** [X-Y minutes]
**Cost:** [Cost estimate with breakdown by mode]

---
```

---

## Cell 1: Cost Warning (Markdown) - **ONLY IF NOTEBOOK USES API CALLS**

```markdown
## ‚ö†Ô∏è Cost Warning

This notebook makes API calls to OpenAI. Estimated costs:

| Mode | API Calls | Estimated Cost |
|------|-----------|----------------|
| **DEMO** (recommended) | [Number] queries √ó [Model] | **$[X.XX-Y.YY]** |
| **FULL** | [Number] queries √ó [Model] | **$[X.XX-Y.YY]** |

**üí° Recommendation:** Start with DEMO mode to understand the notebook before running FULL mode.

**Cost Control:**
- Set `DEMO_MODE = True` in the configuration cell below
- Monitor your API usage at https://platform.openai.com/usage
- DEMO mode processes a sample for quick validation
- FULL mode processes complete dataset for production-quality results

---
```

**Skip this cell if notebook is simulation-based (cost = $0)**

---

## Cell 2: Setup (Code)

```python
# Standard library imports
import os
import sys
import json
import random
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

# Data manipulation and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Statistical packages (if needed)
from scipy import stats
from sklearn.metrics import confusion_matrix, classification_report

# LLM API (if needed)
# import litellm
# from openai import OpenAI

# Environment configuration (if API keys needed)
from dotenv import load_dotenv
load_dotenv()

# Configure plotting
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)

print("‚úÖ Setup complete")
```

---

## Cell 3: Configuration - DEMO/FULL Mode Toggle (Code)

### For API-based notebooks:

```python
# ========================================
# CONFIGURATION: Demo vs Full Mode
# ========================================

# Set DEMO_MODE = False to run on full dataset
DEMO_MODE = True  # Default: Quick demo for tutorial

if DEMO_MODE:
    SAMPLE_SIZE = [X]  # Sample size for demo
    MODEL = "gpt-4o-mini"  # Cheaper model for demo
    print("üöÄ DEMO MODE: Running on sample dataset")
    print(f"   Sample size: {SAMPLE_SIZE} examples")
    print(f"   Model: {MODEL}")
    print(f"   Estimated cost: $[X.XX-Y.YY] | Time: ~[X-Y] minutes")
else:
    SAMPLE_SIZE = None  # Use full dataset
    MODEL = "gpt-4o"  # Production model
    print("üìä FULL MODE: Running on complete dataset")
    print(f"   Full dataset: [X] examples")
    print(f"   Model: {MODEL}")
    print(f"   Estimated cost: $[X.XX-Y.YY] | Time: ~[X-Y] minutes")

print("\nüí° To switch modes, change DEMO_MODE in this cell and re-run notebook")
```

### For simulation-based notebooks (no API calls):

```python
# ========================================
# CONFIGURATION
# ========================================

# Set simulation parameters
NUM_SIMULATIONS = 1000
RANDOM_SEED = 42

# Set random seed
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

print("‚úÖ Configuration set")
print(f"   Simulations: {NUM_SIMULATIONS}")
print(f"   Random seed: {RANDOM_SEED}")
print(f"   Estimated time: <[X] minutes")
print(f"   Cost: $0.00 (simulation-based)")
```

---

## Cell 4: Environment Validation (Code) - **ONLY IF API KEYS REQUIRED**

```python
# Validate API key presence (don't print the actual key!)
if "OPENAI_API_KEY" not in os.environ or not os.environ["OPENAI_API_KEY"]:
    raise ValueError(
        "‚ùå OPENAI_API_KEY not found in environment.\n"
        "Please set it in your .env file:\n"
        "  OPENAI_API_KEY=sk-...\n"
        "Then restart the notebook kernel."
    )

print("‚úÖ OpenAI API key found")

# Optional: Test API connectivity
try:
    from openai import OpenAI
    client = OpenAI()
    # Simple test call (very cheap, ~$0.0001)
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Say 'OK'"}],
        max_tokens=5
    )
    print("‚úÖ API connectivity verified")
except Exception as e:
    raise ValueError(f"‚ùå API test failed: {e}")
```

**Skip this cell if notebook doesn't use API calls**

---

## Remaining Cells: Tutorial Content

Structure tutorial content with clear section headers using markdown cells:

```markdown
## Part 1: [Section Title]

Brief explanation of what this section covers...
```

### Best Practices:

1. **Use descriptive section titles** - "Part 1: Load and Prepare Data", not "Part 1"
2. **Add explanatory markdown before code** - Explain what the next code cell does
3. **Use assertions for validation** - Verify assumptions and data quality
4. **Show progress indicators** - For long-running loops
5. **Visualize results** - Use plots/tables to show key findings
6. **Include interpretation** - Don't just show metrics, explain what they mean

### Example Pattern:

```markdown
## Part 3: Calculate Similarity Metrics

We'll compute four types of similarity:
1. Exact match (strict string comparison)
2. Fuzzy match (Levenshtein distance)
3. BLEU score (n-gram overlap)
4. Semantic similarity (embedding cosine similarity)
```

```python
def calculate_all_similarities(query: str, response: str, reference: str) -> Dict[str, float]:
    """Calculate all similarity metrics."""
    # Type checking (defensive)
    if not isinstance(query, str) or not isinstance(response, str):
        raise TypeError("query and response must be strings")

    # Calculate metrics
    exact = exact_match(response, reference)
    fuzzy = fuzzy_match(response, reference)
    bleu = calculate_bleu(response, reference)
    semantic = semantic_similarity(response, reference)

    return {
        "exact_match": exact,
        "fuzzy_match": fuzzy,
        "bleu_score": bleu,
        "semantic_similarity": semantic
    }

# Test on example
example_response = "To make carbonara, you need eggs, cheese, and guanciale."
example_reference = "Carbonara requires eggs, pecorino cheese, and guanciale."

similarities = calculate_all_similarities(
    query="How do I make carbonara?",
    response=example_response,
    reference=example_reference
)

print("üìä Similarity Metrics:")
for metric, value in similarities.items():
    print(f"  {metric}: {value:.3f}")

# Assertion: Fuzzy should be between exact and semantic
assert similarities["exact_match"] <= similarities["fuzzy_match"] <= 1.0
assert 0.0 <= similarities["semantic_similarity"] <= 1.0
print("\n‚úÖ Validation passed")
```

---

## Final Cell: Summary and Next Steps (Markdown)

```markdown
## Summary

### What We've Accomplished

‚úÖ [Accomplishment 1]
‚úÖ [Accomplishment 2]
‚úÖ [Accomplishment 3]
‚úÖ [Accomplishment 4]

### Key Metrics

- **[Metric 1]:** [Value] ([Interpretation])
- **[Metric 2]:** [Value] ([Interpretation])
- **[Metric 3]:** [Value] ([Interpretation])

### Next Steps

1. **[Next action 1]**: [Description]
2. **[Next action 2]**: [Description]
3. **[Next action 3]**: [Description]

### Key Takeaways

- ‚úÖ **[Takeaway 1]** - [Explanation]
- ‚úÖ **[Takeaway 2]** - [Explanation]
- ‚úÖ **[Takeaway 3]** - [Explanation]

---

**Tutorial Status:** ‚úÖ Complete
**Last Updated:** [YYYY-MM-DD]
**Maintainer:** AI Evaluation Course Team
```

---

## Additional Guidelines

### Assertions and Validation

Use assertions throughout to catch errors early:

```python
# Validate data types
assert isinstance(df, pd.DataFrame), "df must be a DataFrame"

# Validate data shape
assert len(df) > 0, "DataFrame is empty"
assert 'query' in df.columns, "Missing 'query' column"

# Validate value ranges
assert 0.0 <= tpr <= 1.0, f"TPR must be in [0, 1], got {tpr}"

# Validate business logic
assert tp + fn > 0, "No positive examples in dataset"

print("‚úÖ All validations passed")
```

### Progress Indicators

For loops that take >10 seconds:

```python
from tqdm.auto import tqdm  # Optional: nicer progress bars

results = []
for idx, row in enumerate(df.iterrows(), 1):
    # Process row
    result = process_row(row)
    results.append(result)

    # Progress indicator every 10 items
    if idx % 10 == 0:
        print(f"Processed {idx}/{len(df)} examples...")

print(f"\n‚úÖ Processing complete! Processed {len(results)} examples")
```

### Error Handling

For API calls and external dependencies:

```python
try:
    result = call_llm_api(query)
except Exception as e:
    print(f"‚ùå API call failed: {e}")
    result = None  # Or appropriate fallback

# Defensive: Handle None results
if result is None:
    print("‚ö†Ô∏è Skipping example due to API error")
    continue
```

---

**Template Version:** 1.0
**Last Updated:** 2025-11-09
**Applies to:** Lessons 9-11 notebooks
