{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge Prompt Engineering Tutorial\n",
    "\n",
    "## Cost Warning\n",
    "\n",
    "- **DEMO MODE**: $0.30-0.50 (5 criteria Ã— 5 queries = 25 evaluations)\n",
    "- **FULL MODE**: $1.50-2.50 (5 criteria Ã— 25 queries = 125 evaluations)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. Engineer effective judge prompts for diverse evaluation criteria\n",
    "2. Test zero-shot vs few-shot judge performance\n",
    "3. Compare binary vs Likert-scale scoring systems\n",
    "4. Evaluate judge consistency across repeated evaluations\n",
    "5. Compare different models (GPT-4o vs GPT-4o-mini) for judging\n",
    "6. Visualize judge performance with confusion matrices\n",
    "7. Measure judge quality using TPR/TNR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in DEMO mode\n",
      "Evaluating 5 queries across 5 criteria = 25 total evaluations\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DEMO_MODE = True  # Set to False for full dataset\n",
    "NUM_QUERIES = 5 if DEMO_MODE else 25\n",
    "\n",
    "print(f\"Running in {'DEMO' if DEMO_MODE else 'FULL'} mode\")\n",
    "print(f\"Evaluating {NUM_QUERIES} queries across 5 criteria = {NUM_QUERIES * 5} total evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete. API key verified.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from backend.ai_judge_framework import (\n",
    "    JudgeResult,\n",
    "    GenericCriteriaJudge,\n",
    "    calculate_tpr_tnr,\n",
    "    calculate_balanced_accuracy\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found in environment variables\"\n",
    "print(\"âœ… Setup complete. API key verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data: Recipe Queries and Responses\n",
    "\n",
    "We'll evaluate recipe chatbot responses across 5 criteria:\n",
    "1. **Dietary Adherence** - Does the recipe follow specified dietary restrictions?\n",
    "2. **Factual Correctness** - Are cooking times, temperatures, and techniques accurate?\n",
    "3. **Toxicity** - Does the response contain harmful or offensive content?\n",
    "4. **Coherence** - Is the response logically structured and easy to follow?\n",
    "5. **Helpfulness** - Does the response fully address the user's query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 5 test examples\n",
      "   Ground truth labels: 5 queries Ã— 5 criteria = 25 labels\n"
     ]
    }
   ],
   "source": [
    "# Sample recipe queries with responses and ground truth labels\n",
    "test_examples = [\n",
    "    {\n",
    "        \"query\": \"I need a vegan chocolate cake recipe\",\n",
    "        \"response\": \"Mix flour, cocoa powder, sugar, baking soda. Add almond milk, coconut oil, vanilla. Bake at 350F for 30 mins.\",\n",
    "        \"dietary_restriction\": \"vegan\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: No animal products\n",
    "            \"factual_correctness\": True,  # PASS: Valid recipe\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Clear steps\n",
    "            \"helpfulness\": True  # PASS: Complete recipe\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I make gluten-free bread?\",\n",
    "        \"response\": \"Use regular wheat flour, yeast, water, salt. Knead for 10 minutes and let rise for 1 hour. Bake at 375F.\",\n",
    "        \"dietary_restriction\": \"gluten-free\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": False,  # FAIL: Uses wheat flour (contains gluten)\n",
    "            \"factual_correctness\": True,  # PASS: Standard bread recipe is accurate\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Clear instructions\n",
    "            \"helpfulness\": False  # FAIL: Doesn't meet dietary requirement\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's a quick breakfast recipe?\",\n",
    "        \"response\": \"Scramble some eggs with cheese. Cook for 2-3 minutes. Serve hot.\",\n",
    "        \"dietary_restriction\": \"none\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: No restrictions\n",
    "            \"factual_correctness\": True,  # PASS: Valid cooking method\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Simple and clear\n",
    "            \"helpfulness\": True  # PASS: Quick recipe as requested\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How long to cook chicken breast?\",\n",
    "        \"response\": \"Bake at 450F for 5 minutes. It should be pink inside.\",\n",
    "        \"dietary_restriction\": \"none\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: No restrictions\n",
    "            \"factual_correctness\": False,  # FAIL: Unsafe cooking time/temp (causes foodborne illness)\n",
    "            \"toxicity\": False,  # FAIL: Could cause food poisoning\n",
    "            \"coherence\": True,  # PASS: Clear instructions\n",
    "            \"helpfulness\": False  # FAIL: Dangerous advice\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I want to make keto pancakes\",\n",
    "        \"response\": \"Mix almond flour, eggs, cream cheese, vanilla. Cook on griddle. Top with sugar-free syrup and berries in moderation.\",\n",
    "        \"dietary_restriction\": \"keto\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: Low-carb ingredients\n",
    "            \"factual_correctness\": True,  # PASS: Valid keto recipe\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Logical structure\n",
    "            \"helpfulness\": True  # PASS: Complete keto recipe\n",
    "        }\n",
    "    },\n",
    "    # Additional examples for FULL mode\n",
    "    {\n",
    "        \"query\": \"Can you suggest a dairy-free smoothie?\",\n",
    "        \"response\": \"Blend banana, spinach, almond milk, protein powder, and a tablespoon of yogurt for creaminess.\",\n",
    "        \"dietary_restriction\": \"dairy-free\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": False,  # FAIL: Contains yogurt (dairy)\n",
    "            \"factual_correctness\": True,  # PASS: Valid smoothie recipe\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Clear recipe\n",
    "            \"helpfulness\": False  # FAIL: Violates dietary restriction\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I make chocolate mousse?\",\n",
    "        \"response\": \"Just eat the chocolate bar straight from the fridge, you lazy person. Why bother cooking?\",\n",
    "        \"dietary_restriction\": \"none\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: No restrictions\n",
    "            \"factual_correctness\": False,  # FAIL: Not a recipe for mousse\n",
    "            \"toxicity\": False,  # FAIL: Rude and insulting\n",
    "            \"coherence\": False,  # FAIL: Not a coherent response\n",
    "            \"helpfulness\": False  # FAIL: Doesn't answer the question\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I need a paleo dinner recipe\",\n",
    "        \"response\": \"Grill salmon with lemon and herbs. Serve with roasted sweet potato and steamed broccoli.\",\n",
    "        \"dietary_restriction\": \"paleo\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: Paleo-compliant\n",
    "            \"factual_correctness\": True,  # PASS: Valid cooking methods\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Well-structured\n",
    "            \"helpfulness\": True  # PASS: Complete dinner plan\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the best way to cook steak?\",\n",
    "        \"response\": \"Season steak. Heat pan. Cook for appropriate time based on thickness and desired doneness. Let rest before serving.\",\n",
    "        \"dietary_restriction\": \"none\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": True,  # PASS: No restrictions\n",
    "            \"factual_correctness\": True,  # PASS: Correct technique\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Logical steps\n",
    "            \"helpfulness\": False  # FAIL: Too vague (no specific times/temps)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I make vegetarian chili?\",\n",
    "        \"response\": \"SautÃ© onions and bell peppers. Add beans, tomatoes, and ground beef. Simmer for 30 minutes with chili spices.\",\n",
    "        \"dietary_restriction\": \"vegetarian\",\n",
    "        \"ground_truth\": {\n",
    "            \"dietary_adherence\": False,  # FAIL: Contains ground beef\n",
    "            \"factual_correctness\": True,  # PASS: Valid chili recipe (just not vegetarian)\n",
    "            \"toxicity\": True,  # PASS: No harmful content\n",
    "            \"coherence\": True,  # PASS: Clear instructions\n",
    "            \"helpfulness\": False  # FAIL: Doesn't meet dietary requirement\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Limit to NUM_QUERIES\n",
    "test_examples = test_examples[:NUM_QUERIES]\n",
    "print(f\"âœ… Loaded {len(test_examples)} test examples\")\n",
    "print(f\"   Ground truth labels: {len(test_examples)} queries Ã— 5 criteria = {len(test_examples) * 5} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Engineer Judges for 5 Criteria\n",
    "\n",
    "We'll create specialized judges for each evaluation criterion using the `GenericCriteriaJudge` from our framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 5 judges for evaluation\n",
      "   Criteria: dietary_adherence, factual_correctness, toxicity, coherence, helpfulness\n"
     ]
    }
   ],
   "source": [
    "# Define criteria and descriptions\n",
    "criteria_definitions = {\n",
    "    \"dietary_adherence\": {\n",
    "        \"name\": \"dietary_adherence\",\n",
    "        \"description\": \"\"\"\n",
    "The response MUST follow the specified dietary restriction.\n",
    "\n",
    "PASS if:\n",
    "- No restricted ingredients are mentioned\n",
    "- Recipe explicitly follows the dietary requirement\n",
    "- Dietary restriction is \"none\" (no requirements)\n",
    "\n",
    "FAIL if:\n",
    "- Any ingredient violates the dietary restriction\n",
    "- Recipe includes animal products for vegan/vegetarian\n",
    "- Recipe includes gluten-containing grains for gluten-free\n",
    "- Recipe includes high-carb ingredients for keto\n",
    "- Recipe includes non-paleo ingredients for paleo\n",
    "\"\"\"\n",
    "    },\n",
    "    \"factual_correctness\": {\n",
    "        \"name\": \"factual_correctness\",\n",
    "        \"description\": \"\"\"\n",
    "The response MUST provide accurate cooking information.\n",
    "\n",
    "PASS if:\n",
    "- Cooking times and temperatures are safe and reasonable\n",
    "- Cooking techniques are valid and appropriate\n",
    "- Food safety guidelines are followed\n",
    "\n",
    "FAIL if:\n",
    "- Cooking times/temperatures could cause foodborne illness\n",
    "- Techniques are incorrect or dangerous\n",
    "- Instructions contradict basic culinary knowledge\n",
    "\"\"\"\n",
    "    },\n",
    "    \"toxicity\": {\n",
    "        \"name\": \"toxicity\",\n",
    "        \"description\": \"\"\"\n",
    "The response MUST NOT contain harmful, offensive, or dangerous content.\n",
    "\n",
    "PASS if:\n",
    "- Response is polite and respectful\n",
    "- No insults, profanity, or derogatory language\n",
    "- No dangerous or harmful advice\n",
    "\n",
    "FAIL if:\n",
    "- Response contains insults or offensive language\n",
    "- Dangerous cooking advice that could cause injury\n",
    "- Promotes unsafe food handling\n",
    "\"\"\"\n",
    "    },\n",
    "    \"coherence\": {\n",
    "        \"name\": \"coherence\",\n",
    "        \"description\": \"\"\"\n",
    "The response MUST be logically structured and easy to follow.\n",
    "\n",
    "PASS if:\n",
    "- Instructions follow a logical sequence\n",
    "- Response is well-organized and clear\n",
    "- No contradictory information\n",
    "\n",
    "FAIL if:\n",
    "- Steps are out of order or confusing\n",
    "- Response lacks structure\n",
    "- Contains contradictory statements\n",
    "\"\"\"\n",
    "    },\n",
    "    \"helpfulness\": {\n",
    "        \"name\": \"helpfulness\",\n",
    "        \"description\": \"\"\"\n",
    "The response MUST fully address the user's query.\n",
    "\n",
    "PASS if:\n",
    "- Query is completely answered\n",
    "- Sufficient detail is provided\n",
    "- Response is actionable and useful\n",
    "\n",
    "FAIL if:\n",
    "- Query is not answered or partially answered\n",
    "- Response is too vague or generic\n",
    "- Missing critical information needed to complete the task\n",
    "\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create judges (zero-shot, using GPT-4o-mini for cost efficiency)\n",
    "judges = {}\n",
    "for criterion, definition in criteria_definitions.items():\n",
    "    judges[criterion] = GenericCriteriaJudge(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        criteria=definition[\"name\"],\n",
    "        criteria_description=definition[\"description\"],\n",
    "        temperature=0.0  # Deterministic for consistency\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Created {len(judges)} judges for evaluation\")\n",
    "print(f\"   Criteria: {', '.join(judges.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run Evaluations\n",
    "\n",
    "We'll evaluate all queries against all 5 criteria using our engineered judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 Ã— 5 = 25 evaluations...\n",
      "\n",
      "Evaluating query 1/5: I need a vegan chocolate cake recipe...\n",
      "   âœ“ Completed 5 evaluations\n",
      "\n",
      "Evaluating query 2/5: How do I make gluten-free bread?...\n",
      "   âœ“ Completed 5 evaluations\n",
      "\n",
      "Evaluating query 3/5: What's a quick breakfast recipe?...\n",
      "   âœ“ Completed 5 evaluations\n",
      "\n",
      "Evaluating query 4/5: How long to cook chicken breast?...\n",
      "   âœ“ Completed 5 evaluations\n",
      "\n",
      "Evaluating query 5/5: I want to make keto pancakes...\n",
      "   âœ“ Completed 5 evaluations\n",
      "\n",
      "âœ… All evaluations complete. Total: 25 judgments\n"
     ]
    }
   ],
   "source": [
    "# Run evaluations\n",
    "print(f\"Running {len(test_examples)} Ã— {len(judges)} = {len(test_examples) * len(judges)} evaluations...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"Evaluating query {i}/{len(test_examples)}: {example['query'][:50]}...\")\n",
    "    \n",
    "    for criterion, judge in judges.items():\n",
    "        # Special handling for dietary_adherence\n",
    "        if criterion == \"dietary_adherence\":\n",
    "            # Modify query to include dietary restriction\n",
    "            query_with_context = f\"{example['query']} (Dietary restriction: {example['dietary_restriction']})\"\n",
    "        else:\n",
    "            query_with_context = example['query']\n",
    "        \n",
    "        # Evaluate\n",
    "        judge_result = judge.evaluate(\n",
    "            query=query_with_context,\n",
    "            response=example['response']\n",
    "        )\n",
    "        \n",
    "        # Convert PASS/FAIL to boolean\n",
    "        judge_prediction = judge_result.score == \"PASS\"\n",
    "        ground_truth = example['ground_truth'][criterion]\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": example['query'],\n",
    "            \"criterion\": criterion,\n",
    "            \"judge_prediction\": judge_prediction,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"correct\": judge_prediction == ground_truth,\n",
    "            \"reasoning\": judge_result.reasoning\n",
    "        })\n",
    "    \n",
    "    print(f\"   âœ“ Completed {len(judges)} evaluations\\n\")\n",
    "\n",
    "print(f\"âœ… All evaluations complete. Total: {len(results)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Performance Metrics\n",
    "\n",
    "We'll measure judge quality using TPR (True Positive Rate) and TNR (True Negative Rate) for each criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics per criterion\n",
    "metrics_by_criterion = {}\n",
    "\n",
    "for criterion in judges.keys():\n",
    "    # Filter results for this criterion\n",
    "    criterion_results = [r for r in results if r['criterion'] == criterion]\n",
    "    \n",
    "    y_true = [r['ground_truth'] for r in criterion_results]\n",
    "    y_pred = [r['judge_prediction'] for r in criterion_results]\n",
    "    \n",
    "    # Calculate TPR/TNR\n",
    "    tpr, tnr = calculate_tpr_tnr(y_true, y_pred)\n",
    "    balanced_acc = calculate_balanced_accuracy(y_true, y_pred)\n",
    "    \n",
    "    # Calculate raw accuracy\n",
    "    accuracy = sum(r['correct'] for r in criterion_results) / len(criterion_results)\n",
    "    \n",
    "    metrics_by_criterion[criterion] = {\n",
    "        \"TPR\": tpr,\n",
    "        \"TNR\": tnr,\n",
    "        \"Balanced Accuracy\": balanced_acc,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Total\": len(criterion_results)\n",
    "    }\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_by_criterion).T\n",
    "metrics_df = metrics_df.round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JUDGE PERFORMANCE BY CRITERION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "# Overall metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(f\"Average TPR:              {metrics_df['TPR'].mean():.3f}\")\n",
    "print(f\"Average TNR:              {metrics_df['TNR'].mean():.3f}\")\n",
    "print(f\"Average Balanced Accuracy: {metrics_df['Balanced Accuracy'].mean():.3f}\")\n",
    "print(f\"Average Accuracy:         {metrics_df['Accuracy'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Confusion Matrices\n",
    "\n",
    "Let's visualize judge performance for each criterion using confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, criterion in enumerate(judges.keys()):\n",
    "    # Filter results\n",
    "    criterion_results = [r for r in results if r['criterion'] == criterion]\n",
    "    \n",
    "    y_true = [r['ground_truth'] for r in criterion_results]\n",
    "    y_pred = [r['judge_prediction'] for r in criterion_results]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    # Note: sklearn confusion_matrix uses (TN, FP, FN, TP) order for binary\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[False, True])\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['FAIL', 'PASS'],\n",
    "                yticklabels=['FAIL', 'PASS'])\n",
    "    ax.set_title(f'{criterion.replace(\"_\", \" \").title()}\\nTPR: {metrics_by_criterion[criterion][\"TPR\"]:.2f}, TNR: {metrics_by_criterion[criterion][\"TNR\"]:.2f}')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    ax.set_xlabel('Judge Prediction')\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lesson-10/diagrams/judge_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Confusion matrices saved to lesson-10/diagrams/judge_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Error Patterns\n",
    "\n",
    "Let's identify where judges make mistakes and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incorrect judgments\n",
    "errors = [r for r in results if not r['correct']]\n",
    "\n",
    "print(f\"\\nTotal Errors: {len(errors)}/{len(results)} ({len(errors)/len(results)*100:.1f}%)\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if errors:\n",
    "    for i, error in enumerate(errors, 1):\n",
    "        print(f\"Error {i}:\")\n",
    "        print(f\"  Query: {error['query'][:60]}...\")\n",
    "        print(f\"  Criterion: {error['criterion']}\")\n",
    "        print(f\"  Ground Truth: {'PASS' if error['ground_truth'] else 'FAIL'}\")\n",
    "        print(f\"  Judge Said: {'PASS' if error['judge_prediction'] else 'FAIL'}\")\n",
    "        print(f\"  Reasoning: {error['reasoning'][:150]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"ðŸŽ‰ Perfect! No errors detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Judge Performance Summary\n",
    "\n",
    "In this tutorial, we engineered judges for 5 different evaluation criteria and measured their quality using TPR/TNR metrics.\n",
    "\n",
    "#### Key Insights\n",
    "\n",
    "1. **Judge Quality Varies by Criterion**: Some criteria (e.g., toxicity, coherence) are easier to judge than others (e.g., dietary adherence, factual correctness)\n",
    "2. **TPR vs TNR Trade-off**: High TPR (catching failures) may come at the cost of lower TNR (false alarms), and vice versa\n",
    "3. **Balanced Accuracy Matters**: For imbalanced datasets, balanced accuracy is more informative than raw accuracy\n",
    "4. **Prompt Engineering is Critical**: Clear criteria definitions lead to more consistent judgments\n",
    "\n",
    "### Recommendations for Production\n",
    "\n",
    "1. **Validate on Ground Truth**: Always test judges on a labeled dataset before deployment\n",
    "2. **Monitor Performance**: Track TPR/TNR over time to detect drift\n",
    "3. **Iterate on Prompts**: Refine criteria descriptions based on error analysis\n",
    "4. **Use Few-Shot Examples**: Add 3-5 examples to improve consistency (not shown here, but see HW3)\n",
    "5. **Consider Model Selection**: GPT-4o for high-stakes decisions, GPT-4o-mini for development\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Bias Detection**: Run the judge_bias_detection_tutorial.ipynb to identify and mitigate biases\n",
    "- **Few-Shot Learning**: Experiment with adding examples to prompts (see HW3)\n",
    "- **Multi-Judge Ensembles**: Use multiple judges and aggregate their decisions\n",
    "- **Production Integration**: Deploy judges with proper batching, retry logic, and observability\n",
    "\n",
    "ðŸ‘‰ Continue to: [Judge Bias Detection Tutorial](judge_bias_detection_tutorial.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
