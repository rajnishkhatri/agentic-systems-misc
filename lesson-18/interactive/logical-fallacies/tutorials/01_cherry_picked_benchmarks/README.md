# Tutorial 1: Cherry-Picked Benchmarks

**Fallacy Category:** Hasty Generalization
**Difficulty:** Beginner
**Estimated Time:** 20 Minutes

---

## üéØ Learning Objectives

By the end of this tutorial, you will be able to:
1.  **Define** "Cherry-Picked Benchmarks" in the context of AI evaluation.
2.  **Identify** red flags like "Single Metric Reporting" and "Perfect Scores".
3.  **Audit** a dataset to detect selection bias using Python.
4.  **Implement** a robust evaluation strategy using Stratified Sampling.

---

## üó∫Ô∏è The Journey (P√≥lya Phases)

This tutorial follows the P√≥lya Problem Solving methodology:

1.  **[Understand](01_understand.md)**
    *   What is the fallacy?
    *   See a real-world example (Gemini Demo).
    *   See a code example (The "Visa Only" Classifier).

2.  **[Plan](02_plan.md)**
    *   How do we catch it?
    *   Strategy: "Interrogate the Denominator".

3.  **[Tasks](03_tasks.md)**
    *   The Audit Checklist.
    *   Python queries to inspect the data distribution.

4.  **[Execute](04_execute.md)**
    *   The "Gotcha" moment.
    *   Using the evidence to refute a vendor claim.

5.  **[Reflect](05_reflect.md)**
    *   Key Takeaways.
    *   Pattern matching to other fallacies (Goodhart's Law).
    *   Self-Assessment Quiz.

6.  **[Counter](06_counter.md)**
    *   The Solution: Stratified Sampling.
    *   Connection to **HW3 Confusion Matrix**.

---

## üöÄ Getting Started

You can read the markdown files directly, or run the accompanying Jupyter Notebooks for an interactive experience.

[Start with Phase 1: Understand ‚Üí](01_understand.md)

