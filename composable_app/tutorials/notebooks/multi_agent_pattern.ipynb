{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 23: Multi-Agent Collaboration\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you will:\n",
    "- Understand multi-agent collaboration patterns (parallel vs. sequential)\n",
    "- Learn how ReviewerPanel orchestrates 6 specialist reviewers\n",
    "- Master parallel execution with `asyncio.gather()` for 6x speedup\n",
    "- Practice designing specialist vs. adversarial reviewer personas\n",
    "- Implement Secretary consolidation of diverse feedback\n",
    "- Profile performance gains from parallelization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python**: Intermediate proficiency with async/await\n",
    "- **LLM APIs**: Gemini API key (set in `.env` file)\n",
    "- **Composable App**: Familiarity with AbstractWriter and Article dataclass\n",
    "- **Recommended**: Complete [Reflection Pattern Tutorial](../concepts/reflection_pattern.md) first\n",
    "\n",
    "## Estimated Time\n",
    "\n",
    "- **Reading**: 20-25 minutes\n",
    "- **Hands-on exercises**: 15-20 minutes\n",
    "- **Total**: 35-45 minutes\n",
    "\n",
    "## Cost Warning\n",
    "\n",
    "‚ö†Ô∏è **API Cost Estimate**: $0.20 - $0.40\n",
    "\n",
    "This notebook makes the following LLM API calls:\n",
    "- **6 parallel reviewer calls** (~1,500 tokens input √ó 6 = 9,000 tokens)\n",
    "- **1 secretary consolidation** (~3,000 tokens input)\n",
    "- **1 writer revision** (~2,000 tokens input)\n",
    "\n",
    "**Total**: ~14,000 input tokens + ~2,000 output tokens\n",
    "\n",
    "**Gemini 2.0 Flash pricing** (as of 2025):\n",
    "- Input: $0.075 per 1M tokens\n",
    "- Output: $0.30 per 1M tokens\n",
    "\n",
    "**Estimated cost per run**: (14,000 √ó $0.075 / 1M) + (2,000 √ó $0.30 / 1M) ‚âà **$0.00165**\n",
    "\n",
    "If you run all exercises 10 times: **~$0.02**\n",
    "\n",
    "**Cost savings tip**: Use pre-executed outputs provided in this notebook to learn without API calls.\n",
    "\n",
    "## Book Reference\n",
    "\n",
    "> **Pattern 23: Multi-Agent Collaboration** is detailed in *Generative AI Design Patterns* (Lakshmanan & Hapke, O'Reilly 2025), Chapter on \"Multi-Agent Systems and Orchestration\" (pages TBD)\n",
    ">\n",
    "> Related patterns:\n",
    "> - **Pattern 18 (Reflection)**: How agents incorporate feedback\n",
    "> - **Pattern 19 (Dependency Injection)**: ReviewerPanel uses DI for agent composition\n",
    "> - **Pattern 25 (Prompt Caching)**: Reusing system prompts across reviewers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's set up our environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import replace\n",
    "\n",
    "# Add composable_app to path\n",
    "composable_app_path = Path.cwd().parent.parent\n",
    "if str(composable_app_path) not in sys.path:\n",
    "    sys.path.insert(0, str(composable_app_path))\n",
    "\n",
    "# Import composable app modules\n",
    "from composable_app.agents.article import Article\n",
    "from composable_app.agents.generic_writer_agent import (\n",
    "    Writer,\n",
    "    WriterFactory,\n",
    "    AbstractWriter,\n",
    "    GenAIWriter\n",
    ")\n",
    "from composable_app.agents.reviewer_panel import ReviewerPanel\n",
    "from composable_app.utils.prompt_service import PromptService\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üì¶ Composable app path: {composable_app_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for API key\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_path = composable_app_path / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"‚úÖ Loaded .env from {env_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No .env file found at {env_path}\")\n",
    "    print(\"   Create one with: GEMINI_API_KEY=your_key_here\")\n",
    "\n",
    "# Validate API key\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ùå GEMINI_API_KEY not found in environment\")\n",
    "    print(\"   Set it in .env file or export GEMINI_API_KEY='your_key'\")\n",
    "    raise ValueError(\"Missing GEMINI_API_KEY\")\n",
    "elif api_key.startswith(\"your_\") or len(api_key) < 10:\n",
    "    print(\"‚ùå GEMINI_API_KEY looks invalid (placeholder or too short)\")\n",
    "    print(f\"   Current value: {api_key[:20]}...\")\n",
    "    raise ValueError(\"Invalid GEMINI_API_KEY\")\n",
    "else:\n",
    "    print(f\"‚úÖ GEMINI_API_KEY found (length: {len(api_key)})\")\n",
    "    print(f\"   Key preview: {api_key[:10]}...{api_key[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test import of ReviewerPanel components\n",
    "try:\n",
    "    from composable_app.agents.reviewer_panel import (\n",
    "        ReviewerPanel,\n",
    "        GrammarReviewer,\n",
    "        MathReviewer,\n",
    "        DistrictRepReviewer,\n",
    "        ConservativeParentReviewer,\n",
    "        LiberalParentReviewer,\n",
    "        SchoolAdminReviewer,\n",
    "        SecretaryReviewer\n",
    "    )\n",
    "    print(\"‚úÖ All reviewer classes imported successfully\")\n",
    "    print(\"   - GrammarReviewer\")\n",
    "    print(\"   - MathReviewer\")\n",
    "    print(\"   - DistrictRepReviewer\")\n",
    "    print(\"   - ConservativeParentReviewer\")\n",
    "    print(\"   - LiberalParentReviewer\")\n",
    "    print(\"   - SchoolAdminReviewer\")\n",
    "    print(\"   - SecretaryReviewer\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import reviewer classes: {e}\")\n",
    "    print(\"   Make sure composable_app/agents/reviewer_panel.py exists\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ‚úÖ Setup Complete!\n\nIf all cells above executed without errors, you're ready to proceed.\n\n**What we've verified**:\n- ‚úÖ Python environment with async support\n- ‚úÖ Composable app modules accessible\n- ‚úÖ Gemini API key configured\n- ‚úÖ All reviewer classes available\n\n**Cost reminder**: Running the exercises below will cost approximately **$0.20-$0.40** in API calls.\n\n---\n\n## Part 1: Understanding Multi-Agent Collaboration\n\n### What is Multi-Agent Collaboration?\n\n**Multi-agent collaboration** is a design pattern where multiple AI agents work together to solve a problem that would be difficult for a single agent. Each agent specializes in a specific aspect of the task.\n\n**Key characteristics**:\n- üéØ **Specialization**: Each agent has a specific role (e.g., grammar review, math accuracy)\n- üîÑ **Coordination**: Agents communicate and combine their outputs\n- ‚ö° **Parallelization**: Independent agents can run simultaneously\n- üé≠ **Diversity**: Different perspectives lead to better outcomes\n\n### Real-World Analogy\n\nThink of a **manuscript review process**:\n\n```\nSingle Expert Review (Sequential)\n‚îú‚îÄ Editor reads manuscript (30 min)\n‚îú‚îÄ Technical reviewer checks facts (30 min)\n‚îú‚îÄ Style reviewer checks writing (30 min)\n‚îî‚îÄ Legal reviewer checks compliance (30 min)\nTotal: 2 hours (sequential)\n\nMulti-Expert Panel (Parallel)\n‚îú‚îÄ Editor reads manuscript ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îú‚îÄ Technical reviewer checks facts ‚îÄ‚î§\n‚îú‚îÄ Style reviewer checks writing ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚îÄ All happen simultaneously\n‚îî‚îÄ Legal reviewer checks compliance ‚îò\nTotal: 30 minutes (parallel) ‚úÖ 4x faster!\n```\n\n### Multi-Agent Patterns in LLM Applications\n\nThere are two main orchestration patterns:\n\n#### 1. Sequential Pattern (Chain of Agents)\n\n```\nAgent 1 ‚Üí Output 1 ‚Üí Agent 2 ‚Üí Output 2 ‚Üí Agent 3 ‚Üí Final Output\n```\n\n**When to use**:\n- ‚úÖ Each agent depends on previous agent's output\n- ‚úÖ Clear workflow order (e.g., Draft ‚Üí Review ‚Üí Revise)\n- ‚úÖ Example: Writing pipeline (Researcher ‚Üí Writer ‚Üí Editor)\n\n**Trade-offs**:\n- ‚ùå Slower (total time = sum of all agents)\n- ‚úÖ Simpler to debug (linear flow)\n- ‚ùå Bottleneck if one agent is slow\n\n#### 2. Parallel Pattern (Panel of Agents)\n\n```\n            ‚îå‚îÄ Agent 1 ‚Üí Output 1 ‚îÄ‚îê\nInput ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ Agent 2 ‚Üí Output 2 ‚îÄ‚îº‚îÄ‚Üí Aggregator ‚Üí Final Output\n            ‚îî‚îÄ Agent 3 ‚Üí Output 3 ‚îÄ‚îò\n```\n\n**When to use**:\n- ‚úÖ Agents are independent (don't need each other's outputs)\n- ‚úÖ Want diverse perspectives on same input\n- ‚úÖ Example: Review panel (6 reviewers evaluate same article)\n\n**Trade-offs**:\n- ‚úÖ Faster (total time = max of any single agent)\n- ‚úÖ More robust (one agent failure doesn't stop others)\n- ‚ùå More complex (need aggregation logic)\n- ‚ùå Higher API cost (multiple simultaneous calls)\n\n### ComposableApp's Multi-Agent Architecture\n\nThe **ReviewerPanel** implements the **parallel pattern**:\n\n```\n                        ‚îå‚îÄ GrammarReviewer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                        ‚îú‚îÄ MathReviewer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\nArticle (draft) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ DistrictRepReviewer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚Üí SecretaryReviewer ‚Üí Consolidated Feedback\n                        ‚îú‚îÄ ConservativeParent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n                        ‚îú‚îÄ LiberalParent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n                        ‚îî‚îÄ SchoolAdminReviewer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                        \n                        All 6 run in parallel (asyncio.gather)\n```\n\n**Why this design?**:\n1. **Independence**: Each reviewer evaluates the article independently\n2. **Diversity**: 6 different perspectives (3 specialist, 3 adversarial)\n3. **Speed**: Parallel execution = 6x faster than sequential\n4. **Robustness**: One reviewer failure doesn't stop others\n\n---\n\n### Specialist vs. Adversarial Reviewers\n\nThe ReviewerPanel uses two types of agents:\n\n#### Specialist Reviewers (Domain Experts)\n\n**Purpose**: Ensure technical accuracy and quality\n\n1. **GrammarReviewer**\n   - Checks spelling, grammar, punctuation\n   - Ensures grade-appropriate reading level\n   - Validates sentence structure\n\n2. **MathReviewer**\n   - Verifies mathematical accuracy\n   - Checks formulas and calculations\n   - Ensures proper notation\n\n3. **DistrictRepReviewer**\n   - Validates alignment with curriculum standards\n   - Checks learning objectives coverage\n   - Ensures educational value\n\n**Example feedback**:\n```\nGrammar: \"Paragraph 2, sentence 3: 'photosynthesis occur' should be 'photosynthesis occurs'\"\nMath: \"Equation on line 5 is incorrect: should be E=mc¬≤ not E=mc\"\nDistrict: \"Missing connection to 9th grade biology standard BS.9.2.1\"\n```\n\n#### Adversarial Reviewers (Stakeholder Perspectives)\n\n**Purpose**: Identify potential controversies or concerns\n\n4. **ConservativeParentReviewer**\n   - Flags content that conservative parents might object to\n   - Checks for political bias (left-leaning)\n   - Ensures traditional values alignment\n\n5. **LiberalParentReviewer**\n   - Flags content that liberal parents might object to\n   - Checks for political bias (right-leaning)\n   - Ensures inclusive language\n\n6. **SchoolAdminReviewer**\n   - Identifies potential legal/liability issues\n   - Checks compliance with school policies\n   - Flags controversial topics\n\n**Example feedback**:\n```\nConservative: \"Evolution section may be controversial in some districts. Consider adding 'theory' qualifier.\"\nLiberal: \"Gender roles example in paragraph 4 may be seen as stereotypical. Use more inclusive examples.\"\nSchool Admin: \"Chemistry experiment on page 3 requires safety warning and parental consent.\"\n```\n\n---\n\n### Why Use Adversarial Reviewers?\n\n**Traditional approach**: Only use expert reviewers\n\n```\nArticle ‚Üí Grammar + Math + Curriculum Review ‚Üí Publish\n‚Üì\nControversy erupts after publication ‚ùå\n```\n\n**Multi-agent approach**: Include adversarial perspectives\n\n```\nArticle ‚Üí Specialist Reviews + Adversarial Reviews ‚Üí Identify issues early ‚Üí Fix before publish\n‚Üì\nMinimal controversy, broader acceptance ‚úÖ\n```\n\n**Benefits**:\n- üõ°Ô∏è **Risk mitigation**: Catch controversial content before publication\n- üéØ **Broader appeal**: Address concerns from diverse stakeholders\n- üìä **Better decisions**: Surface trade-offs between competing values\n- üí∞ **Cost savings**: Fixing issues pre-publication is cheaper than post-publication crisis management\n\n**Real-world example**:\nA science textbook included an experiment that was educationally sound but required chemicals unavailable to low-income schools. The **SchoolAdminReviewer** flagged this equity issue, leading to an alternative experiment that all schools could perform.\n\n---\n\n### Performance: Sequential vs. Parallel\n\nLet's analyze the performance difference:\n\n**Sequential execution** (one reviewer at a time):\n```python\ntotal_time = 0\nfor reviewer in reviewers:\n    start = time.time()\n    feedback = await reviewer.review(article)\n    elapsed = time.time() - start\n    total_time += elapsed\n    \n# If each reviewer takes 5 seconds:\n# Total = 6 √ó 5s = 30 seconds\n```\n\n**Parallel execution** (all reviewers simultaneously):\n```python\nstart = time.time()\nfeedbacks = await asyncio.gather(\n    reviewer1.review(article),\n    reviewer2.review(article),\n    reviewer3.review(article),\n    reviewer4.review(article),\n    reviewer5.review(article),\n    reviewer6.review(article),\n)\ntotal_time = time.time() - start\n\n# Total = max(5s) = 5 seconds\n# Speedup: 30s / 5s = 6x faster! üöÄ\n```\n\n**When parallelization helps**:\n- ‚úÖ **I/O-bound operations** (LLM API calls, database queries)\n- ‚úÖ **Independent tasks** (reviewers don't need each other's output)\n- ‚úÖ **Multiple agents** (more agents = more speedup)\n\n**When it doesn't help**:\n- ‚ùå **CPU-bound operations** (local computation already maxes CPU)\n- ‚ùå **Dependent tasks** (agent 2 needs agent 1's output)\n- ‚ùå **Rate limits** (API restricts concurrent requests)\n\n---\n\n### Secretary Pattern: Consolidating Feedback\n\nAfter 6 reviewers provide feedback, we need to **consolidate** it into actionable guidance for the writer.\n\n**Problem**: Without consolidation, writer receives:\n```\nGrammar: \"Fix 12 spelling errors\"\nMath: \"Equation 3 is wrong\"\nDistrict: \"Add learning objective\"\nConservative: \"Remove controversial example\"\nLiberal: \"Add inclusive language\"\nAdmin: \"Add safety warning\"\n\nWriter: \"I have 6 different priorities. Which should I focus on first?\" üòï\n```\n\n**Solution**: **SecretaryReviewer** consolidates feedback\n\n**Secretary's role**:\n1. **Synthesize**: Combine related feedback (e.g., grammar + style)\n2. **Prioritize**: Rank feedback by importance (critical errors first)\n3. **Resolve conflicts**: When reviewers disagree, provide balanced guidance\n4. **Simplify**: Convert technical feedback into clear action items\n\n**Example consolidated feedback**:\n```\nCRITICAL (Fix immediately):\n- Equation in paragraph 3 is mathematically incorrect (MathReviewer)\n- Missing safety warning for chemistry experiment (SchoolAdminReviewer)\n\nHIGH PRIORITY (Fix before review):\n- 12 spelling/grammar errors throughout (GrammarReviewer)\n- Learning objective not explicitly stated (DistrictRepReviewer)\n\nOPTIONAL (Consider for revision):\n- Evolution section may need \"theory\" qualifier for broader acceptance (ConservativeParent)\n- Gender example could be more inclusive (LiberalParent)\n```\n\n**Implementation** (we'll see the code later):\n```python\nsecretary = SecretaryReviewer()\nconsolidated = await secretary.consolidate(\n    article=article,\n    reviews=[grammar_feedback, math_feedback, ...]\n)\n```\n\n---\n\n### Key Takeaways\n\n‚úÖ **Multi-agent collaboration** = multiple specialized agents working together\n‚úÖ **Parallel pattern** = agents run simultaneously (faster, more robust)\n‚úÖ **Sequential pattern** = agents run one after another (simpler, dependent tasks)\n‚úÖ **Specialist reviewers** = domain experts (grammar, math, curriculum)\n‚úÖ **Adversarial reviewers** = stakeholder perspectives (parents, admin)\n‚úÖ **Secretary pattern** = consolidates diverse feedback into actionable guidance\n‚úÖ **Performance**: Parallel = 6x faster for 6 independent agents\n\n**Next**: We'll see the actual ReviewerPanel implementation in code!\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "# Performance Comparison Summary\nimport pandas as pd\n\n# Collect all metrics\nall_metrics = [metrics_sequential, metrics_parallel, metrics_batched]\n\n# Create comparison table\ncomparison_data = []\nfor m in all_metrics:\n    comparison_data.append({\n        \"Strategy\": m.strategy,\n        \"Total Time (s)\": f\"{m.total_time:.2f}\",\n        \"Reviews\": m.num_reviews,\n        \"Speedup\": f\"{m.speedup:.2f}x\",\n        \"Throughput (rev/s)\": f\"{m.throughput:.2f}\"\n    })\n\ndf = pd.DataFrame(comparison_data)\n\nprint(\"PERFORMANCE COMPARISON SUMMARY\")\nprint(\"=\" * 70)\nprint(df.to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\nüìä Key Insights:\")\nprint(f\"  1. Parallel execution is {metrics_parallel.speedup:.1f}x faster than sequential\")\nprint(f\"  2. Batched execution balances speed ({metrics_batched.speedup:.1f}x) with rate limit safety\")\nprint(f\"  3. All strategies complete the same {metrics_sequential.num_reviews} reviews\")\nprint(f\"  4. Cost is IDENTICAL for all strategies (same number of API calls)\")\n\nprint(\"\\nüí° When to use each strategy:\")\nprint(\"  ‚Ä¢ Sequential: Debugging, strict rate limits, low concurrency environment\")\nprint(\"  ‚Ä¢ Parallel: Production with generous rate limits, minimize latency\")\nprint(\"  ‚Ä¢ Batched: Production with strict rate limits, balance speed and safety\")\n\nprint(\"\\n‚ö†Ô∏è  Rate Limit Considerations:\")\nprint(\"  ‚Ä¢ Gemini API: 60 requests/minute (1 request/second)\")\nprint(\"  ‚Ä¢ 6 parallel reviewers = instant burst, then wait\")\nprint(\"  ‚Ä¢ Batched (2 at a time) = smoother rate limit compliance\")\n\nprint(\"\\nüéØ ComposableApp's Choice:\")\nprint(\"  Current implementation uses SEQUENTIAL execution\")\nprint(\"  Reason: Simplifies debugging, predictable token usage, avoids rate limits\")\nprint(\"  Trade-off: Slower user experience (30s vs 5s for 6 reviewers)\")\n\nprint(\"\\n‚úÖ Exercise: Modify `do_first_round_reviews()` to use `asyncio.gather()`\")\nprint(\"  See: composable_app/agents/reviewer_panel.py:100-131\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Part 8: Self-Assessment and Next Steps\n\nTest your understanding of multi-agent collaboration patterns with these questions.\n\n---\n\n### Self-Assessment Questions\n\n<details>\n<summary><strong>Question 1: When should you use parallel vs. sequential agent execution?</strong></summary>\n\n**Answer**:\n\n**Use Parallel Execution when**:\n- ‚úÖ Tasks are **independent** (agents don't need each other's outputs)\n- ‚úÖ Operations are **I/O-bound** (LLM API calls, database queries)\n- ‚úÖ You want to **minimize latency** (user waiting time)\n- ‚úÖ API rate limits allow concurrent requests\n\n**Example**: Round 1 reviews where each reviewer evaluates independently\n\n**Use Sequential Execution when**:\n- ‚úÖ Tasks are **dependent** (agent N needs output from agent N-1)\n- ‚úÖ You need **predictable order** for debugging\n- ‚úÖ **Strict rate limits** don't allow concurrent requests\n- ‚úÖ Want to **minimize complexity** in development\n\n**Example**: Round 2 reviews where reviewers see Round 1 feedback\n\n**Performance trade-off**:\n- Parallel: 6 agents = max(5s) = 5 seconds\n- Sequential: 6 agents = 6 √ó 5s = 30 seconds\n\n**Cost**: Same (6 API calls either way)\n</details>\n\n<details>\n<summary><strong>Question 2: What does `return_exceptions=True` do in `asyncio.gather()`, and why is it important?</strong></summary>\n\n**Answer**:\n\n**Without `return_exceptions=True` (default)**:\n```python\nresults = await asyncio.gather(task1, task2, task3)\n# If task2 fails, entire gather() raises exception\n# task3 may be cancelled, you lose all results\n```\n\n**With `return_exceptions=True`**:\n```python\nresults = await asyncio.gather(task1, task2, task3, return_exceptions=True)\n# If task2 fails, gather() returns [result1, Exception, result3]\n# You get partial results and can handle failure gracefully\n```\n\n**Why it's important for multi-agent systems**:\n- üõ°Ô∏è **Robustness**: One reviewer failure doesn't stop others\n- üìä **Partial results**: Get 5 out of 6 reviews instead of 0\n- üêõ **Better debugging**: Can inspect which specific agent failed and why\n- üéØ **Production-ready**: Graceful degradation instead of complete failure\n\n**Best practice**:\n```python\nresults = await asyncio.gather(*tasks, return_exceptions=True)\nsuccessful = [r for r in results if not isinstance(r, Exception)]\nfailed = [r for r in results if isinstance(r, Exception)]\n\nfor exc in failed:\n    logger.error(f\"Agent failed: {exc}\")\n```\n</details>\n\n<details>\n<summary><strong>Question 3: What are the key differences between specialist and adversarial reviewers?</strong></summary>\n\n**Answer**:\n\n| Aspect | Specialist Reviewers | Adversarial Reviewers |\n|--------|---------------------|----------------------|\n| **Goal** | Technical accuracy and quality | Identify controversies and concerns |\n| **Criteria** | Objective, measurable | Subjective, value-based |\n| **Examples** | Grammar, Math, District Rep | Conservative/Liberal Parents, Admin |\n| **Feedback type** | \"Spelling error on line 5\" | \"This topic may upset some parents\" |\n| **Conflicts** | Rarely conflict with each other | Often conflict with each other |\n| **Purpose** | Ensure correctness | Surface stakeholder concerns early |\n\n**Why include both**:\n- ‚úÖ **Specialist alone**: Content is accurate but may cause controversy\n- ‚úÖ **Adversarial alone**: Content is safe but may have factual errors\n- ‚úÖ **Both together**: Content is accurate AND broadly acceptable\n\n**Real-world value**:\nAdversarial reviewers catch issues that would cause problems after publication, when fixing is much more expensive.\n</details>\n\n<details>\n<summary><strong>Question 4: What are the three main responsibilities of the Secretary consolidation pattern?</strong></summary>\n\n**Answer**:\n\nThe Secretary (meta-reviewer) has three main responsibilities:\n\n**1. Synthesize and Prioritize**\n```\nInput (raw reviews):\n- GrammarReviewer: \"12 spelling errors\"\n- MathReviewer: \"Equation on line 15 is wrong\"\n- DistrictRep: \"Article is too long\"\n\nOutput (prioritized):\nCRITICAL: Fix equation (affects learning)\nHIGH: Fix 12 spelling errors (professionalism)\nMODERATE: Reduce length (budget concern)\n```\n\n**2. Resolve Conflicts**\n```\nInput (conflicting reviews):\n- ConservativeParent: \"Remove paragraph 3\"\n- LiberalParent: \"Expand paragraph 3\"\n\nOutput (balanced resolution):\nREQUIRES DISCUSSION: Keep paragraph 3 but revise for balance\n- Use neutral language (addresses conservative concern)\n- Maintain context (addresses liberal concern)\n```\n\n**3. Simplify into Action Items**\n```\nInput (technical reviews):\n- GrammarReviewer: \"Passive voice in sentences 2, 5, 8...\"\n- MathReviewer: \"Use √ó not * for multiplication\"\n\nOutput (actionable):\nTO-DO:\n1. Change line 15: \"5 * 3 = 15\" ‚Üí \"5 √ó 3 = 15\"\n2. Convert passive voice in 3 sentences to active voice\n```\n\n**Why this matters**:\nWithout consolidation, writer gets 6 potentially conflicting perspectives and doesn't know where to start. With consolidation, writer gets one clear set of prioritized, conflict-resolved action items.\n</details>\n\n<details>\n<summary><strong>Question 5: How do you design an effective reviewer persona?</strong></summary>\n\n**Answer**:\n\nApply these **4 design principles**:\n\n**Principle 1: Clear, Focused Mandate**\n```\n‚úÖ Good: \"Check spelling, grammar, punctuation. Report line numbers.\"\n‚ùå Bad: \"Review the article for quality\"\n```\n\n**Principle 2: Specific Values and Priorities**\n```\n‚úÖ Good: \"Focus on Western civilization, downplay colonialism's negative aspects\"\n‚ùå Bad: \"Care about traditional values\"\n```\n\n**Principle 3: Representative of Real Stakeholders**\n```\n‚úÖ Good: Based on actual parent feedback patterns in school districts\n‚ùå Bad: Stereotype not grounded in real-world concerns\n```\n\n**Principle 4: Creates Productive Tension**\n```\n‚úÖ Good: Conservative wants traditional algorithms, Liberal wants real-world contexts\n   (Both valid, forces better solution that balances both)\n‚ùå Bad: Reviewer A wants short content, Reviewer B wants long content\n   (Artificial conflict, no underlying values)\n```\n\n**Testing your persona**:\n- Can you explain WHY this reviewer would object to X?\n- Does this reviewer catch issues others would miss?\n- Would real stakeholders recognize themselves in this persona?\n- Does disagreement with other reviewers lead to better content?\n</details>\n\n<details>\n<summary><strong>Question 6: What's the performance and cost impact of parallel execution?</strong></summary>\n\n**Answer**:\n\n**Performance Impact**:\n```\nSequential: 6 reviewers √ó 5s each = 30 seconds\nParallel:   6 reviewers, max(5s)  = 5 seconds\nSpeedup:    30s / 5s = 6x faster ‚úÖ\n```\n\n**Cost Impact**:\n```\nSequential: 6 API calls √ó 2,000 tokens = 12,000 tokens\nParallel:   6 API calls √ó 2,000 tokens = 12,000 tokens\nCost difference: $0.00 (same number of API calls) ‚úÖ\n```\n\n**Key insight**: Parallel execution is **faster** but **not cheaper**. You're making the same API calls, just concurrently instead of sequentially.\n\n**Trade-offs**:\n\n| Aspect | Sequential | Parallel |\n|--------|-----------|----------|\n| **Speed** | Slow (30s) | Fast (5s) |\n| **Cost** | Same | Same |\n| **Complexity** | Simple | More complex |\n| **Debugging** | Easy (linear) | Harder (concurrent) |\n| **Rate limits** | Gentle | May hit limits |\n| **User experience** | Poor (long wait) | Good (short wait) |\n\n**Production recommendation**:\n- Use **parallel** for independent tasks (Round 1 reviews)\n- Use **sequential** for dependent tasks (Round 2 reviews)\n- Use **batched parallel** if rate limits are strict\n</details>\n\n<details>\n<summary><strong>Question 7: Why is the two-round review pattern beneficial?</strong></summary>\n\n**Answer**:\n\n**Round 1: Independent Reviews**\n```python\nreviews_so_far = []  # Empty - no knowledge of others\nfor reviewer in panel:\n    review = await reviewer.review(article, reviews_so_far)\n```\n\n**Benefits**:\n- ‚úÖ Prevents groupthink (reviewers not influenced by others)\n- ‚úÖ Captures true diverse perspectives\n- ‚úÖ Each reviewer focuses on their specialty without bias\n\n**Round 2: Informed Reviews**\n```python\nreviews_so_far = round1_reviews  # Can see what others said\nfor reviewer in panel:\n    review = await reviewer.review(article, reviews_so_far)\n```\n\n**Benefits**:\n- ‚úÖ Reviewers can respond to each other's points\n- ‚úÖ Conflicts surface explicitly (conservative vs liberal)\n- ‚úÖ More nuanced feedback (\"I agree with Grammar, but...\")\n- ‚úÖ Cross-domain insights (math reviewer notices grammar issue)\n\n**Example of Round 2 value**:\n```\nRound 1:\n- MathReviewer: \"Equation is correct\"\n- GrammarReviewer: \"No issues\"\n\nRound 2 (after seeing each other):\n- MathReviewer: \"I see Grammar didn't mention it, but the equation \n  notation is technically correct but confusing for 9th graders. \n  Consider simplifying.\"\n```\n\n**Trade-off**: 2x the API calls, but significantly better feedback quality.\n</details>\n\n<details>\n<summary><strong>Question 8: How do you handle rate limits in multi-agent systems?</strong></summary>\n\n**Answer**:\n\n**Problem**: Sending 6 parallel requests may exceed API rate limits\n\n**Gemini API limits** (2025):\n- Free tier: 15 requests/minute\n- Paid tier: 60 requests/minute\n\n**Solution 1: Batched Parallel Execution**\n```python\nbatch_size = 2  # Adjust based on your API tier\nbatches = [reviewers[i:i+batch_size] for i in range(0, len(reviewers), batch_size)]\n\nall_reviews = []\nfor batch in batches:\n    tasks = [r.review(article) for r in batch]\n    batch_reviews = await asyncio.gather(*tasks, return_exceptions=True)\n    all_reviews.extend(batch_reviews)\n    await asyncio.sleep(1.0)  # Small delay between batches\n\n# Result: 3 batches √ó 3.5s = ~10.5s (vs 30s sequential, 5s full parallel)\n```\n\n**Solution 2: Semaphore (Concurrency Control)**\n```python\nsemaphore = asyncio.Semaphore(10)  # Max 10 concurrent\n\nasync def review_with_limit(reviewer, article):\n    async with semaphore:\n        return await reviewer.review(article)\n\ntasks = [review_with_limit(r, article) for r in reviewers]\nreviews = await asyncio.gather(*tasks, return_exceptions=True)\n```\n\n**Solution 3: Exponential Backoff (Retry on 429)**\n```python\nasync def review_with_retry(reviewer, article, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await reviewer.review(article)\n        except RateLimitError:\n            if attempt == max_retries - 1:\n                raise\n            wait_time = 2 ** attempt  # 1s, 2s, 4s\n            await asyncio.sleep(wait_time)\n```\n\n**Best practice**: Use batching + backoff together for production systems.\n</details>\n\n---\n\n### Related Patterns\n\nThe Multi-Agent Collaboration pattern (Pattern 23) works best when combined with:\n\n- **Pattern 18 (Reflection)**: How agents incorporate feedback\n  - Writer uses consolidated review to revise article\n  - Iterative improvement through multiple review cycles\n  - **Code**: `composable_app/agents/generic_writer_agent.py:74-89` (revise_article)\n\n- **Pattern 19 (Dependency Injection)**: ReviewerPanel uses DI\n  - AbstractWriter interface allows swapping writer implementations\n  - WriterFactory creates writers based on type\n  - **Tutorial**: [`architecture_deep_dive.md`](../concepts/architecture_deep_dive.md)\n\n- **Pattern 25 (Prompt Caching)**: Reuse system prompts\n  - Each reviewer's system prompt is loaded once and reused\n  - Significant cost savings for multi-round reviews\n  - **Code**: `composable_app/utils/prompt_service.py`\n\n- **Pattern 32 (Guardrails)**: Input validation before review\n  - Validate article meets basic requirements before sending to panel\n  - Catch inappropriate content early (before expensive 6-agent review)\n  - **Tutorial**: [`llm_as_judge_tutorial.ipynb`](llm_as_judge_tutorial.ipynb)\n\n---\n\n### Book Reference\n\n> **Pattern 23: Multi-Agent Collaboration** is detailed in:\n>\n> *Generative AI Design Patterns* by Valliappa Lakshmanan and Martin Hapke (O'Reilly, 2025)\n>\n> **Chapter**: Multi-Agent Systems and Orchestration (pages TBD)\n>\n> **Key concepts covered**:\n> - Parallel vs. sequential agent orchestration\n> - Specialist and adversarial agent design\n> - Consolidation and aggregation patterns\n> - Performance optimization with async/await\n> - Cost-effective multi-agent architectures\n\n**Related chapters**:\n- Chapter 6: RAG pattern (retrieval before generation)\n- Chapter 8: Reflection pattern (incorporating feedback)\n- Chapter 9: Dependency injection (modular agent design)\n\n---\n\n### Next Steps\n\n**To practice multi-agent patterns**:\n\n1. **Modify ReviewerPanel for parallel execution**\n   - Exercise: Update `do_first_round_reviews()` to use `asyncio.gather()`\n   - File: `composable_app/agents/reviewer_panel.py:100-131`\n   - Test: Run performance comparison (should see 5-6x speedup)\n\n2. **Design your own reviewer persona**\n   - Exercise: Create a \"STEM Equity Advocate\" reviewer\n   - Requirements: Check for diverse examples, accessible language, low-cost experiments\n   - Test: Run on sample article about physics or chemistry\n\n3. **Implement batched execution**\n   - Exercise: Add semaphore-based rate limiting to ReviewerPanel\n   - Constraint: Max 2 concurrent reviewer calls\n   - Measure: Compare timing (should be between sequential and full parallel)\n\n4. **Enhance secretary consolidation**\n   - Exercise: Improve secretary's system prompt to better resolve conflicts\n   - Test: Create article with known conservative/liberal tension\n   - Evaluate: Does secretary provide balanced, actionable guidance?\n\n**To deepen understanding**:\n\n- ‚úÖ **Complete tutorial**: [`reflection_pattern.md`](../concepts/reflection_pattern.md)\n  - Learn how writers incorporate consolidated feedback\n  \n- ‚úÖ **Complete tutorial**: [`architecture_deep_dive.md`](../concepts/architecture_deep_dive.md)\n  - Understand dependency injection used in ReviewerPanel\n\n- ‚úÖ **Explore codebase**: Read `composable_app/agents/reviewer_panel.py`\n  - Study full implementation with error handling\n  - See how logging and evaluation recording work\n\n- ‚úÖ **Real-world application**: Use ReviewerPanel in your own domain\n  - Replace curriculum reviewers with your domain experts\n  - Design adversarial reviewers for your stakeholders\n  - Test on real content from your application\n\n---\n\n### Congratulations! üéâ\n\nYou've completed the Multi-Agent Collaboration pattern tutorial. You now understand:\n\n‚úÖ Parallel vs. sequential orchestration patterns\n‚úÖ How to design specialist and adversarial reviewers\n‚úÖ Secretary consolidation for conflict resolution\n‚úÖ Performance optimization with `asyncio.gather()`\n‚úÖ Common pitfalls and production best practices\n‚úÖ Cost and rate limit considerations\n\n**Your multi-agent systems are now production-ready!**\n\n---\n\n### Feedback and Questions\n\nHave questions or suggestions? Please:\n- üìù Open an issue: [github.com/composable-app/issues](https://github.com/)\n- üí¨ Join discussions: [github.com/composable-app/discussions](https://github.com/)\n- üìß Email the authors: [provided in book]\n\n**Tutorial changelog**: See [`TUTORIAL_CHANGELOG.md`](../../TUTORIAL_CHANGELOG.md) for updates when code changes.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 7: Common Pitfalls and How to Avoid Them\n\nWhen implementing multi-agent systems with parallel execution, several common pitfalls can cause issues. Here's how to recognize and avoid them.\n\n### Pitfall 1: Not Using `return_exceptions=True` in asyncio.gather()\n\n**Problem**: By default, `asyncio.gather()` raises the first exception and cancels remaining tasks.\n\n```python\n# ‚ùå Bad: One failure stops everything\nreviews = await asyncio.gather(\n    reviewer1.review(article),\n    reviewer2.review(article),  # If this fails...\n    reviewer3.review(article),  # ...this never runs\n)\n```\n\n**Consequence**: If one reviewer fails (API timeout, rate limit, bug), you lose ALL reviews.\n\n**Solution**: Use `return_exceptions=True` to get partial results:\n\n```python\n# ‚úÖ Good: Continue despite failures\nreviews = await asyncio.gather(\n    reviewer1.review(article),\n    reviewer2.review(article),  # If this fails...\n    reviewer3.review(article),  # ...this still runs!\n    return_exceptions=True\n)\n\n# Filter out exceptions\nsuccessful_reviews = [r for r in reviews if not isinstance(r, Exception)]\nfailed_reviews = [r for r in reviews if isinstance(r, Exception)]\n\n# Log failures for debugging\nfor exc in failed_reviews:\n    logger.error(f\"Reviewer failed: {exc}\")\n```\n\n**Code reference**: See demo in cell above comparing with/without `return_exceptions=True`\n\n---\n\n### Pitfall 2: Poor Reviewer Persona Design\n\n**Problem**: Vague or overlapping reviewer mandates lead to redundant or low-quality feedback.\n\n**Bad example**:\n```python\n# ‚ùå Vague, overlapping roles\nReviewer1: \"Review the article for quality\"\nReviewer2: \"Check if the article is good\"\nReviewer3: \"Evaluate the content\"\n```\n\n**Result**: All three reviewers give similar generic feedback like \"looks good\" or \"needs improvement.\"\n\n**Solution**: Give each reviewer a **specific, focused mandate**:\n\n```python\n# ‚úÖ Clear, non-overlapping roles\nGrammarReviewer: \"Check spelling, grammar, punctuation. Report specific errors with line numbers.\"\nMathReviewer: \"Verify all equations, calculations, and mathematical notation for accuracy.\"\nDistrictRep: \"Ensure alignment with curriculum standards and appropriate length for printing budget.\"\n```\n\n**Design principles** (from Part 4):\n1. ‚úÖ Clear, focused mandate (not \"check quality\")\n2. ‚úÖ Specific values and priorities (what does this reviewer care about?)\n3. ‚úÖ Representative of real stakeholders (not stereotypes)\n4. ‚úÖ Creates productive tension with other reviewers (not artificial conflicts)\n\n---\n\n### Pitfall 3: Ignoring Rate Limits\n\n**Problem**: Sending too many parallel requests triggers API rate limits.\n\n**Bad example**:\n```python\n# ‚ùå Sends 100 requests instantly\nreviewers = [ReviewerAgent(f\"Reviewer{i}\") for i in range(100)]\nreviews = await asyncio.gather(*[r.review(article) for r in reviewers])\n# Result: API returns 429 Too Many Requests, many reviews fail\n```\n\n**Gemini API limits** (as of 2025):\n- Free tier: 15 requests/minute\n- Paid tier: 60 requests/minute\n\n**Solution 1: Batched parallel execution** (shown in Part 6):\n```python\n# ‚úÖ Process in batches to respect rate limits\nbatch_size = 10  # Adjust based on your API tier\nbatches = [reviewers[i:i+batch_size] for i in range(0, len(reviewers), batch_size)]\n\nall_reviews = []\nfor batch in batches:\n    tasks = [r.review(article) for r in batch]\n    batch_reviews = await asyncio.gather(*tasks, return_exceptions=True)\n    all_reviews.extend(batch_reviews)\n    await asyncio.sleep(1.0)  # Small delay between batches\n```\n\n**Solution 2: Use semaphore for concurrency control**:\n```python\n# ‚úÖ Limit concurrent requests with semaphore\nsemaphore = asyncio.Semaphore(10)  # Max 10 concurrent requests\n\nasync def review_with_limit(reviewer, article):\n    async with semaphore:  # Acquire semaphore\n        return await reviewer.review(article)\n    # Semaphore released automatically\n\ntasks = [review_with_limit(r, article) for r in reviewers]\nreviews = await asyncio.gather(*tasks, return_exceptions=True)\n```\n\n---\n\n### Pitfall 4: Secretary Doesn't Resolve Conflicts\n\n**Problem**: Secretary just concatenates feedback without prioritization or conflict resolution.\n\n**Bad consolidation**:\n```\nConservativeParent: Remove paragraph 3\nLiberalParent: Expand paragraph 3\n\nSecretary consolidation:\n- Remove paragraph 3 (ConservativeParent)\n- Expand paragraph 3 (LiberalParent)\n\nWriter: \"These contradict each other! What should I do?\" üòï\n```\n\n**Solution**: Secretary must **explicitly resolve conflicts** with balanced guidance:\n\n**Good consolidation**:\n```\nREQUIRES DISCUSSION: Paragraph 3 feedback (conflicting)\n\nConservative parent concern: Content may be controversial\nLiberal parent concern: Context is educationally important\n\nRECOMMENDATION: Keep paragraph but revise for balance:\n1. Use neutral, factual language (not judgmental)\n2. Present multiple perspectives (not just one side)\n3. Keep length moderate (addresses both concerns)\n\nThis approach maintains educational value while respecting diverse community values.\n```\n\n**Implementation tip**: In secretary's system prompt, explicitly instruct it to:\n- Identify conflicts: \"When reviewers disagree, label it REQUIRES DISCUSSION\"\n- Analyze both perspectives: \"Explain each reviewer's concern\"\n- Propose balanced solutions: \"Recommend approach that addresses both concerns\"\n\n---\n\n### Pitfall 5: No Logging or Observability\n\n**Problem**: When something goes wrong, you have no visibility into what happened.\n\n**Bad example**:\n```python\n# ‚ùå No logging\nreviews = await asyncio.gather(*review_tasks)\nconsolidated = await secretary.consolidate(reviews)\nreturn consolidated  # If something fails, no way to debug\n```\n\n**Solution**: Log all agent interactions for debugging and evaluation:\n\n```python\n# ‚úÖ Comprehensive logging\nimport logging\nfrom composable_app.utils import save_for_eval as evals\n\nlogger = logging.getLogger(__name__)\n\n# Log individual reviews\nfor reviewer, review in reviews:\n    await evals.record_ai_response(\n        f\"{reviewer.name}_review\",\n        ai_input={\"article\": article, \"topic\": topic},\n        ai_response=review\n    )\n    logger.info(f\"{reviewer.name} completed review ({len(review)} chars)\")\n\n# Log consolidation\nawait evals.record_ai_response(\n    \"consolidated_review\",\n    ai_input={\"reviews\": reviews},\n    ai_response=consolidated\n)\n\n# Log failures\nfor exc in failed_reviews:\n    logger.error(f\"Review failed: {type(exc).__name__}: {exc}\")\n```\n\n**Benefits**:\n- üìä Track which reviewers fail most often\n- üêõ Debug why secretary's consolidation is poor\n- üìà Measure review quality over time\n- üí∞ Monitor token usage and costs\n\n**Code reference**: `composable_app/agents/reviewer_panel.py:56` shows logging in `ReviewerAgent.review()`\n\n---\n\n### Pitfall 6: Sequential Execution When Parallel Would Work\n\n**Problem**: Using sequential execution for independent tasks wastes time.\n\n**Bad example**:\n```python\n# ‚ùå Sequential when agents are independent (Round 1)\nfor reviewer in review_panel:\n    review = await reviewer.review(article, reviews_so_far=[])\n    reviews.append(review)\n# Takes: 6 √ó 5s = 30 seconds\n```\n\n**When to use sequential**:\n- ‚úÖ Tasks are **dependent** (Round 2 reviews need to see Round 1 results)\n- ‚úÖ Debugging (linear execution is easier to trace)\n- ‚úÖ Strict rate limits (can't handle concurrent requests)\n\n**When to use parallel**:\n- ‚úÖ Tasks are **independent** (Round 1 reviews don't need each other)\n- ‚úÖ I/O-bound operations (LLM API calls, database queries)\n- ‚úÖ Production environment with adequate rate limits\n\n**Solution**: Use parallel for Round 1, sequential for Round 2:\n\n```python\n# ‚úÖ Round 1: Parallel (independent reviews)\nreview_tasks = [r.review(article, reviews_so_far=[]) for r in review_panel]\nround1_reviews = await asyncio.gather(*review_tasks, return_exceptions=True)\n# Takes: max(5s) = 5 seconds (6x faster!)\n\n# ‚úÖ Round 2: Sequential (reviews depend on Round 1)\nround2_reviews = []\nfor reviewer in review_panel:\n    review = await reviewer.review(article, reviews_so_far=round1_reviews)\n    round2_reviews.append(review)\n# Takes: 6 √ó 5s = 30 seconds (but necessary for correct results)\n```\n\n---\n\n### Pitfall 7: Forgetting Cost Implications\n\n**Problem**: Running many agents in parallel doesn't change API costs, but can surprise users.\n\n**Misconception**:\n```\n\"Parallel execution is 6x faster, so it must be cheaper!\"\n```\n\n**Reality**:\n- Sequential: 6 reviewers √ó 2,000 tokens = 12,000 tokens\n- Parallel: 6 reviewers √ó 2,000 tokens = 12,000 tokens\n- **Cost is the same!** You're making the same API calls, just faster.\n\n**Solution**: Always display cost warnings in notebooks and UIs:\n\n```python\n# ‚úÖ Clear cost warning\nprint(\"\"\"\n‚ö†Ô∏è  COST ESTIMATE: $0.20 - $0.40\n\nThis operation will make 6 parallel reviewer calls plus 1 secretary consolidation:\n- 6 reviewer calls: ~9,000 input tokens\n- 1 secretary call: ~3,000 input tokens\n- Total output: ~2,000 tokens\n\nEstimated cost: $0.001 (6 calls) + $0.0003 (1 call) ‚âà $0.0013 per run\n\nPress Enter to continue or Ctrl+C to cancel...\n\"\"\")\ninput()\n```\n\n**Cost tracking**:\n```python\n# Track token usage\ntotal_input_tokens = sum(r.usage.input_tokens for r in review_results)\ntotal_output_tokens = sum(r.usage.output_tokens for r in review_results)\n\nprint(f\"üí∞ Total tokens used: {total_input_tokens} input + {total_output_tokens} output\")\nprint(f\"üíµ Estimated cost: ${(total_input_tokens * 0.075 + total_output_tokens * 0.30) / 1_000_000:.4f}\")\n```\n\n---\n\n### Pitfall 8: Not Testing with Mocks First\n\n**Problem**: Developing multi-agent systems directly against live APIs is slow and expensive.\n\n**Bad workflow**:\n```\n1. Write code for 6-agent system\n2. Run against live API\n3. Wait 30 seconds per test\n4. Discover bug\n5. Fix bug\n6. Repeat (costs add up quickly)\n```\n\n**Solution**: Use mocks for development, real APIs for final testing:\n\n```python\n# ‚úÖ Development: Use mocks\nasync def mock_review(reviewer_name: str) -> str:\n    await asyncio.sleep(0.1)  # Fast\n    return f\"{reviewer_name}: Mock review feedback\"\n\n# Test your orchestration logic\nreviews = await asyncio.gather(*[mock_review(r) for r in reviewers])\nconsolidated = mock_secretary_consolidate(reviews)\n\n# ‚úÖ Production: Use real APIs\nfrom composable_app.agents.reviewer_panel import ReviewerAgent\nreviewer = ReviewerAgent(Reviewer.GRAMMAR_REVIEWER)\nreview = await reviewer.review(topic, article, [])\n```\n\n**Benefits**:\n- üöÄ Fast iteration (0.1s vs 5s per test)\n- üí∞ Zero API costs during development\n- üß™ Easy to test edge cases (simulate failures, timeouts)\n- üîß Focus on logic, not API quirks\n\n**Code reference**: See Part 6 performance profiling cells using `realistic_mock_review()`\n\n---\n\n### Checklist: Multi-Agent System Health\n\nBefore deploying your multi-agent system to production, verify:\n\n- [ ] ‚úÖ Using `return_exceptions=True` in `asyncio.gather()`\n- [ ] ‚úÖ Each reviewer has clear, specific mandate\n- [ ] ‚úÖ Rate limiting handled with batching or semaphores\n- [ ] ‚úÖ Secretary explicitly resolves conflicting feedback\n- [ ] ‚úÖ All agent interactions logged for debugging\n- [ ] ‚úÖ Parallel execution for independent tasks, sequential for dependent\n- [ ] ‚úÖ Cost warnings displayed to users\n- [ ] ‚úÖ Development workflow uses mocks, production uses real APIs\n- [ ] ‚úÖ Error handling covers API timeouts, rate limits, malformed responses\n- [ ] ‚úÖ Performance profiled (sequential vs parallel timing)\n\n---\n\n### Summary: Common Pitfalls\n\n| Pitfall | Impact | Solution |\n|---------|--------|----------|\n| No `return_exceptions=True` | One failure loses all reviews | Add `return_exceptions=True` to gather() |\n| Vague reviewer personas | Redundant, low-quality feedback | Clear, focused mandates for each reviewer |\n| Ignoring rate limits | API errors, failed reviews | Batched execution or semaphore control |\n| No conflict resolution | Confusing, contradictory guidance | Secretary must explicitly resolve conflicts |\n| No logging | Can't debug failures | Log all agent interactions with evals |\n| Sequential when parallel works | 6x slower than necessary | Use parallel for independent tasks |\n| Forgetting costs | Budget surprises | Display cost warnings, track token usage |\n| Testing against live API | Slow, expensive development | Use mocks for dev, real APIs for final test |\n\n**Next**: Self-assessment questions to test your understanding!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Strategy 3: Batched parallel execution\nprint(\"STRATEGY 3: BATCHED PARALLEL (Rate Limit Management)\")\nprint(\"=\" * 70)\nprint(\"Running reviewers in batches of 2...\\n\")\n\nstart_time = time.time()\nreviews_batched = []\n\n# Split reviewers into batches of 2\nbatch_size = 2\nbatches = [reviewers[i:i+batch_size] for i in range(0, len(reviewers), batch_size)]\n\nprint(f\"Total batches: {len(batches)}\")\nprint(f\"Batch size: {batch_size}\\n\")\n\nfor batch_num, batch in enumerate(batches, 1):\n    print(f\"Batch {batch_num}/{len(batches)}: {batch}\")\n    \n    # Run this batch in parallel\n    tasks = [realistic_mock_review(reviewer) for reviewer in batch]\n    batch_reviews = await asyncio.gather(*tasks)\n    reviews_batched.extend(batch_reviews)\n    \n    elapsed = time.time() - start_time\n    print(f\"  Batch completed. Elapsed total: {elapsed:.2f}s\\n\")\n\ntotal_time_batched = time.time() - start_time\n\n# Calculate metrics\nmetrics_batched = PerformanceMetrics(\n    strategy=\"Batched Parallel (batch=2)\",\n    total_time=total_time_batched,\n    num_reviews=len(reviews_batched),\n    speedup=total_time_seq / total_time_batched,\n    throughput=len(reviews_batched) / total_time_batched\n)\n\nprint(\"=\" * 70)\nprint(metrics_batched)\nprint(f\"\\n‚è±Ô∏è  Expected: ~10.5 seconds (3 batches √ó 3.5s average)\")\nprint(f\"‚è±Ô∏è  Actual: {total_time_batched:.2f} seconds\")\nprint(f\"\\nüöÄ SPEEDUP: {metrics_batched.speedup:.1f}x faster than sequential\")\nprint(f\"‚öñÔ∏è  TRADE-OFF: Slower than full parallel, but respects rate limits\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Strategy 3: Batched Parallel (Rate Limit Management)\n\nIn production, you may hit API rate limits if you send too many concurrent requests. A **batched approach** balances speed and rate limits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Strategy 2: Parallel execution with asyncio.gather()\nprint(\"STRATEGY 2: PARALLEL EXECUTION (asyncio.gather)\")\nprint(\"=\" * 70)\nprint(\"Running all reviewers simultaneously...\\n\")\n\nstart_time = time.time()\n\n# Create tasks for all reviewers\ntasks = [realistic_mock_review(reviewer) for reviewer in reviewers]\n\nprint(f\"Starting {len(tasks)} reviewers in parallel...\")\n\n# Execute all in parallel\nreviews_parallel = await asyncio.gather(*tasks)\n\ntotal_time_parallel = time.time() - start_time\n\n# Calculate metrics\nmetrics_parallel = PerformanceMetrics(\n    strategy=\"Parallel (asyncio.gather)\",\n    total_time=total_time_parallel,\n    num_reviews=len(reviews_parallel),\n    speedup=total_time_seq / total_time_parallel,\n    throughput=len(reviews_parallel) / total_time_parallel\n)\n\nprint(f\"All reviewers completed!\\n\")\nprint(\"=\" * 70)\nprint(metrics_parallel)\nprint(f\"\\n‚è±Ô∏è  Expected: ~3.5-5.0 seconds (max of 5 reviewers)\")\nprint(f\"‚è±Ô∏è  Actual: {total_time_parallel:.2f} seconds\")\nprint(f\"\\nüöÄ SPEEDUP: {metrics_parallel.speedup:.1f}x faster than sequential!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Strategy 2: Parallel Execution with asyncio.gather()",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Strategy 1: Sequential execution\nreviewers = [\"Grammar\", \"Math\", \"District\", \"ConservativeParent\", \"LiberalParent\"]\n\nprint(\"STRATEGY 1: SEQUENTIAL EXECUTION\")\nprint(\"=\" * 70)\nprint(\"Running reviewers one at a time...\\n\")\n\nstart_time = time.time()\nreviews_sequential = []\n\nfor i, reviewer in enumerate(reviewers, 1):\n    print(f\"[{i}/{len(reviewers)}] Starting {reviewer}...\")\n    review = await realistic_mock_review(reviewer)\n    reviews_sequential.append(review)\n    elapsed = time.time() - start_time\n    print(f\"    Completed. Elapsed total: {elapsed:.2f}s\\n\")\n\ntotal_time_seq = time.time() - start_time\n\n# Calculate metrics\nmetrics_sequential = PerformanceMetrics(\n    strategy=\"Sequential\",\n    total_time=total_time_seq,\n    num_reviews=len(reviews_sequential),\n    speedup=1.0,  # Baseline\n    throughput=len(reviews_sequential) / total_time_seq\n)\n\nprint(\"=\" * 70)\nprint(metrics_sequential)\nprint(\"\\n‚è±Ô∏è  Expected: ~17.5 seconds (5 reviewers √ó 3.5s average)\")\nprint(f\"‚è±Ô∏è  Actual: {total_time_seq:.2f} seconds\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Strategy 1: Sequential Execution (Baseline)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Performance testing configuration\nimport random\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Track performance metrics for comparison.\"\"\"\n    strategy: str\n    total_time: float\n    num_reviews: int\n    speedup: float\n    throughput: float  # reviews per second\n    \n    def __str__(self):\n        return f\"\"\"\nStrategy: {self.strategy}\n  Total time:    {self.total_time:.2f}s\n  Reviews:       {self.num_reviews}\n  Speedup:       {self.speedup:.2f}x\n  Throughput:    {self.throughput:.2f} reviews/sec\n\"\"\"\n\n# Mock reviewer with realistic API latency\nasync def realistic_mock_review(\n    reviewer_name: str,\n    min_delay: float = 2.0,\n    max_delay: float = 5.0\n) -> str:\n    \"\"\"\n    Simulate a reviewer with variable API latency.\n    \n    Real LLM APIs have variable response times depending on:\n    - Model load\n    - Prompt complexity\n    - Output length\n    - Network conditions\n    \"\"\"\n    delay = random.uniform(min_delay, max_delay)\n    await asyncio.sleep(delay)\n    return f\"{reviewer_name} review completed (took {delay:.2f}s)\"\n\nprint(\"‚úÖ Performance testing setup complete\")\nprint(\"\\nRealistic delays configured:\")\nprint(\"  - Minimum: 2.0 seconds (fast response)\")\nprint(\"  - Maximum: 5.0 seconds (slow response)\")\nprint(\"  - Average: ~3.5 seconds (typical)\")\nprint(\"\\nThis simulates real Gemini API latency without API costs.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 6: Performance Profiling - Sequential vs. Parallel\n\nNow let's dive deep into **performance analysis** to quantify the benefits of parallel execution.\n\n### What We'll Measure\n\nWe'll compare three execution strategies:\n\n1. **Sequential (baseline)** - One reviewer at a time\n2. **Parallel with asyncio.gather()** - All reviewers simultaneously\n3. **Batched parallel** - Groups of reviewers in parallel (for rate limit management)\n\nFor each strategy, we'll measure:\n- ‚è±Ô∏è **Total execution time**\n- üìä **Speedup factor** (vs. sequential)\n- üí∞ **API cost** (same for all, but we'll verify)\n- üéØ **Throughput** (reviews per second)\n\n---\n\n### Performance Testing Setup\n\nWe'll use mock reviewers with realistic delays to simulate API latency without actual API costs.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Key Consolidation Strategies\n\nThe secretary uses several strategies to create useful consolidated feedback:\n\n#### Strategy 1: Prioritization by Impact\n\n```\nCRITICAL > HIGH PRIORITY > MODERATE > OPTIONAL\n```\n\n**Criteria for CRITICAL**:\n- ‚ùå Factual errors that mislead students\n- ‚ùå Safety issues\n- ‚ùå Legal/compliance violations\n- ‚ùå Spelling/grammar in key concepts\n\n**Criteria for HIGH PRIORITY**:\n- ‚ö†Ô∏è Clarity issues that confuse students\n- ‚ö†Ô∏è Missing curriculum alignments\n- ‚ö†Ô∏è Inefficient structure\n\n**Criteria for OPTIONAL**:\n- üí° Enhancement suggestions\n- üí° Alternative approaches\n- üí° Future improvements\n\n#### Strategy 2: Synthesis of Related Feedback\n\n**Before consolidation**:\n```\nGrammarReviewer: \"Sentence structure in paragraph 2 is awkward.\"\nDistrictRep: \"Paragraph 2 is hard to understand.\"\nLiberalParent: \"Paragraph 2 uses jargon that ELL students might struggle with.\"\n```\n\n**After consolidation**:\n```\nHIGH PRIORITY: Revise paragraph 2 for clarity\n- Multiple reviewers noted comprehension issues\n- Specific problems: awkward sentence structure, jargon usage\n- Recommendation: Simplify sentences and define technical terms\n- Benefits: Improves understanding for all students, especially ELL learners\n```\n\n#### Strategy 3: Conflict Resolution\n\n**Conflicting feedback**:\n```\nConservativeParent: \"Remove discussion of evolution.\"\nLiberalParent: \"Expand evolution section with more details.\"\n```\n\n**Secretary's resolution**:\n```\nREQUIRES DISCUSSION: Evolution content (conflicting feedback)\n\nCurrent state: Brief mention of evolution in context of biological adaptation\n\nConservative parent concern: Evolution is controversial in some communities\nLiberal parent concern: Evolution is foundational to understanding biology\n\nRECOMMENDATION: Keep current brief mention with these modifications:\n1. Add note: \"Evolution by natural selection is the scientific consensus \n   explanation for biological diversity\" (factual, not advocacy)\n2. Provide \"opt-out\" notice for districts where evolution is contested\n3. Offer alternative activities focusing on observable adaptation (birds, bacteria)\n\nThis approach:\n- Maintains scientific integrity (liberal parent concern)\n- Respects community values (conservative parent concern)  \n- Gives districts flexibility (admin/district concern)\n```\n\n#### Strategy 4: Preserving Positive Feedback\n\nMany consolidations focus only on problems. The secretary also highlights what's working:\n\n```\nAPPROVED ASPECTS:\n‚úÖ Clear learning objectives (DistrictRep)\n‚úÖ Age-appropriate language (GrammarReviewer)\n‚úÖ Good use of real-world examples (all reviewers)\n‚úÖ Diverse representation in examples (LiberalParent)\n\n‚Üí These elements should be preserved in revisions\n```\n\n**Why this matters**: Prevents the writer from \"fixing\" things that aren't broken.\n\n---\n\n### Design Patterns in Secretary Consolidation\n\nThe secretary implementation uses several design patterns:\n\n#### Pattern 1: Aggregator Pattern\n\n```python\n# Collects outputs from multiple agents (reviewers)\n# Produces single unified output (consolidated review)\n\nInput:  [Review1, Review2, Review3, Review4, Review5]\n         ‚Üì\nAggregator (Secretary)\n         ‚Üì\nOutput: ConsolidatedReview\n```\n\n#### Pattern 2: Decorator Pattern (Implicit)\n\nThe secretary \"decorates\" raw reviews with:\n- Priority levels\n- Conflict resolution\n- Actionable recommendations\n\n#### Pattern 3: Chain of Responsibility (Two-Round Review)\n\n```\nRound 1: Independent reviews ‚Üí Secretary Consolidation ‚Üí Interim Feedback\nRound 2: Informed reviews (with Round 1 feedback) ‚Üí Secretary Consolidation ‚Üí Final Feedback\n```\n\n---\n\n### Real-World Secretary Output\n\nIn production, the secretary's output becomes the input to the **Writer's revision** (Pattern 18: Reflection).\n\n**Full workflow**:\n```\n1. Writer creates draft\n2. ReviewerPanel evaluates (6 reviewers in parallel)\n3. Secretary consolidates feedback\n4. Writer revises based on consolidated feedback (not raw reviews)\n5. Repeat until approved\n```\n\n**Key benefit**: Writer gets **one clear set of instructions** instead of 6 potentially conflicting perspectives.\n\n---\n\n### Key Takeaways: Secretary Pattern\n\n‚úÖ **Aggregation**: Combines multiple agent outputs into one\n‚úÖ **Prioritization**: Ranks feedback by importance  \n‚úÖ **Conflict resolution**: Handles disagreements gracefully\n‚úÖ **Simplification**: Converts technical feedback to action items\n‚úÖ **Synthesis**: Combines related feedback\n‚úÖ **Preservation**: Highlights what's working (not just problems)\n\n**When to use**:\n- ‚úÖ Multiple agents provide overlapping feedback\n- ‚úÖ Agents may disagree\n- ‚úÖ Downstream consumer (writer) needs clarity, not complexity\n- ‚úÖ Want to preserve positive feedback alongside critiques\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Manual secretary consolidation (simulating what the LLM would do)\ndef mock_secretary_consolidate(reviews: list) -> str:\n    \"\"\"Simulate secretary's consolidation logic.\"\"\"\n    \n    consolidation = \"\"\"\nCONSOLIDATED REVIEW SUMMARY\n================================\n\nCRITICAL (Fix Before Publication):\n1. Spelling Errors (GrammarReviewer)\n   - Line 5: 'occured' ‚Üí 'occurred'\n   - Line 12: 'seperate' ‚Üí 'separate'  \n   - Line 18: 'recieve' ‚Üí 'receive'\n\nAPPROVED ASPECTS:\n2. Length and Structure (DistrictRep)\n   - ‚úÖ Article length (~400 words) is appropriate\n   - ‚úÖ Well-structured and relatable examples\n   - ‚úÖ No changes needed\n\n3. Content Appropriateness (ConservativeParent)\n   - ‚úÖ Factually accurate\n   - ‚úÖ No controversial content\n   - ‚úÖ Approved for use\n\nOPTIONAL ENHANCEMENT (Consider for Future):\n4. Contemporary Connection (LiberalParent)\n   - Suggestion: Add brief section on climate change impact on photosynthesis\n   - Rationale: Connects to student interest in environmental issues\n   - Placement: Could add 1-2 sentences at end without increasing length significantly\n   - Decision: Optional - discuss with curriculum team\n\nOVERALL RECOMMENDATION:\nFix the 3 critical spelling errors, then publish. Climate change connection \nis a good idea but not required for initial publication.\n\"\"\"\n    return consolidation.strip()\n\n# Apply consolidation\nconsolidated_feedback = mock_secretary_consolidate(mock_reviews)\n\nprint(\"\\nSECRETARY CONSOLIDATED FEEDBACK\")\nprint(\"=\" * 70)\nprint(consolidated_feedback)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\n‚úÖ Benefits of consolidation:\")\nprint(\"  1. Clear priorities (CRITICAL vs APPROVED vs OPTIONAL)\")\nprint(\"  2. Specific action items (exact line numbers, exact changes)\")\nprint(\"  3. Positive feedback preserved (approved aspects listed)\")\nprint(\"  4. Conflicts resolved (climate change suggestion framed as optional)\")\nprint(\"  5. Overall recommendation provided (fix 3 errors, then publish)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mock reviews from different reviewers\nmock_reviews = [\n    (Reviewer.GRAMMAR_REVIEWER, \n     \"Fix 3 spelling errors: 'occured' (line 5) should be 'occurred', \"\n     \"'seperate' (line 12) should be 'separate', 'recieve' (line 18) should be 'receive'.\"),\n    \n    (Reviewer.DISTRICT_REP,\n     \"Article is clear and well-structured. Length is appropriate at ~400 words. \"\n     \"Good use of examples that students can relate to.\"),\n    \n    (Reviewer.CONSERVATIVE_PARENT,\n     \"The photosynthesis article is factually accurate and appropriate. \"\n     \"No controversial content detected. Approve for use.\"),\n    \n    (Reviewer.LIBERAL_PARENT,\n     \"Good scientific content. Consider adding: How does climate change affect photosynthesis? \"\n     \"This connects science to contemporary environmental issues students care about.\"),\n]\n\nprint(\"MOCK REVIEWER FEEDBACK\")\nprint(\"=\" * 70)\nfor i, (reviewer, feedback) in enumerate(mock_reviews, 1):\n    print(f\"\\n{i}. {reviewer.name}:\")\n    print(f\"   {feedback}\")\n\nprint(\"\\n\" + \"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### How Secretary Consolidation Works\n\nLet's trace through the consolidation process step by step.\n\n#### Step 1: Collect All Reviews\n\nThe secretary receives reviews as a list of `(Reviewer, review_text)` tuples:\n\n```python\nreviews_so_far = [\n    (Reviewer.GRAMMAR_REVIEWER, \"Fix spelling errors on lines 5, 8, 12...\"),\n    (Reviewer.DISTRICT_REP, \"Article is too long. Reduce to 300 words...\"),\n    (Reviewer.CONSERVATIVE_PARENT, \"Remove paragraph 3...\"),\n    (Reviewer.LIBERAL_PARENT, \"Expand paragraph 3...\"),\n    (Reviewer.SCHOOL_ADMIN, \"Add safety warning...\"),\n]\n```\n\n#### Step 2: Format for LLM\n\nReviews are formatted with clear delimiters:\n\n```python\nreviews_text = []\nfor reviewer, review in reviews_so_far:\n    reviews_text.append(f\"BEGIN review by {reviewer.name}:\\\\n{review}\\\\nEND review\\\\n\")\n```\n\n**Formatted output**:\n```\nBEGIN review by GRAMMAR_REVIEWER:\nFix spelling errors on lines 5, 8, 12...\nEND review\n\nBEGIN review by DISTRICT_REP:\nArticle is too long. Reduce to 300 words...\nEND review\n\nBEGIN review by CONSERVATIVE_PARENT:\nRemove paragraph 3...\nEND review\n\nBEGIN review by LIBERAL_PARENT:\nExpand paragraph 3...\nEND review\n```\n\n#### Step 3: Create Consolidation Prompt\n\n```python\nprompt_vars = {\n    \"prompt_name\": \"Secretary_consolidate_reviews\",\n    \"topic\": topic,\n    \"article\": article,\n    \"reviews\": reviews_text  # All formatted reviews\n}\nprompt = PromptService.render_prompt(**prompt_vars)\n```\n\n#### Step 4: LLM Consolidation\n\nThe LLM (acting as secretary) analyzes all reviews and produces consolidated feedback:\n\n**Example consolidated output**:\n```\nCRITICAL (Fix Immediately):\n1. Add safety warning and parental consent requirement for chemistry experiment (SchoolAdmin)\n2. Fix 12 spelling errors throughout the article (GrammarReviewer)\n\nHIGH PRIORITY (Address Before Final Review):\n3. Reduce article length from 500 to 350 words (DistrictRep) - suggest cutting\n   examples rather than core content\n4. Math notation: Use √ó instead of * for multiplication (MathReviewer)\n\nREQUIRES DISCUSSION (Conflicting Feedback):\n5. Paragraph 3 feedback:\n   - Conservative parent wants to remove discussion of colonialism's negative impacts\n   - Liberal parent wants to expand this section\n   \n   RECOMMENDATION: Keep paragraph but revise for balance:\n   - Acknowledge both positive contributions AND negative impacts\n   - Use neutral, factual language\n   - Keep length moderate (current length is acceptable)\n   - Focus on historical context rather than moral judgments\n   \n   This approach addresses both concerns: Conservative parent gets more balanced \n   framing, Liberal parent gets to keep the critical context.\n\nOPTIONAL (Consider for Future Revisions):\n6. Consider adding more diverse examples in future revisions\n```\n\n---\n\n### Simulating Secretary Consolidation\n\nLet's create a mock consolidation example to see the pattern in action:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Read the secretary's system prompt\nprint(\"SECRETARY SYSTEM PROMPT\")\nprint(\"=\" * 70)\nsecretary_prompt = (prompts_dir / \"secretary_system_prompt.j2\").read_text()\nprint(secretary_prompt.strip())\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\nüîç Analysis:\")\nprint(\"  - Role: Secretary of curriculum review panel\")\nprint(\"  - Task: Summarize ALL reviewer feedback\")\nprint(\"  - Output: Specific directions for the writer\")\nprint(\"  - Goal: Enable writer to revise based on consolidated feedback\")\nprint(\"\\nNote: The prompt is deliberately simple, allowing the LLM to use\")\nprint(\"its reasoning capabilities to handle complex consolidation tasks.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 5: Secretary Consolidation Pattern\n\nAfter 6 reviewers provide their feedback, we face a critical challenge: **how do we turn diverse, sometimes conflicting feedback into actionable guidance?**\n\nThis is where the **Secretary pattern** comes in.\n\n### The Problem: Information Overload\n\nWithout consolidation, the writer receives:\n\n```\nGrammarReviewer: \"Fix 12 spelling errors on lines 5, 8, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39.\"\n\nDistrictRepReviewer: \"Article is too long. Reduce from 500 words to 300 words to save printing costs.\"\n\nConservativeParent: \"Remove paragraph 3 which discusses the negative impacts of colonialism. \n                     Focus on positive contributions of Western civilization.\"\n\nLiberalParent: \"Paragraph 3 is the most important! It provides critical context about power \n                dynamics. Actually expand it with more examples.\"\n\nMathReviewer: \"Equation on line 15 should use √ó instead of * for multiplication.\"\n\nSchoolAdmin: \"Chemistry experiment on page 2 requires safety warning and parental consent form.\"\n```\n\n**Writer's reaction**: üòµ \"Where do I even start? Conservative and Liberal parents directly contradict each other!\"\n\n---\n\n### The Solution: Secretary Consolidation\n\nThe **PanelSecretary** acts as a **meta-reviewer** that:\n\n1. **Synthesizes** - Combines related feedback\n2. **Prioritizes** - Ranks by importance (CRITICAL ‚Üí HIGH ‚Üí OPTIONAL)\n3. **Resolves conflicts** - Provides balanced guidance when reviewers disagree\n4. **Simplifies** - Converts technical feedback into clear action items\n\n**Code Reference**: [`composable_app/agents/reviewer_panel.py:65-98`](../../agents/reviewer_panel.py#L65-L98)\n\n### Secretary's System Prompt\n\nLet's examine what instructions the secretary receives:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis: Contrasting Personas\n\nNotice the stark differences between the two adversarial reviewers:\n\n| Aspect | Conservative Parent | Liberal Parent |\n|--------|-------------------|----------------|\n| **History focus** | Patriotic narratives, Western civilization | Individual agency, critical thinking |\n| **Controversial topics** | Downplay slavery, colonialism | Highlight power dynamics, social justice |\n| **Math approach** | Traditional algorithms, foundational skills | Real-world contexts, problem-solving |\n| **Values** | Civic virtue, accuracy, efficiency | Diverse perspectives, contemporary relevance |\n\n**Why include both?**\n- ‚úÖ **Surface trade-offs**: Make implicit tensions explicit\n- ‚úÖ **Broader acceptance**: Address concerns from both ends of spectrum\n- ‚úÖ **Better content**: Balancing diverse viewpoints creates richer material\n- ‚úÖ **Risk mitigation**: Catch issues that would alienate either group\n\n**Example conflict**:\n\n```\nArticle: \"The American Revolution was a fight for independence from British colonial rule.\"\n\nConservative Parent: \"‚úÖ Good! Emphasizes patriotic narrative and American independence.\"\n\nLiberal Parent: \"‚ö†Ô∏è  Missing context: What about enslaved people who weren't granted liberty? \n                Consider adding: 'While the Revolution secured independence for colonists, \n                it did not extend these rights to enslaved Africans.'\"\n\nSecretary (consolidation): \"Consider adding a nuanced note about the limitations of \n                           Revolutionary-era liberty to provide historical context while \n                           maintaining the patriotic narrative focus.\"\n```\n\nThis conflict is **valuable** - it leads to more historically accurate and broadly acceptable content.\n\n---\n\n### Designing Effective Reviewer Personas\n\nBased on the ComposableApp's implementation, here are **4 principles** for designing reviewer personas:\n\n#### Principle 1: Clear, Focused Mandate\n\n**Good** (Grammar Reviewer):\n```\n\"You are a stickler for formal language in all school content.\"\n```\n- ‚úÖ Clear role (formal language expert)\n- ‚úÖ Focused scope (grammar, not content)\n- ‚úÖ Objective criteria\n\n**Bad**:\n```\n\"You are an expert in education who reviews content for quality.\"\n```\n- ‚ùå Vague role (what kind of quality?)\n- ‚ùå Broad scope (overlaps with other reviewers)\n- ‚ùå Subjective criteria\n\n#### Principle 2: Specific Values and Priorities\n\n**Good** (Conservative Parent):\n```\n\"You want a focus on Western civilization and a more positive view of \nAmerican/European history, and want to downplay aspects like the history \nof slavery, colonialism...\"\n```\n- ‚úÖ Explicit values (patriotic, traditional)\n- ‚úÖ Specific priorities (Western civ, positive framing)\n- ‚úÖ Clear stance on controversial topics\n\n**Bad**:\n```\n\"You care about traditional values.\"\n```\n- ‚ùå Vague values (what traditions?)\n- ‚ùå No specific priorities\n- ‚ùå Unclear how to apply\n\n#### Principle 3: Representative of Real Stakeholders\n\n**Good** (Liberal Parent):\n```\n\"You value the connection between history and contemporary issues, \nhighlighting themes of power, liberty, and individual rights.\"\n```\n- ‚úÖ Reflects actual liberal parent concerns\n- ‚úÖ Based on real-world feedback patterns\n- ‚úÖ Predictive of actual controversies\n\n**Bad**:\n```\n\"You want content to be progressive and modern.\"\n```\n- ‚ùå Stereotype, not representative\n- ‚ùå Not based on real feedback\n- ‚ùå Won't catch actual issues\n\n#### Principle 4: Productive Tension\n\n**Good** (Conservative vs. Liberal):\n```\nConservative: \"Focus on traditional algorithms\"\nLiberal: \"Emphasize real-world contexts\"\n```\n- ‚úÖ Genuine disagreement\n- ‚úÖ Both valid perspectives\n- ‚úÖ Forces better solutions (balance both)\n\n**Bad**:\n```\nReviewer A: \"Make content short\"\nReviewer B: \"Make content long\"\n```\n- ‚ùå Artificial conflict\n- ‚ùå No underlying values\n- ‚ùå Doesn't lead to better content\n\n---\n\n### Exercise: Design Your Own Reviewer Persona\n\nTry designing a new reviewer for a different domain. Example: **STEM Equity Advocate**\n\n```python\npersona = \\\"\\\"\\\"\nYou are a STEM equity advocate who wants to ensure that science and math content is \naccessible to all students regardless of background.\n\nPriorities:\n- Examples should reflect diverse cultures and communities\n- Experiments should not require expensive materials unavailable to low-income schools\n- Language should avoid jargon and be accessible to English language learners\n- Real-world applications should connect to varied student experiences, not just suburban contexts\n\nRed flags:\n- Examples that assume access to technology, travel, or expensive hobbies\n- Experiments requiring specialized equipment not available in all schools\n- Cultural references that alienate non-Western or non-middle-class students\n- Language that favors native English speakers\n\\\"\\\"\\\"\n```\n\n**Questions to ask**:\n1. ‚úÖ Does this represent a real stakeholder?\n2. ‚úÖ Are the priorities specific and actionable?\n3. ‚úÖ Will this catch issues others miss?\n4. ‚úÖ Does this create productive tension with other reviewers?\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Adversarial reviewer prompts\nprint(\"=\" * 70)\nprint(\"ADVERSARIAL REVIEWERS (Subjective, Value-Based)\")\nprint(\"=\" * 70)\n\nprint(\"\\n1. CONSERVATIVE PARENT REVIEWER\")\nprint(\"-\" * 70)\nconservative_prompt = (prompts_dir / \"conservative_parent_system_prompt.j2\").read_text()\nprint(conservative_prompt.strip())\n\nprint(\"\\n\\n2. LIBERAL PARENT REVIEWER\")\nprint(\"-\" * 70)\nliberal_prompt = (prompts_dir / \"liberal_parent_system_prompt.j2\").read_text()\nprint(liberal_prompt.strip())\n\nprint(\"\\n\\n3. SCHOOL ADMIN REVIEWER\")\nprint(\"-\" * 70)\nif (prompts_dir / \"school_admin_system_prompt.j2\").exists():\n    admin_prompt = (prompts_dir / \"school_admin_system_prompt.j2\").read_text()\n    print(admin_prompt.strip())\nelse:\n    print(\"(Note: school_admin_system_prompt.j2 not found - using SCHOOL_ADMIN as secretary)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Read system prompts for different reviewer types\nprompts_dir = composable_app_path / \"prompts\"\n\n# Specialist reviewer prompts\nprint(\"=\" * 70)\nprint(\"SPECIALIST REVIEWERS (Objective, Technical)\")\nprint(\"=\" * 70)\n\nprint(\"\\n1. GRAMMAR REVIEWER\")\nprint(\"-\" * 70)\ngrammar_prompt = (prompts_dir / \"grammar_reviewer_system_prompt.j2\").read_text()\nprint(grammar_prompt.strip())\n\nprint(\"\\n\\n2. DISTRICT REP REVIEWER\")\nprint(\"-\" * 70)\ndistrict_prompt = (prompts_dir / \"district_rep_system_prompt.j2\").read_text()\nprint(district_prompt.strip())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 4: Specialist vs. Adversarial Reviewer Design\n\nThe power of multi-agent systems comes from **diversity of perspectives**. The ReviewerPanel uses two types of agents with very different roles.\n\n### The Two Types of Reviewers\n\n#### Type 1: Specialist Reviewers (Technical Experts)\n\n**Goal**: Ensure accuracy, quality, and compliance\n\n**Personas**:\n1. **GrammarReviewer** - Formal language expert\n2. **DistrictRepReviewer** - Budget and clarity focus  \n3. *(MathReviewer is not in current enum but would be a specialist)*\n\n**Characteristics**:\n- ‚úÖ Objective, measurable criteria\n- ‚úÖ Domain expertise (grammar, curriculum standards)\n- ‚úÖ Focus on correctness and quality\n- ‚úÖ Non-controversial feedback\n\n#### Type 2: Adversarial Reviewers (Stakeholder Perspectives)\n\n**Goal**: Identify potential controversies and concerns\n\n**Personas**:\n1. **ConservativeParentReviewer** - Traditional values, patriotic narratives\n2. **LiberalParentReviewer** - Critical thinking, diverse perspectives\n3. **SchoolAdminReviewer** - Legal/liability, budget concerns\n\n**Characteristics**:\n- ‚úÖ Subjective, value-based criteria\n- ‚úÖ Stakeholder representation (parents, administrators)\n- ‚úÖ Focus on acceptance and controversy avoidance\n- ‚úÖ Often conflicting feedback\n\n---\n\n### Examining System Prompts\n\nLet's look at the actual system prompts that define each reviewer's persona:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Key Takeaways: Parallel Execution\n\n‚úÖ **`asyncio.gather()`** runs multiple async functions concurrently\n‚úÖ **Performance**: 5 reviewers in parallel = ~5x faster than sequential\n‚úÖ **`return_exceptions=True`**: Continue despite individual failures\n‚úÖ **Filter results**: Separate successful reviews from exceptions\n‚úÖ **Use case**: Perfect for I/O-bound operations (LLM API calls, database queries)\n\n**When to use parallel vs. sequential**:\n- ‚úÖ **Parallel**: Independent tasks (reviewers don't need each other's output in Round 1)\n- ‚ùå **Sequential**: Dependent tasks (Round 2 reviewers need to see Round 1 feedback)\n\n---\n\n### Real-World Performance\n\nIn production with real LLM API calls:\n- **Sequential**: 6 reviewers √ó 5s each = **30 seconds**\n- **Parallel**: 6 reviewers = **5-6 seconds** (6x speedup)\n- **Cost**: Same (6 API calls either way)\n- **User experience**: Much better (5s vs 30s wait time)\n\nThe ComposableApp **currently uses sequential execution** in Round 1 and Round 2 (as shown in the code). This is a deliberate choice to:\n1. Simplify debugging (linear execution)\n2. Avoid rate limiting issues\n3. Make token usage more predictable\n\n**Exercise for you**: Modify `do_first_round_reviews()` to use `asyncio.gather()` for parallel execution!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demo: WITH return_exceptions=True (robust)\nprint(\"WITH return_exceptions=True (robust)\")\nprint(\"=\" * 60)\n\nresults = await asyncio.gather(\n    mock_review_with_failure(\"Grammar\", should_fail=False),\n    mock_review_with_failure(\"Math\", should_fail=True),  # This one fails\n    mock_review_with_failure(\"District\", should_fail=False),\n    return_exceptions=True  # ‚úÖ Return exceptions as values\n)\n\nprint(f\"\\nüìä Results returned: {len(results)} items\")\nfor i, result in enumerate(results):\n    if isinstance(result, Exception):\n        print(f\"  {i+1}. ‚ùå Exception: {result}\")\n    else:\n        print(f\"  {i+1}. ‚úÖ Success: {result}\")\n\n# Filter out exceptions\nsuccessful_reviews = [r for r in results if not isinstance(r, Exception)]\nfailed_reviews = [r for r in results if isinstance(r, Exception)]\n\nprint(f\"\\n‚úÖ Successful reviews: {len(successful_reviews)}\")\nprint(f\"‚ùå Failed reviews: {len(failed_reviews)}\")\nprint(f\"\\nüéØ Key benefit: Got 2 out of 3 reviews despite 1 failure!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mock review function that sometimes fails\nasync def mock_review_with_failure(reviewer_name: str, should_fail: bool = False) -> str:\n    \"\"\"Simulate a reviewer that might fail.\"\"\"\n    print(f\"  ‚è≥ {reviewer_name} started reviewing...\")\n    await asyncio.sleep(1.0)\n    \n    if should_fail:\n        print(f\"  ‚ùå {reviewer_name} FAILED (simulated error)\")\n        raise ValueError(f\"{reviewer_name} encountered an error during review\")\n    \n    print(f\"  ‚úÖ {reviewer_name} finished\")\n    return f\"{reviewer_name} review: All good!\"\n\n# Demo: WITHOUT return_exceptions (default behavior)\nprint(\"WITHOUT return_exceptions=True (default)\")\nprint(\"=\" * 60)\ntry:\n    results = await asyncio.gather(\n        mock_review_with_failure(\"Grammar\", should_fail=False),\n        mock_review_with_failure(\"Math\", should_fail=True),  # This one fails\n        mock_review_with_failure(\"District\", should_fail=False),\n    )\n    print(f\"‚úÖ All reviews completed: {results}\")\nexcept Exception as e:\n    print(f\"‚ùå Exception raised: {e}\")\n    print(f\"   Problem: Only got 1 review, lost the other 2!\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Handling Failures with return_exceptions=True\n\nWhat happens if one reviewer fails? By default, `asyncio.gather()` will raise the first exception and cancel remaining tasks. This is problematic for reviews - we want to get as much feedback as possible even if one reviewer fails.\n\n**Solution**: Use `return_exceptions=True` to continue despite failures.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Parallel execution - all reviewers at once\nprint(\"PARALLEL EXECUTION (asyncio.gather)\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\n# Create tasks for all reviewers\ntasks = [mock_review(reviewer, delay_seconds=2.0) for reviewer in reviewers]\n\n# Execute all tasks in parallel\nreviews_parallel = await asyncio.gather(*tasks)\n\ntotal_time_parallel = time.time() - start_time\n\nprint(f\"\\nüìä Results:\")\nprint(f\"  Total time: {total_time_parallel:.2f} seconds\")\nprint(f\"  Reviews completed: {len(reviews_parallel)}\")\nprint(f\"  Average time per review: {total_time_parallel / len(reviewers):.2f}s\")\n\n# Compare performance\nprint(f\"\\nüöÄ PERFORMANCE COMPARISON:\")\nprint(f\"  Sequential: {total_time_sequential:.2f}s\")\nprint(f\"  Parallel:   {total_time_parallel:.2f}s\")\nprint(f\"  Speedup:    {total_time_sequential / total_time_parallel:.1f}x faster!\")\nprint(f\"  Time saved: {total_time_sequential - total_time_parallel:.2f}s ({(1 - total_time_parallel/total_time_sequential)*100:.0f}% reduction)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Parallel Execution with asyncio.gather()",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sequential execution - one reviewer at a time\nreviewers = [\"Grammar\", \"Math\", \"District\", \"Conservative\", \"Liberal\"]\n\nprint(\"SEQUENTIAL EXECUTION\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\nreviews_sequential = []\nfor reviewer in reviewers:\n    review = await mock_review(reviewer, delay_seconds=2.0)\n    reviews_sequential.append(review)\n\ntotal_time_sequential = time.time() - start_time\n\nprint(f\"\\nüìä Results:\")\nprint(f\"  Total time: {total_time_sequential:.2f} seconds\")\nprint(f\"  Reviews completed: {len(reviews_sequential)}\")\nprint(f\"  Average time per review: {total_time_sequential / len(reviewers):.2f}s\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Sequential Execution (Baseline)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Mock review function (simulates API call without actually calling it)\nasync def mock_review(reviewer_name: str, delay_seconds: float = 2.0) -> str:\n    \"\"\"Simulate a reviewer taking time to review (without API call).\"\"\"\n    print(f\"  ‚è≥ {reviewer_name} started reviewing...\")\n    await asyncio.sleep(delay_seconds)  # Simulate API latency\n    review_text = f\"{reviewer_name} completed review: Article looks good overall.\"\n    print(f\"  ‚úÖ {reviewer_name} finished (took {delay_seconds}s)\")\n    return review_text\n\n# Test the mock function\nprint(\"Testing mock_review function:\")\nprint(\"=\" * 60)\nreview = await mock_review(\"GrammarReviewer\", delay_seconds=1.0)\nprint(f\"\\nReview output: {review}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 3: Parallel Execution with asyncio.gather()\n\nNow let's see the key performance optimization: **parallel execution** of independent reviewers.\n\n### The Problem: Sequential Execution is Slow\n\n**Current implementation** (from the code above) runs reviewers **sequentially**:\n\n```python\nasync def do_first_round_reviews(article, topic) -> list:\n    review_panel = [ReviewerAgent(reviewer) for reviewer in list(Reviewer)[:-1]]\n    first_round_reviews = list()\n    \n    for reviewer_agent in review_panel:  # ‚ùå Sequential loop\n        review = await reviewer_agent.review(topic, article, reviews_so_far=[])\n        first_round_reviews.append((reviewer_agent.reviewer, review))\n    \n    return first_round_reviews\n```\n\n**Time breakdown** (if each reviewer takes 5 seconds):\n```\nGrammarReviewer:        [====] 5s\nMathReviewer:           [====] 5s\nDistrictRepReviewer:    [====] 5s\nConservativeParent:     [====] 5s\nLiberalParent:          [====] 5s\n\nTotal: 5 √ó 5s = 25 seconds ‚ùå\n```\n\n### The Solution: Parallel Execution\n\n**Optimized version** using `asyncio.gather()`:\n\n```python\nasync def do_first_round_reviews_parallel(article, topic) -> list:\n    review_panel = [ReviewerAgent(reviewer) for reviewer in list(Reviewer)[:-1]]\n    \n    # Create all review tasks\n    review_tasks = [\n        reviewer_agent.review(topic, article, reviews_so_far=[])\n        for reviewer_agent in review_panel\n    ]\n    \n    # Execute all tasks in parallel ‚úÖ\n    reviews = await asyncio.gather(*review_tasks)\n    \n    # Pair reviewers with their reviews\n    first_round_reviews = [\n        (reviewer_agent.reviewer_type(), review)\n        for reviewer_agent, review in zip(review_panel, reviews)\n    ]\n    \n    return first_round_reviews\n```\n\n**Time breakdown** (parallel):\n```\nGrammarReviewer:        [====]\nMathReviewer:           [====]\nDistrictRepReviewer:    [====]  All run simultaneously\nConservativeParent:     [====]\nLiberalParent:          [====]\n\nTotal: max(5s) = 5 seconds ‚úÖ 5x faster!\n```\n\n### Understanding asyncio.gather()\n\n**`asyncio.gather(*tasks)`** runs multiple async functions concurrently:\n\n```python\n# Sequential (one after another)\nresult1 = await func1()  # Wait for func1\nresult2 = await func2()  # Wait for func2\nresult3 = await func3()  # Wait for func3\n# Total: time(func1) + time(func2) + time(func3)\n\n# Parallel (all at once)\nresults = await asyncio.gather(\n    func1(),  # Start all three immediately\n    func2(),\n    func3(),\n)\n# Total: max(time(func1), time(func2), time(func3))\n```\n\n**Key parameters**:\n- **`*tasks`** - Unpacks list of coroutines\n- **`return_exceptions=True`** - Continue if one task fails (we'll demo this)\n\n---\n\n### Demo: Simulating Parallel Review\n\nLet's create a **mock review function** that simulates the time each reviewer takes, without actually calling the API:\n\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Component 3: Review Orchestration Workflow\n\nThe `get_panel_review_of_article()` function orchestrates the entire multi-agent review process. It uses a **two-round review** pattern:\n\n**Code Reference**: [`composable_app/agents/reviewer_panel.py:100-131`](../../agents/reviewer_panel.py#L100-L131)\n\n```python\nasync def get_panel_review_of_article(topic: str, article: Article) -> str:\n    # Round 1: Independent reviews\n    first_round_reviews = await do_first_round_reviews(article, topic)\n    \n    # Round 2: Reviews after seeing others' feedback\n    final_reviews = await do_second_round_reviews(article, first_round_reviews, topic)\n    \n    # Round 3: Secretary consolidates\n    return await summarize_reviews(article, final_reviews, topic)\n```\n\n#### Round 1: Independent Reviews\n\n```python\nasync def do_first_round_reviews(article, topic) -> list:\n    # Each reviewer evaluates independently (no knowledge of others)\n    review_panel = [ReviewerAgent(reviewer) for reviewer in list(Reviewer)[:-1]]\n    \n    first_round_reviews = list()\n    for reviewer_agent in review_panel:\n        review = await reviewer_agent.review(topic, article, reviews_so_far=[])\n        first_round_reviews.append((reviewer_agent.reviewer, review))\n    \n    return first_round_reviews\n```\n\n**Why independent first?**\n- ‚úÖ Prevents groupthink (reviewers not biased by others)\n- ‚úÖ Captures diverse perspectives\n- ‚úÖ Each reviewer focuses on their specialty\n\n#### Round 2: Informed Reviews\n\n```python\nasync def do_second_round_reviews(article, first_round_reviews, topic) -> list:\n    # Each reviewer can now see what others said\n    review_panel = [ReviewerAgent(reviewer) for reviewer in list(Reviewer)[:-1]]\n    \n    final_reviews = list()\n    for reviewer_agent in review_panel:\n        # Pass first_round_reviews so they can respond to others\n        review = await reviewer_agent.review(topic, article, first_round_reviews)\n        final_reviews.append((reviewer_agent.reviewer_type(), review))\n    \n    return final_reviews\n```\n\n**Why second round?**\n- ‚úÖ Reviewers can address each other's points\n- ‚úÖ Conflicts surface (e.g., conservative vs. liberal)\n- ‚úÖ More nuanced feedback (\\\"I agree with Grammar but...\\\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a PanelSecretary\nfrom composable_app.agents.reviewer_panel import PanelSecretary\n\nsecretary = PanelSecretary()\n\nprint(f\"‚úÖ Created Secretary: {secretary.name()}\")\nprint(f\"\\\\nüìù Secretary's role:\")\nprint(f\"  - Receives reviews from all 6 reviewers\")\nprint(f\"  - Consolidates into single prioritized summary\")\nprint(f\"  - Resolves conflicts between reviewers\")\nprint(f\"  - Provides actionable guidance to writer\")\nprint(f\"\\\\nüîß Configuration:\")\nprint(f\"  - Model: {llms.DEFAULT_MODEL}\")\nprint(f\"  - System prompt: prompts/secretary_system_prompt.j2\")\nprint(f\"  - Output type: str (consolidated review text)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Component 2: PanelSecretary\n\nThe **PanelSecretary** consolidates feedback from all reviewers into a single, prioritized summary.\n\n**Code Reference**: [`composable_app/agents/reviewer_panel.py:65-98`](../../agents/reviewer_panel.py#L65-L98)\n\n```python\nclass PanelSecretary:\n    def __init__(self):\n        self.id = f\"PanelSecretary {uuid.uuid4()}\"\n        system_prompt = PromptService.render_prompt(\"secretary_system_prompt\")\n        \n        self.agent = Agent(\n            llms.DEFAULT_MODEL,\n            output_type=str,\n            model_settings=llms.default_model_settings(),\n            retries=2,\n            system_prompt=system_prompt\n        )\n    \n    async def consolidate(\n        self, \n        topic: str, \n        article: Article, \n        reviews_so_far: List[Tuple[Reviewer, str]]\n    ) -> str:\n        # Format all reviews\n        reviews_text = []\n        for reviewer, review in reviews_so_far:\n            reviews_text.append(f\"BEGIN review by {reviewer.name}:\\\\n{review}\\\\nEND review\\\\n\")\n        \n        # Create consolidation prompt\n        prompt_vars = {\n            \"prompt_name\": \"Secretary_consolidate_reviews\",\n            \"topic\": topic,\n            \"article\": article,\n            \"reviews\": reviews_text\n        }\n        \n        prompt = PromptService.render_prompt(**prompt_vars)\n        result = await self.agent.run(prompt)\n        \n        # Log consolidated review\n        await evals.record_ai_response(\n            \"consolidated_review\",\n            ai_input=prompt_vars,\n            ai_response=result.output\n        )\n        \n        return result.output\n```\n\n**Secretary's responsibilities**:\n1. **Synthesize** - Combine related feedback (e.g., grammar + style issues)\n2. **Prioritize** - Rank by importance (CRITICAL ‚Üí HIGH ‚Üí OPTIONAL)\n3. **Resolve conflicts** - When reviewers disagree, provide balanced guidance\n4. **Simplify** - Convert technical feedback into clear action items\n\nThe secretary receives **all 6 reviews** and produces a **single consolidated review** for the writer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create individual reviewers\nfrom composable_app.agents.reviewer_panel import ReviewerAgent\n\n# Specialist reviewers\ngrammar_reviewer = ReviewerAgent(Reviewer.GRAMMAR_REVIEWER)\ndistrict_rep = ReviewerAgent(Reviewer.DISTRICT_REP)\n\n# Adversarial reviewers\nconservative_parent = ReviewerAgent(Reviewer.CONSERVATIVE_PARENT)\nliberal_parent = ReviewerAgent(Reviewer.LIBERAL_PARENT)\n\nprint(\"‚úÖ Created 4 reviewers:\")\nprint(f\"  1. {grammar_reviewer.name()}\")\nprint(f\"  2. {district_rep.name()}\")\nprint(f\"  3. {conservative_parent.name()}\")\nprint(f\"  4. {liberal_parent.name()}\")\n\nprint(f\"\\\\nüìã Each reviewer has:\")\nprint(f\"  - Unique ID (with UUID)\")\nprint(f\"  - Reviewer type enum\")\nprint(f\"  - Pydantic AI agent (configured with {llms.DEFAULT_MODEL})\")\nprint(f\"  - System prompt loaded from prompts/{Reviewer.GRAMMAR_REVIEWER.name.lower()}_system_prompt.j2\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating Individual Reviewers\n\nLet's create a few reviewers and examine their personas:\n\n**Note**: We won't actually call the review methods yet (to avoid API costs). We'll just inspect the reviewer configuration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a sample article to review\nsample_article = Article(\n    title=\"Photosynthesis: How Plants Make Food\",\n    summary=\"An explanation of the photosynthesis process for 9th grade students.\",\n    full_text=\"\"\"\nPhotosynthesis is the process by which plants make their own food using sunlight, water, and carbon dioxide.\nThe equation for photosynthesis is: 6CO‚ÇÇ + 6H‚ÇÇO + light energy ‚Üí C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ + 6O‚ÇÇ\n\nPlants contain chlorophyll in their leaves, which captures light energy from the sun. This energy is used to \nconvert carbon dioxide from the air and water from the soil into glucose (a type of sugar) and oxygen.\n\nThe glucose provides energy for the plant to grow, while the oxygen is released into the atmosphere as a \nbyproduct. This is why forests are often called the \"lungs of the Earth\" - they produce oxygen that we breathe!\n    \"\"\".strip(),\n    keywords=[\"photosynthesis\", \"chlorophyll\", \"glucose\", \"oxygen\", \"carbon dioxide\"]\n)\n\nprint(\"Sample Article Created:\")\nprint(\"=\" * 60)\nprint(f\"Title: {sample_article.title}\")\nprint(f\"Summary: {sample_article.summary}\")\nprint(f\"\\\\nFull text ({len(sample_article.full_text)} characters):\")\nprint(sample_article.full_text[:200] + \"...\")\nprint(f\"\\\\nKeywords: {', '.join(sample_article.keywords)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Component 1: ReviewerAgent Class\n\nThe `ReviewerAgent` class represents a single reviewer. Let's look at its key features:\n\n**Code Reference**: [`composable_app/agents/reviewer_panel.py:25-63`](../../agents/reviewer_panel.py#L25-L63)\n\n```python\nclass ReviewerAgent:\n    def __init__(self, reviewer: Reviewer):\n        self.reviewer = reviewer\n        self.id = f\"{reviewer} Agent {uuid.uuid4()}\"\n        \n        # Load reviewer-specific system prompt\n        system_prompt_file = f\"{reviewer.name}_system_prompt\".lower()\n        system_prompt = PromptService.render_prompt(system_prompt_file)\n        \n        # Create Pydantic AI agent\n        self.agent = Agent(\n            llms.DEFAULT_MODEL,\n            output_type=str,\n            model_settings=llms.default_model_settings(),\n            retries=2,\n            system_prompt=system_prompt\n        )\n    \n    async def review(self, topic: str, article: Article, reviews_so_far: List[Tuple[Reviewer, str]]) -> str:\n        # Build prompt with article and previous reviews\n        reviews_text = []\n        for reviewer, review in reviews_so_far:\n            reviews_text.append(f\"BEGIN review by {reviewer.name}:\\\\n{review}\\\\nEND review\\\\n\")\n        \n        prompt_vars = {\n            \"prompt_name\": \"ReviewerAgent_review_prompt\",\n            \"topic\": topic,\n            \"article\": article,\n            \"reviews\": reviews_text\n        }\n        \n        # Generate review\n        prompt = PromptService.render_prompt(**prompt_vars)\n        result = await self.agent.run(prompt)\n        \n        # Log for evaluation\n        await evals.record_ai_response(\n            f\"{self.reviewer.name}_review\",\n            ai_input=prompt_vars,\n            ai_response=result.output\n        )\n        \n        return result.output\n```\n\n**Key design decisions**:\n\n1. **Enum-based reviewer types** - Uses `Reviewer` enum for type safety\n2. **Dynamic system prompts** - Each reviewer loads its own persona from `prompts/{reviewer}_system_prompt.j2`\n3. **Reviews awareness** - Can see previous reviews (`reviews_so_far`) for multi-round review\n4. **Evaluation logging** - Records all reviews to `logs/evals.log` for analysis\n\nLet's create a sample reviewer to see how it works:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Let's examine the Reviewer enum (defines the 6 reviewer types)\nfrom composable_app.agents.reviewer_panel import Reviewer\n\nprint(\"Available Reviewer Types:\")\nprint(\"=\" * 50)\nfor reviewer in Reviewer:\n    print(f\"  {reviewer.value}. {reviewer.name}\")\n    \nprint(f\"\\nTotal reviewers: {len(list(Reviewer))}\")\nprint(\"\\nNote: SCHOOL_ADMIN is used as the secretary, not as a reviewer\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 2: ReviewerPanel Architecture\n\nNow let's examine the actual code that implements the multi-agent pattern. We'll look at the ReviewerPanel architecture step by step.\n\n### Overview: ReviewerPanel Components\n\nThe ReviewerPanel consists of:\n1. **ReviewerAgent** - Base class for individual reviewers\n2. **PanelSecretary** - Consolidates feedback from all reviewers\n3. **get_panel_review_of_article()** - Orchestrates the review workflow\n\nLet's explore each component.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}