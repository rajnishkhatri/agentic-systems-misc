{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pattern 17 & 32: LLM-as-Judge & Guardrails (Composable App Tutorial)\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this tutorial, you will:\n",
        "- Understand the LLM-as-Judge evaluation paradigm\n",
        "- Learn when to use judges vs. rule-based validation\n",
        "- Implement InputGuardrail for content policy enforcement\n",
        "- Design boolean judge prompts with accept conditions\n",
        "- Execute guardrails in parallel with asyncio.gather()\n",
        "- Create custom guardrails for domain-specific validation\n",
        "\n",
        "## Prerequisites\n",
        "- **Python**: Intermediate proficiency with async/await\n",
        "- **LLM basics**: Understanding of prompting and structured outputs\n",
        "- **Setup**: OpenAI API key configured in `.env`\n",
        "- **Prior tutorials**: Recommended to complete [Multi-Agent Workflow](../concepts/multi_agent_workflow.md)\n",
        "\n",
        "## Estimated Time\n",
        "25-30 minutes (reading + execution)\n",
        "\n",
        "## Cost Estimate\n",
        "\u26a0\ufe0f **API costs**: ~$0.01-0.05 (5-8 guardrail checks using SMALL_MODEL = gpt-4o-mini)\n",
        "\n",
        "> **Book Reference**: This pattern is detailed in *Generative AI Design Patterns*\n",
        "> (Lakshmanan & Hapke, 2025):\n",
        "> - Chapter 17: \"LLM-as-Judge\" (evaluation)\n",
        "> - Chapter 32: \"Guardrails\" (input/output validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What is LLM-as-Judge?\n",
        "\n",
        "**Task 2.2.2**: Conceptual section - LLM-as-judge paradigm, when to use vs. rule-based\n",
        "\n",
        "**LLM-as-Judge** is a pattern where a language model acts as an automated evaluator or validator. Instead of writing complex rule-based logic, we leverage an LLM's reasoning capabilities to assess quality, safety, or compliance.\n",
        "\n",
        "### Two Primary Use Cases\n",
        "\n",
        "#### 1. Evaluation (Pattern 17)\n",
        "Assess **output quality** after generation:\n",
        "- \"Is this response helpful?\"\n",
        "- \"Does the article contain hallucinations?\"\n",
        "- \"Rate the tone from 1-5\"\n",
        "\n",
        "#### 2. Guardrails (Pattern 32)\n",
        "Validate **input/output** before/after processing:\n",
        "- \"Is this topic appropriate for K-12?\"\n",
        "- \"Does the query contain harmful content?\"\n",
        "- \"Is the response compliant with policy?\"\n",
        "\n",
        "### When to Use LLM-as-Judge\n",
        "\n",
        "| Scenario | LLM-as-Judge \u2705 | Rule-Based \u2705 |\n",
        "|----------|----------------|---------------|\n",
        "| Subjective quality (helpfulness, tone) | \u2705 Better | \u274c Difficult |\n",
        "| Complex criteria (\"age-appropriate\") | \u2705 Better | \u274c Hard to define |\n",
        "| Rapid iteration on criteria | \u2705 Change prompt | \u274c Rewrite code |\n",
        "| Simple pattern matching (profanity) | \u274c Overkill | \u2705 Faster/cheaper |\n",
        "| Factual accuracy with ground truth | \u274c Use metrics | \u2705 Exact match |\n",
        "| Deterministic validation (email format) | \u274c Use regex | \u2705 Precise |\n",
        "\n",
        "### Composable App Use Case\n",
        "\n",
        "**TaskAssigner** uses an InputGuardrail to validate user queries:\n",
        "\n",
        "```python\n",
        "# From agents/task_assigner.py:31-33\n",
        "self.topic_guardrail = InputGuardrail(\n",
        "    name=\"topic_guardrail\",\n",
        "    accept_condition=\"The topic is appropriate for K-12 educational content\"\n",
        ")\n",
        "```\n",
        "\n",
        "**Why LLM judge?** \"Appropriate for K-12\" is subjective and context-dependent:\n",
        "- \u2705 \"World War II\" \u2192 Appropriate (historical education)\n",
        "- \u2705 \"Solving quadratic equations\" \u2192 Appropriate (math)\n",
        "- \u274c \"How to hack a website\" \u2192 Inappropriate (harmful)\n",
        "- \u274c \"Adult content\" \u2192 Inappropriate (age-inappropriate)\n",
        "\n",
        "Rule-based approach would require hundreds of if/else statements. LLM judge handles nuance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup Cell\n",
        "\n",
        "**Task 2.2.1**: Setup cell with imports, API key, cost warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to path for imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect execution context\n",
        "try:\n",
        "    current_dir = Path(__file__).parent.resolve()\n",
        "except NameError:\n",
        "    current_dir = Path.cwd()\n",
        "\n",
        "# Find repo root\n",
        "repo_root = current_dir\n",
        "while not (repo_root / '.env').exists() and repo_root != repo_root.parent:\n",
        "    repo_root = repo_root.parent\n",
        "\n",
        "# Add composable_app to path\n",
        "composable_app_path = repo_root / 'composable_app'\n",
        "sys.path.insert(0, str(composable_app_path))\n",
        "\n",
        "# IMPORTANT: Change working directory to composable_app for PromptService\n",
        "# PromptService uses relative path 'prompts/' which must resolve to composable_app/prompts/\n",
        "os.chdir(composable_app_path)\n",
        "print(f\"\u2705 Working directory set to: {os.getcwd()}\")\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "env_path = repo_root / '.env'\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Check for OpenAI API key (InputGuardrail uses OpenAI via llms.py)\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise EnvironmentError(\n",
        "        \"\u274c OPENAI_API_KEY not found.\\n\"\n",
        "        \"   Get your key at: https://platform.openai.com/api-keys\\n\"\n",
        "        \"   Add to .env: OPENAI_API_KEY=sk-...\"\n",
        "    )\n",
        "\n",
        "print(f\"\u2705 Environment loaded from: {env_path}\")\n",
        "print(\"\u2705 OpenAI API key found\")\n",
        "\n",
        "# Guardrail dependencies\n",
        "from utils.guardrails import InputGuardrail, InputGuardrailException\n",
        "from utils.prompt_service import PromptService\n",
        "from utils import llms\n",
        "import asyncio\n",
        "\n",
        "# Verify prompt templates are accessible\n",
        "prompts_dir = Path('prompts')\n",
        "if not prompts_dir.exists():\n",
        "    raise FileNotFoundError(f\"Prompts directory not found at: {prompts_dir.absolute()}\")\n",
        "\n",
        "print(\"\u2705 Setup complete\")\n",
        "print(f\"\u26a0\ufe0f This notebook will make API calls. Estimated cost: $0.01-0.05\")\n",
        "print(f\"   Using SMALL_MODEL: {llms.SMALL_MODEL} (fast & cheap)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## InputGuardrail Implementation\n",
        "\n",
        "**Task 2.2.3**: Code section - InputGuardrail implementation (utils/guardrails.py:14-43)\n",
        "\n",
        "### Architecture\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[User Query] --> B{InputGuardrail}\n",
        "    B -->|LLM Judge| C{Accept Condition<br/>Met?}\n",
        "    C -->|Yes| D[\u2705 Process Query]\n",
        "    C -->|No| E[\u274c Reject with<br/>Exception]\n",
        "    \n",
        "    D --> F[TaskAssigner]\n",
        "    F --> G[Writer]\n",
        "    \n",
        "    style C fill:#fff3cd\n",
        "    style D fill:#d4edda\n",
        "    style E fill:#f8d7da\n",
        "```\n",
        "\n",
        "### InputGuardrail Class\n",
        "\n",
        "From [`utils/guardrails.py:14-43`](../../utils/guardrails.py#L14-L43):\n",
        "\n",
        "```python\n",
        "class InputGuardrail:\n",
        "    def __init__(self, name: str, accept_condition: str):\n",
        "        \"\"\"Initialize guardrail with acceptance criteria.\n",
        "        \n",
        "        Args:\n",
        "            name: Guardrail identifier for logging\n",
        "            accept_condition: Condition text (e.g., 'The topic is appropriate for K-12')\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.accept_condition = accept_condition\n",
        "        \n",
        "        # Create judge agent that returns boolean\n",
        "        prompt = PromptService.render_prompt(\n",
        "            \"InputGuardrail_prompt\",\n",
        "            accept_condition=accept_condition\n",
        "        )\n",
        "        \n",
        "        self.agent = Agent(\n",
        "            llms.SMALL_MODEL,        # Fast & cheap for boolean decisions\n",
        "            output_type=bool,        # Structured output: True or False\n",
        "            system_prompt=prompt,\n",
        "            retries=2\n",
        "        )\n",
        "    \n",
        "    async def is_acceptable(self, prompt: str, raise_exception: bool = False) -> bool:\n",
        "        \"\"\"Check if input meets acceptance condition.\n",
        "        \n",
        "        Returns:\n",
        "            True if acceptable, False otherwise\n",
        "            \n",
        "        Raises:\n",
        "            InputGuardrailException: If raise_exception=True and input rejected\n",
        "        \"\"\"\n",
        "        result = await self.agent.run(prompt)\n",
        "        is_acceptable = result.output\n",
        "        \n",
        "        # Log decision\n",
        "        logger.info(f\"Guardrail {self.name}: {is_acceptable}\")\n",
        "        \n",
        "        if not is_acceptable and raise_exception:\n",
        "            raise InputGuardrailException(f\"Input rejected by {self.name}\")\n",
        "        \n",
        "        return is_acceptable\n",
        "```\n",
        "\n",
        "### Key Design Decisions\n",
        "\n",
        "1. **Why SMALL_MODEL?**\n",
        "   - Boolean decisions don't need advanced reasoning\n",
        "   - Faster response time (100-300ms vs. 500-1000ms)\n",
        "   - Lower cost (~10x cheaper than BEST_MODEL)\n",
        "\n",
        "2. **Why `output_type=bool`?**\n",
        "   - Pydantic AI ensures structured output (not text parsing)\n",
        "   - Automatic retry if LLM returns non-boolean\n",
        "   - Type-safe in Python code\n",
        "\n",
        "3. **Why `raise_exception` parameter?**\n",
        "   - **True**: Use in critical paths (block execution)\n",
        "   - **False**: Use in monitoring (log but continue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Boolean Judge Design\n",
        "\n",
        "**Task 2.2.4**: Code section - Boolean judge design with accept_condition prompts\n",
        "\n",
        "### The Guardrail Prompt\n",
        "\n",
        "From [`prompts/InputGuardrail_prompt.j2`](../../prompts/InputGuardrail_prompt.j2):\n",
        "\n",
        "```jinja2\n",
        "You are a content moderation expert.\n",
        "\n",
        "Your task is to evaluate whether the user's input meets this condition:\n",
        "{{ accept_condition }}\n",
        "\n",
        "Respond with:\n",
        "- True: If the condition is met\n",
        "- False: If the condition is NOT met\n",
        "\n",
        "Be objective and apply the condition consistently.\n",
        "```\n",
        "\n",
        "### Designing Effective Accept Conditions\n",
        "\n",
        "**Good accept conditions** are:\n",
        "- \u2705 **Clear**: \"The topic is appropriate for K-12 education\"\n",
        "- \u2705 **Specific**: \"The query does not contain profanity or hate speech\"\n",
        "- \u2705 **Binary**: Can be answered True/False\n",
        "- \u2705 **Objective**: Different judges would agree\n",
        "\n",
        "**Bad accept conditions**:\n",
        "- \u274c **Vague**: \"The input is good\" (what is \"good\"?)\n",
        "- \u274c **Complex**: \"The topic is educational AND engaging AND appropriate\" (too many criteria)\n",
        "- \u274c **Subjective**: \"The query is interesting\" (varies by person)\n",
        "\n",
        "### Examples\n",
        "\n",
        "Let's create and test guardrails with different accept conditions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create guardrail for K-12 appropriateness\n",
        "k12_guardrail = InputGuardrail(\n",
        "    name=\"k12_appropriateness\",\n",
        "    accept_condition=\"The topic is appropriate for K-12 educational content\"\n",
        ")\n",
        "\n",
        "print(\"\u2705 Created K-12 Appropriateness Guardrail\")\n",
        "print(f\"   Guardrail ID: {k12_guardrail.id}\")\n",
        "print(f\"   Using model: {llms.SMALL_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## K-12 Content Appropriateness Demo\n",
        "\n",
        "**Task 2.2.5**: Code section - K-12 content appropriateness validation demo\n",
        "\n",
        "Let's test the guardrail with various topics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cases: Some appropriate, some not\n",
        "test_topics = [\n",
        "    \"Battle of the Bulge\",                    # Should be appropriate (history)\n",
        "    \"Solving quadratic equations\",            # Should be appropriate (math)\n",
        "    \"How photosynthesis works\",               # Should be appropriate (science)\n",
        "    \"How to hack into a computer system\",     # Should NOT be appropriate (harmful)\n",
        "    \"Explicit adult content\",                 # Should NOT be appropriate (age-inappropriate)\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing K-12 Appropriateness Guardrail\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "\n",
        "for topic in test_topics:\n",
        "    print(f\"\\n\ud83d\udcdd Topic: '{topic}'\")\n",
        "    \n",
        "    # Check guardrail (no exception, just return boolean)\n",
        "    is_appropriate = await k12_guardrail.is_acceptable(topic, raise_exception=False)\n",
        "    \n",
        "    result_icon = \"\u2705\" if is_appropriate else \"\u274c\"\n",
        "    result_text = \"ACCEPTED\" if is_appropriate else \"REJECTED\"\n",
        "    \n",
        "    print(f\"   {result_icon} Result: {result_text}\")\n",
        "    \n",
        "    results.append({\n",
        "        \"topic\": topic,\n",
        "        \"appropriate\": is_appropriate\n",
        "    })\n",
        "    \n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Summary: {sum(r['appropriate'] for r in results)}/{len(results)} topics accepted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Results\n",
        "\n",
        "**Expected behavior**:\n",
        "- \u2705 Educational topics (history, math, science) \u2192 Accepted\n",
        "- \u274c Harmful topics (hacking, adult content) \u2192 Rejected\n",
        "\n",
        "**Why this works**:\n",
        "- LLM has been trained on educational standards\n",
        "- Understands context and nuance\n",
        "- \"Battle of the Bulge\" \u2192 Recognizes as legitimate history topic\n",
        "- \"How to hack\" \u2192 Recognizes as potentially harmful instruction\n",
        "\n",
        "**Note**: LLM judges can occasionally make mistakes. For production:\n",
        "1. Test with comprehensive examples\n",
        "2. Monitor and log all decisions\n",
        "3. Combine with rule-based checks for critical cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Parallel Guardrail Execution\n",
        "\n",
        "**Task 2.2.6**: Code section - Parallel guardrail execution with asyncio.gather()\n",
        "\n",
        "### Why Parallel Execution?\n",
        "\n",
        "When multiple guardrails validate the same input:\n",
        "- \u274c **Sequential**: Total latency = Sum of all guardrails (slow)\n",
        "- \u2705 **Parallel**: Total latency = Max of slowest guardrail (faster)\n",
        "\n",
        "**Example**: 3 guardrails, each takes 200ms:\n",
        "- Sequential: 600ms total\n",
        "- Parallel: 200ms total (3x faster!)\n",
        "\n",
        "### Implementation\n",
        "\n",
        "From [`agents/task_assigner.py:65-67`](../../agents/task_assigner.py#L65-L67):\n",
        "\n",
        "```python\n",
        "# Parallel execution: guardrail + agent classification\n",
        "_, result = await asyncio.gather(\n",
        "    self.topic_guardrail.is_acceptable(topic, raise_exception=True),  # Blocks if rejected\n",
        "    self.agent.run(prompt)                                             # Runs concurrently\n",
        ")\n",
        "```\n",
        "\n",
        "### Demo: Multiple Guardrails in Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create multiple guardrails\n",
        "guardrails = [\n",
        "    InputGuardrail(\n",
        "        name=\"k12_appropriate\",\n",
        "        accept_condition=\"The topic is appropriate for K-12 educational content\"\n",
        "    ),\n",
        "    InputGuardrail(\n",
        "        name=\"no_profanity\",\n",
        "        accept_condition=\"The text does not contain profanity or offensive language\"\n",
        "    ),\n",
        "    InputGuardrail(\n",
        "        name=\"educational_intent\",\n",
        "        accept_condition=\"The query has clear educational intent\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"\u2705 Created {len(guardrails)} guardrails\\n\")\n",
        "\n",
        "# Test topic\n",
        "topic = \"Explain the water cycle\"\n",
        "print(f\"\ud83d\udcdd Testing: '{topic}'\\n\")\n",
        "\n",
        "# Sequential execution (for comparison)\n",
        "import time\n",
        "\n",
        "print(\"\u23f1\ufe0f Sequential Execution:\")\n",
        "start = time.time()\n",
        "for guard in guardrails:\n",
        "    result = await guard.is_acceptable(topic)\n",
        "    print(f\"   {guard.name}: {result}\")\n",
        "sequential_time = time.time() - start\n",
        "print(f\"   Total: {sequential_time:.2f}s\\n\")\n",
        "\n",
        "# Parallel execution\n",
        "print(\"\u26a1 Parallel Execution:\")\n",
        "start = time.time()\n",
        "results = await asyncio.gather(\n",
        "    *[guard.is_acceptable(topic) for guard in guardrails]\n",
        ")\n",
        "parallel_time = time.time() - start\n",
        "\n",
        "for guard, result in zip(guardrails, results):\n",
        "    print(f\"   {guard.name}: {result}\")\n",
        "print(f\"   Total: {parallel_time:.2f}s\\n\")\n",
        "\n",
        "speedup = sequential_time / parallel_time\n",
        "print(f\"\ud83d\ude80 Speedup: {speedup:.1f}x faster with parallel execution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Handling with Parallel Guardrails\n",
        "\n",
        "**Question**: What happens if one guardrail rejects but we need all to pass?\n",
        "\n",
        "**Answer**: Use `raise_exception=True` - `asyncio.gather()` will propagate exception immediately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Exception handling\n",
        "bad_topic = \"How to hack into systems\"\n",
        "\n",
        "print(f\"\ud83d\udcdd Testing bad topic: '{bad_topic}'\\n\")\n",
        "\n",
        "try:\n",
        "    # Parallel execution with exception on failure\n",
        "    results = await asyncio.gather(\n",
        "        *[guard.is_acceptable(bad_topic, raise_exception=True) for guard in guardrails]\n",
        "    )\n",
        "    print(\"\u2705 All guardrails passed\")\n",
        "except InputGuardrailException as e:\n",
        "    print(f\"\u274c Guardrail rejected: {e}\")\n",
        "    print(\"   Query blocked before processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Creating Custom Guardrails\n",
        "\n",
        "**Task 2.2.7**: Code section - Creating custom guardrails (toxicity, domain-specific)\n",
        "\n",
        "### Custom Guardrail Examples\n",
        "\n",
        "You can create guardrails for any domain or policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Toxicity detection\n",
        "toxicity_guard = InputGuardrail(\n",
        "    name=\"toxicity_detection\",\n",
        "    accept_condition=\"The text does not contain toxic, hateful, or offensive language\"\n",
        ")\n",
        "\n",
        "# Example 2: Medical compliance\n",
        "medical_guard = InputGuardrail(\n",
        "    name=\"medical_compliance\",\n",
        "    accept_condition=\"The query does not request medical diagnosis or treatment advice\"\n",
        ")\n",
        "\n",
        "# Example 3: Legal compliance\n",
        "legal_guard = InputGuardrail(\n",
        "    name=\"legal_compliance\",\n",
        "    accept_condition=\"The query does not request legal advice or services\"\n",
        ")\n",
        "\n",
        "# Example 4: Domain-specific (coding assistant)\n",
        "coding_guard = InputGuardrail(\n",
        "    name=\"coding_appropriate\",\n",
        "    accept_condition=\"The query is related to software development, programming, or technology\"\n",
        ")\n",
        "\n",
        "# Example 5: PII detection\n",
        "pii_guard = InputGuardrail(\n",
        "    name=\"no_pii\",\n",
        "    accept_condition=\"The text does not contain personally identifiable information (names, addresses, SSN, credit cards)\"\n",
        ")\n",
        "\n",
        "print(\"\u2705 Created 5 custom guardrails:\")\n",
        "for guard in [toxicity_guard, medical_guard, legal_guard, coding_guard, pii_guard]:\n",
        "    print(f\"   - {guard.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Custom Guardrails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cases for different guardrails\n",
        "test_cases = [\n",
        "    (\"Can you help me debug this Python function?\", coding_guard, True),\n",
        "    (\"How do I make spaghetti carbonara?\", coding_guard, False),\n",
        "    (\"I need advice on a legal contract\", legal_guard, False),\n",
        "    (\"What is contract law?\", legal_guard, True),  # Educational, not advice\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing Custom Guardrails\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for query, guard, expected in test_cases:\n",
        "    result = await guard.is_acceptable(query)\n",
        "    status = \"\u2705 PASS\" if result == expected else \"\u274c FAIL\"\n",
        "    \n",
        "    print(f\"\\n{status} {guard.name}\")\n",
        "    print(f\"   Query: '{query}'\")\n",
        "    print(f\"   Expected: {expected}, Got: {result}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Common Pitfalls\n",
        "\n",
        "**Task 2.2.8**: Common Pitfalls - Using BEST_MODEL (slow), missing exception handling\n",
        "\n",
        "### \u274c Pitfall 1: Using BEST_MODEL for Guardrails\n",
        "\n",
        "**Problem**: Using expensive, slow model for simple boolean decisions\n",
        "\n",
        "```python\n",
        "# \u274c BAD: Using BEST_MODEL\n",
        "agent = Agent(\n",
        "    llms.BEST_MODEL,  # \"gemini-2.0-flash\" - overkill for boolean\n",
        "    output_type=bool\n",
        ")\n",
        "# Cost: ~$0.10 per 1000 checks\n",
        "# Latency: 500-1000ms\n",
        "```\n",
        "\n",
        "**Solution**: Use SMALL_MODEL\n",
        "\n",
        "```python\n",
        "# \u2705 GOOD: Using SMALL_MODEL\n",
        "agent = Agent(\n",
        "    llms.SMALL_MODEL,  # \"gemini-2.5-flash-lite\" - perfect for boolean\n",
        "    output_type=bool\n",
        ")\n",
        "# Cost: ~$0.01 per 1000 checks (10x cheaper)\n",
        "# Latency: 100-300ms (3x faster)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \u274c Pitfall 2: Missing Exception Handling\n",
        "\n",
        "**Problem**: Guardrail exception crashes entire application\n",
        "\n",
        "```python\n",
        "# \u274c BAD: No exception handling\n",
        "writer = await assigner.assign_writer(user_query)\n",
        "# If guardrail rejects, app crashes with InputGuardrailException\n",
        "```\n",
        "\n",
        "**Solution**: Catch and handle gracefully\n",
        "\n",
        "```python\n",
        "# \u2705 GOOD: Handle rejection gracefully\n",
        "try:\n",
        "    writer = await assigner.assign_writer(user_query)\n",
        "except InputGuardrailException as e:\n",
        "    logger.warning(f\"Query rejected: {e}\")\n",
        "    return {\"error\": \"Your query does not meet content guidelines\"}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \u274c Pitfall 3: Vague Accept Conditions\n",
        "\n",
        "**Problem**: Inconsistent or unpredictable guardrail behavior\n",
        "\n",
        "```python\n",
        "# \u274c BAD: Too vague\n",
        "guard = InputGuardrail(\n",
        "    name=\"quality\",\n",
        "    accept_condition=\"The input is good quality\"\n",
        ")\n",
        "# What is \"good\"? Results will be inconsistent\n",
        "```\n",
        "\n",
        "**Solution**: Be specific and objective\n",
        "\n",
        "```python\n",
        "# \u2705 GOOD: Clear, specific criteria\n",
        "guard = InputGuardrail(\n",
        "    name=\"query_clarity\",\n",
        "    accept_condition=\"The query is a complete sentence with clear intent\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \u274c Pitfall 4: Not Logging Guardrail Decisions\n",
        "\n",
        "**Problem**: Can't debug why queries were rejected\n",
        "\n",
        "**Solution**: Always log decisions (InputGuardrail does this automatically)\n",
        "\n",
        "```python\n",
        "# Check logs/guards.json for all decisions\n",
        "import json\n",
        "\n",
        "with open(\"logs/guards.json\") as f:\n",
        "    for line in f:\n",
        "        decision = json.loads(line)\n",
        "        print(f\"{decision['timestamp']}: {decision['guardrail_name']} \u2192 {decision['result']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udca1 Tip: Combine LLM Judge with Rule-Based Checks\n",
        "\n",
        "**Best practice**: Use both for defense-in-depth\n",
        "\n",
        "```python\n",
        "def combined_validation(text: str) -> bool:\n",
        "    # Layer 1: Fast rule-based checks\n",
        "    if contains_profanity(text):  # Regex check - instant\n",
        "        return False\n",
        "    \n",
        "    if len(text) > 10000:  # Length check\n",
        "        return False\n",
        "    \n",
        "    # Layer 2: LLM judge for nuanced validation\n",
        "    return await guardrail.is_acceptable(text)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Self-Assessment\n",
        "\n",
        "**Task 2.2.9**: Self-assessment and book references\n",
        "\n",
        "### Question 1: Concept Check\n",
        "**When should you use LLM-as-Judge instead of rule-based validation?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: Use LLM-as-Judge when:\n",
        "\n",
        "1. **Criteria are subjective**: \"Is this tone appropriate?\" - No clear rules\n",
        "2. **Context matters**: \"Is 'shooting' appropriate?\" - Depends on context (photography vs. violence)\n",
        "3. **Rapid iteration needed**: Changing prompts is easier than rewriting code\n",
        "4. **Nuance required**: \"Age-appropriate\" varies by topic complexity\n",
        "\n",
        "**Use rule-based when**:\n",
        "- Deterministic validation needed (email format, phone number)\n",
        "- Speed is critical (<10ms response time)\n",
        "- Exact pattern matching (profanity list, banned words)\n",
        "- Cost must be minimal ($0.00001 vs. $0.0001 per check)\n",
        "\n",
        "**Best**: Combine both - rule-based for fast filtering, LLM for nuanced decisions\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Implementation\n",
        "**Why does TaskAssigner use `asyncio.gather()` for guardrail + agent classification?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: Parallel execution for speed:\n",
        "\n",
        "```python\n",
        "# From agents/task_assigner.py:65-67\n",
        "_, result = await asyncio.gather(\n",
        "    self.topic_guardrail.is_acceptable(topic, raise_exception=True),\n",
        "    self.agent.run(prompt)\n",
        ")\n",
        "```\n",
        "\n",
        "**Why parallel?**\n",
        "- Guardrail check: ~200ms\n",
        "- Agent classification: ~300ms\n",
        "- **Sequential**: 200ms + 300ms = 500ms total\n",
        "- **Parallel**: max(200ms, 300ms) = 300ms total (1.7x faster)\n",
        "\n",
        "**Behavior**:\n",
        "- If guardrail rejects (raises exception), `gather()` cancels agent call immediately\n",
        "- If both succeed, we get classification result\n",
        "- Saves ~200ms per request in happy path\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3: Design Trade-offs\n",
        "**What are the downsides of using LLM-as-Judge for guardrails?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: Key limitations:\n",
        "\n",
        "1. **Not 100% deterministic**: Same query might get different results occasionally\n",
        "   - Solution: Use temperature=0, clear conditions, monitor decisions\n",
        "\n",
        "2. **Added latency**: 100-300ms vs. <1ms for regex\n",
        "   - Solution: Use SMALL_MODEL, parallel execution, caching\n",
        "\n",
        "3. **API costs**: $0.0001 per check (vs. $0 for rule-based)\n",
        "   - Solution: Acceptable for most use cases, add rule-based pre-filter\n",
        "\n",
        "4. **Potential bias**: LLM may have cultural or political biases\n",
        "   - Solution: Test extensively, use diverse examples, monitor in production\n",
        "\n",
        "5. **Failure modes**: API down, rate limits, timeout\n",
        "   - Solution: Implement retries, fallback to rule-based, circuit breaker\n",
        "\n",
        "**When acceptable**: Most applications where nuanced validation matters more than perfect determinism\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4: Advanced\n",
        "**How would you implement a guardrail that rejects PII (personally identifiable information)?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: Hybrid approach (rule-based + LLM):\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "# Layer 1: Fast regex checks for obvious PII\n",
        "def quick_pii_check(text: str) -> bool:\n",
        "    \"\"\"Rule-based pre-filter for common PII patterns.\"\"\"\n",
        "    \n",
        "    # SSN pattern: XXX-XX-XXXX\n",
        "    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n",
        "        return True\n",
        "    \n",
        "    # Credit card: 16 digits\n",
        "    if re.search(r'\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}', text):\n",
        "        return True\n",
        "    \n",
        "    # Email addresses\n",
        "    if re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text):\n",
        "        return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "# Layer 2: LLM judge for nuanced PII\n",
        "pii_guardrail = InputGuardrail(\n",
        "    name=\"pii_detection\",\n",
        "    accept_condition=\"\"\"The text does not contain personally identifiable information including:\n",
        "    - Full names with context (e.g., 'John Smith lives at...')\n",
        "    - Home addresses\n",
        "    - Phone numbers\n",
        "    - Social Security Numbers\n",
        "    - Credit card numbers\n",
        "    - Driver's license numbers\n",
        "    - Medical record numbers\n",
        "    \n",
        "    Generic names without context are acceptable (e.g., 'Alice and Bob example').\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Combined validation\n",
        "async def contains_pii(text: str) -> bool:\n",
        "    # Quick check first (fast reject)\n",
        "    if quick_pii_check(text):\n",
        "        return True\n",
        "    \n",
        "    # LLM check for nuanced cases\n",
        "    return not await pii_guardrail.is_acceptable(text)\n",
        "```\n",
        "\n",
        "**Why hybrid?**\n",
        "- Regex catches 90% of cases instantly (no API cost)\n",
        "- LLM catches edge cases (\"My address is 123 Main Street\" - no regex pattern)\n",
        "- Best of both worlds: speed + nuance\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Book References & Further Reading\n",
        "\n",
        "### Generative AI Design Patterns (Lakshmanan & Hapke, O'Reilly 2025)\n",
        "\n",
        "\ud83d\udcd6 **Chapter 17: LLM-as-Judge**\n",
        "- Judge design patterns\n",
        "- Evaluation vs. validation use cases\n",
        "- Structured output parsing\n",
        "- Judge bias and mitigation\n",
        "\n",
        "\ud83d\udcd6 **Chapter 32: Guardrails**\n",
        "- Input/output validation architecture\n",
        "- Multi-layer defense strategies\n",
        "- Performance optimization\n",
        "- Production deployment\n",
        "\n",
        "**Related Chapters**:\n",
        "- **Chapter 18**: \"Reflection\" - Self-correction with judges\n",
        "- **Chapter 25**: \"Prompt Caching\" - Template-based guardrail prompts\n",
        "- **Chapter 30**: \"Structured Outputs\" - Boolean and Pydantic models\n",
        "\n",
        "### External Resources\n",
        "- [Pydantic AI Structured Outputs](https://ai.pydantic.dev/)\n",
        "- [Guardrails AI Library](https://github.com/guardrails-ai/guardrails) - Additional validation layers\n",
        "- [NIST AI Safety Guidelines](https://www.nist.gov/artificial-intelligence)\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "### Continue Learning\n",
        "1. **[Horizontal Services](../concepts/horizontal_services.md)** - Guardrails in the broader architecture\n",
        "2. **[Evaluation Tutorial](evaluation_tutorial.ipynb)** - Using LLM-as-judge for quality metrics\n",
        "3. **[Multi-Agent Pattern](multi_agent_pattern.ipynb)** - ReviewerPanel as judge ensemble\n",
        "\n",
        "### Hands-On Practice\n",
        "1. **Create domain-specific guardrail**: Design for your application (medical, legal, etc.)\n",
        "2. **Test edge cases**: Find queries that fool the guardrail\n",
        "3. **Measure latency**: Compare SMALL_MODEL vs. BEST_MODEL vs. rule-based\n",
        "4. **Monitor in production**: Log all decisions, track false positives/negatives\n",
        "\n",
        "### Advanced Exercises\n",
        "1. **Multi-criteria guardrails**: Single judge evaluating multiple conditions\n",
        "2. **Confidence scores**: Return probability instead of boolean\n",
        "3. **Ensemble judges**: Multiple LLMs voting on decision\n",
        "4. **Adaptive guardrails**: Adjust strictness based on user reputation\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've learned Pattern 17 & 32: LLM-as-Judge and Guardrails. You can now build safe, policy-compliant LLM applications with intelligent input validation.\n",
        "\n",
        "**Tutorial Version**: 1.0  \n",
        "**Last Updated**: 2025-11-04  \n",
        "**Estimated Time**: 25-30 minutes  \n",
        "**API Cost**: ~$0.05-0.10"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}