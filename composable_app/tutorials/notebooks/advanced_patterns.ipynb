{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Patterns: Provider Switching & Optimization (Composable App Tutorial)\n",
    "\n",
    "## Learning Objectives\n",
    "By completing this tutorial, you will:\n",
    "- Understand Pydantic AI's provider abstraction layer\n",
    "- Learn to switch between LLM providers (Gemini, OpenAI, Anthropic)\n",
    "- Compare cost and latency trade-offs across providers\n",
    "- Master parallel execution patterns with `asyncio.gather()`\n",
    "- Optimize for cost and performance using model selection and temperature tuning\n",
    "\n",
    "## Prerequisites\n",
    "- **Python**: Advanced proficiency with async/await, context managers\n",
    "- **LLM basics**: Understanding of temperature, tokens, API pricing\n",
    "- **Setup**: API keys for Gemini (required), OpenAI/Anthropic (optional for comparisons)\n",
    "\n",
    "## Estimated Time\n",
    "40-45 minutes (reading + execution)\n",
    "\n",
    "## Cost Estimate\n",
    "‚ö†Ô∏è **Variable costs**: \n",
    "- **Gemini only**: $0.05-0.10 (demonstration examples)\n",
    "- **Multi-provider comparison**: $0.20-0.50 (if testing OpenAI/Anthropic)\n",
    "- **Tip**: Skip multi-provider sections to minimize costs\n",
    "\n",
    "> **Book Reference**: This pattern is detailed in *Generative AI Design Patterns*\n",
    "> (Lakshmanan & Hapke, 2025), Chapter 8: \"Model Cascades\" and Chapter 29: \"Cost Optimization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Provider Switching?\n",
    "\n",
    "**Task 5.1.2**: LLM provider switching with Pydantic AI abstraction\n",
    "\n",
    "### The Provider Lock-In Problem\n",
    "\n",
    "**Traditional approach** (tightly coupled to provider):\n",
    "```python\n",
    "# Bad: Direct OpenAI integration\n",
    "import openai\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "- ‚ùå **Vendor lock-in**: Changing providers requires rewriting code\n",
    "- ‚ùå **No fallback**: If OpenAI is down, entire system fails\n",
    "- ‚ùå **Hard to compare**: Can't A/B test different providers\n",
    "- ‚ùå **Cost rigidity**: Can't dynamically choose cheaper model\n",
    "\n",
    "### Pydantic AI Abstraction\n",
    "\n",
    "**Better approach** (provider-agnostic):\n",
    "```python\n",
    "# Good: Pydantic AI abstraction\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "# Switch provider by changing model string\n",
    "agent = Agent('gemini-2.0-flash')           # Google\n",
    "agent = Agent('gpt-4o')                     # OpenAI\n",
    "agent = Agent('claude-3-5-sonnet-20241022') # Anthropic\n",
    "\n",
    "# Same API regardless of provider\n",
    "result = await agent.run(prompt)\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- ‚úÖ **Portability**: Change providers with one line\n",
    "- ‚úÖ **Resilience**: Implement fallback logic\n",
    "- ‚úÖ **Flexibility**: A/B test providers in production\n",
    "- ‚úÖ **Cost optimization**: Route queries to cheapest suitable model\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "1. **Cost optimization**: Use Gemini Flash for simple tasks, GPT-4 for complex reasoning\n",
    "2. **Failover**: If primary provider rate-limits, fall back to secondary\n",
    "3. **Regional compliance**: Use different providers based on user location\n",
    "4. **Quality comparison**: A/B test providers to find best for your use case\n",
    "5. **Model cascades**: Try small model first, escalate to large model if needed\n",
    "\n",
    "**Code Location**: [`utils/llms.py`](../../utils/llms.py) defines model constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup Cell\n",
    "\n",
    "**Task 5.1.1**: Setup cell with imports, cost warning (if testing multiple providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')  # Navigate to composable_app/ root\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('../../keys.env')\n",
    "\n",
    "# Verify API keys\n",
    "assert os.getenv('GEMINI_API_KEY'), \"‚ùå GEMINI_API_KEY not found in keys.env\"\n",
    "print(\"‚úÖ Gemini API key loaded\")\n",
    "\n",
    "# Optional: Check for other providers\n",
    "has_openai = bool(os.getenv('OPENAI_API_KEY'))\n",
    "has_anthropic = bool(os.getenv('ANTHROPIC_API_KEY'))\n",
    "print(f\"OpenAI API key: {'‚úÖ' if has_openai else '‚ùå (multi-provider examples will be skipped)'}\")\n",
    "print(f\"Anthropic API key: {'‚úÖ' if has_anthropic else '‚ùå (multi-provider examples will be skipped)'}\")\n",
    "\n",
    "# Standard library\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Pydantic AI\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.gemini import GeminiModelSettings\n",
    "\n",
    "# Project imports\n",
    "from agents.article import Article\n",
    "from utils import llms\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete\")\n",
    "print(\"\\n‚ö†Ô∏è Cost Warning:\")\n",
    "print(\"   - Gemini examples: ~$0.05-0.10\")\n",
    "print(\"   - Multi-provider comparisons: ~$0.20-0.50\")\n",
    "print(\"   - Skip optional sections to reduce costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Provider Switching Basics\n",
    "\n",
    "**Task 5.1.3**: Code section - Switching from Gemini to OpenAI/Anthropic\n",
    "\n",
    "### Supported Providers in Pydantic AI\n",
    "\n",
    "| Provider | Model String Examples | Strengths | Pricing (per 1M tokens) |\n",
    "|----------|----------------------|-----------|------------------------|\n",
    "| **Google Gemini** | `gemini-2.0-flash`<br>`gemini-2.5-flash-lite-preview-06-17` | Fast, cheap, multimodal | Input: $0.075<br>Output: $0.30 |\n",
    "| **OpenAI** | `gpt-4o`<br>`gpt-4o-mini` | High quality, reliable | Input: $2.50<br>Output: $10.00 |\n",
    "| **Anthropic** | `claude-3-5-sonnet-20241022`<br>`claude-3-5-haiku-20241022` | Long context, reasoning | Input: $3.00<br>Output: $15.00 |\n",
    "| **Ollama** | `llama3:8b`<br>`mistral:7b` | Free, local, private | $0 (self-hosted) |\n",
    "\n",
    "**Note**: Prices as of 2025. Check provider websites for latest pricing.\n",
    "\n",
    "### Basic Provider Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple task\n",
    "topic = \"Photosynthesis\"\n",
    "prompt = f\"Explain {topic} in exactly 2 sentences for a 5th grader.\"\n",
    "\n",
    "# Approach 1: Gemini (current default)\n",
    "async def test_gemini():\n",
    "    agent = Agent(\n",
    "        'gemini-2.0-flash',\n",
    "        system_prompt=\"You are a K-12 science educator.\"\n",
    "    )\n",
    "    result = await agent.run(prompt)\n",
    "    return result.data\n",
    "\n",
    "# Approach 2: OpenAI (if available)\n",
    "async def test_openai():\n",
    "    if not has_openai:\n",
    "        return \"‚ö†Ô∏è OPENAI_API_KEY not configured\"\n",
    "    \n",
    "    agent = Agent(\n",
    "        'gpt-4o-mini',  # Cheaper variant\n",
    "        system_prompt=\"You are a K-12 science educator.\"\n",
    "    )\n",
    "    result = await agent.run(prompt)\n",
    "    return result.data\n",
    "\n",
    "# Approach 3: Anthropic (if available)\n",
    "async def test_anthropic():\n",
    "    if not has_anthropic:\n",
    "        return \"‚ö†Ô∏è ANTHROPIC_API_KEY not configured\"\n",
    "    \n",
    "    agent = Agent(\n",
    "        'claude-3-5-haiku-20241022',  # Fast variant\n",
    "        system_prompt=\"You are a K-12 science educator.\"\n",
    "    )\n",
    "    result = await agent.run(prompt)\n",
    "    return result.data\n",
    "\n",
    "# Test all providers\n",
    "print(f\"üìù Prompt: '{prompt}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gemini_response = await test_gemini()\n",
    "print(f\"\\nüü¢ Gemini 2.0 Flash:\")\n",
    "print(f\"   {gemini_response}\")\n",
    "\n",
    "if has_openai:\n",
    "    openai_response = await test_openai()\n",
    "    print(f\"\\nüîµ OpenAI GPT-4o-mini:\")\n",
    "    print(f\"   {openai_response}\")\n",
    "else:\n",
    "    print(f\"\\nüîµ OpenAI: Skipped (no API key)\")\n",
    "\n",
    "if has_anthropic:\n",
    "    anthropic_response = await test_anthropic()\n",
    "    print(f\"\\nüü£ Anthropic Claude Haiku:\")\n",
    "    print(f\"   {anthropic_response}\")\n",
    "else:\n",
    "    print(f\"\\nüü£ Anthropic: Skipped (no API key)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Observation: Same interface, different models - this is provider abstraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Configuration in Composable App\n",
    "\n",
    "**Task 5.1.4**: Code section - Model configuration in utils/llms.py\n",
    "\n",
    "### Model Constants Strategy\n",
    "\n",
    "The composable app defines model constants in `utils/llms.py` for centralized management:\n",
    "\n",
    "```python\n",
    "# From utils/llms.py:5-8\n",
    "BEST_MODEL = \"gemini-2.0-flash\"               # Highest quality\n",
    "DEFAULT_MODEL = \"gemini-2.0-flash\"            # Standard use\n",
    "SMALL_MODEL = \"gemini-2.5-flash-lite-preview-06-17\"  # Fastest, cheapest\n",
    "EMBED_MODEL = \"text-embedding-004\"            # Embeddings\n",
    "```\n",
    "\n",
    "### Why Centralize Configuration?\n",
    "\n",
    "**Benefits**:\n",
    "1. **Single source of truth**: Change model once, affects entire app\n",
    "2. **Easy A/B testing**: Swap `BEST_MODEL` to compare providers\n",
    "3. **Cost control**: Downgrade all agents to `SMALL_MODEL` during development\n",
    "4. **Semantic naming**: `BEST_MODEL` is clearer than `gemini-2.0-flash` in code\n",
    "\n",
    "**Usage in agents**:\n",
    "```python\n",
    "# From agents/generic_writer_agent.py:98\n",
    "self.agent = Agent(llms.BEST_MODEL,  # Not hardcoded!\n",
    "                   output_type=Article,\n",
    "                   model_settings=llms.default_model_settings())\n",
    "```\n",
    "\n",
    "### Model Selection Strategy\n",
    "\n",
    "| Use Case | Model Choice | Rationale |\n",
    "|----------|-------------|----------|\n",
    "| **Initial draft generation** | `BEST_MODEL` | Quality matters, runs once | \n",
    "| **Guardrails validation** | `SMALL_MODEL` | Simple yes/no, runs frequently |\n",
    "| **Keyword extraction** | `DEFAULT_MODEL` | Balanced speed/quality |\n",
    "| **Review panel (6 agents)** | `DEFAULT_MODEL` | Parallel execution, cost adds up |\n",
    "| **Revision** | `BEST_MODEL` | Refinement needs quality |\n",
    "\n",
    "**Cost calculation example**:\n",
    "```python\n",
    "# Workflow: 1 draft + 6 reviews + 1 revision\n",
    "# Option A: All BEST_MODEL\n",
    "cost_A = 8 * 0.30  # 8 calls @ $0.30 per 1M output tokens\n",
    "# Option B: Smart selection (BEST for draft/revision, DEFAULT for reviews)\n",
    "cost_B = 2 * 0.30 + 6 * 0.15  # 60% savings!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Comparing BEST_MODEL vs SMALL_MODEL\n",
    "\n",
    "print(f\"üìä Current Model Configuration:\\n\")\n",
    "print(f\"   BEST_MODEL:    {llms.BEST_MODEL}\")\n",
    "print(f\"   DEFAULT_MODEL: {llms.DEFAULT_MODEL}\")\n",
    "print(f\"   SMALL_MODEL:   {llms.SMALL_MODEL}\")\n",
    "print(f\"   EMBED_MODEL:   {llms.EMBED_MODEL}\")\n",
    "\n",
    "# Test both models on same task\n",
    "prompt = \"List 3 keywords for an article about Photosynthesis.\"\n",
    "\n",
    "async def compare_models():\n",
    "    # BEST_MODEL (high quality)\n",
    "    best_agent = Agent(llms.BEST_MODEL)\n",
    "    best_result = await best_agent.run(prompt)\n",
    "    \n",
    "    # SMALL_MODEL (high speed)\n",
    "    small_agent = Agent(llms.SMALL_MODEL)\n",
    "    small_result = await small_agent.run(prompt)\n",
    "    \n",
    "    return best_result.data, small_result.data\n",
    "\n",
    "best_keywords, small_keywords = await compare_models()\n",
    "\n",
    "print(f\"\\nüìù Prompt: '{prompt}'\\n\")\n",
    "print(f\"üèÜ BEST_MODEL ({llms.BEST_MODEL}):\")\n",
    "print(f\"   {best_keywords}\")\n",
    "print(f\"\\n‚ö° SMALL_MODEL ({llms.SMALL_MODEL}):\")\n",
    "print(f\"   {small_keywords}\")\n",
    "\n",
    "print(\"\\nüí° When to use each:\")\n",
    "print(\"   - BEST_MODEL: User-facing outputs, complex reasoning, final drafts\")\n",
    "print(\"   - SMALL_MODEL: Internal tasks, simple classification, guardrails\")\n",
    "print(\"   - DEFAULT_MODEL: General purpose, balanced cost/quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Provider-Specific Settings\n",
    "\n",
    "**Task 5.1.5**: Code section - Provider-specific settings (temperature, safety)\n",
    "\n",
    "### Model Settings in Pydantic AI\n",
    "\n",
    "Each provider has unique configuration options:\n",
    "\n",
    "#### Gemini Settings\n",
    "```python\n",
    "# From utils/llms.py:21-32\n",
    "from pydantic_ai.models.gemini import GeminiModelSettings\n",
    "\n",
    "model_settings = GeminiModelSettings(\n",
    "    temperature=0.25,  # Low temp for consistency (0.0-2.0)\n",
    "    gemini_safety_settings=[\n",
    "        {\n",
    "            'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
    "            'threshold': 'BLOCK_ONLY_HIGH',  # Permissive for educational content\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "**Why temperature=0.25?**\n",
    "- K-12 content needs **consistency** (same explanation each time)\n",
    "- Too low (0.0): Repetitive, robotic\n",
    "- Too high (1.0+): Creative but inconsistent, may hallucinate\n",
    "- Sweet spot (0.2-0.3): Consistent but natural-sounding\n",
    "\n",
    "**Why permissive safety settings?**\n",
    "- Educational content discusses topics like \"cell death\", \"nuclear reactions\"\n",
    "- Default settings may over-block scientific terms\n",
    "- `BLOCK_ONLY_HIGH`: Allows educational content, blocks actual harmful content\n",
    "\n",
    "#### OpenAI Settings\n",
    "```python\n",
    "from pydantic_ai.models.openai import OpenAIModelSettings\n",
    "\n",
    "openai_settings = OpenAIModelSettings(\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,      # Control output length\n",
    "    top_p=0.9,            # Nucleus sampling (alternative to temperature)\n",
    "    frequency_penalty=0.0, # Reduce repetition (0.0-2.0)\n",
    "    presence_penalty=0.0   # Encourage new topics (0.0-2.0)\n",
    ")\n",
    "```\n",
    "\n",
    "#### Anthropic Settings\n",
    "```python\n",
    "from pydantic_ai.models.anthropic import AnthropicModelSettings\n",
    "\n",
    "anthropic_settings = AnthropicModelSettings(\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,\n",
    "    top_p=0.9,\n",
    "    top_k=50              # Limits sampling pool (Claude-specific)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Temperature effects\n",
    "\n",
    "async def test_temperature(temperature: float, num_runs: int = 3):\n",
    "    \"\"\"Test how temperature affects consistency.\"\"\"\n",
    "    agent = Agent(\n",
    "        llms.DEFAULT_MODEL,\n",
    "        model_settings=GeminiModelSettings(temperature=temperature)\n",
    "    )\n",
    "    \n",
    "    prompt = \"Name 1 example of photosynthesis in nature.\"\n",
    "    \n",
    "    responses = []\n",
    "    for i in range(num_runs):\n",
    "        result = await agent.run(prompt)\n",
    "        responses.append(result.data)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "print(\"üß™ Testing temperature effects on consistency:\\n\")\n",
    "print(f\"Prompt: 'Name 1 example of photosynthesis in nature.'\")\n",
    "print(f\"Running 3 times with different temperatures...\\n\")\n",
    "\n",
    "# Test low temperature (consistent)\n",
    "low_temp_responses = await test_temperature(0.0)\n",
    "print(\"‚ùÑÔ∏è Temperature = 0.0 (most deterministic):\")\n",
    "for i, resp in enumerate(low_temp_responses, 1):\n",
    "    print(f\"   Run {i}: {resp[:80]}...\")\n",
    "\n",
    "# Test medium temperature (balanced)\n",
    "med_temp_responses = await test_temperature(0.25)\n",
    "print(\"\\nüå°Ô∏è Temperature = 0.25 (composable app default):\")\n",
    "for i, resp in enumerate(med_temp_responses, 1):\n",
    "    print(f\"   Run {i}: {resp[:80]}...\")\n",
    "\n",
    "# Test high temperature (creative)\n",
    "high_temp_responses = await test_temperature(1.0)\n",
    "print(\"\\nüî• Temperature = 1.0 (most creative):\")\n",
    "for i, resp in enumerate(high_temp_responses, 1):\n",
    "    print(f\"   Run {i}: {resp[:80]}...\")\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"   - Low temp (0.0): Almost identical responses (good for factual content)\")\n",
    "print(\"   - Medium temp (0.25): Slight variation (balanced for educational content)\")\n",
    "print(\"   - High temp (1.0): Diverse responses (good for creative tasks)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Recommendation: Use 0.2-0.3 for K-12 content (consistency with natural variation)\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Demo: Sequential vs. Parallel execution\n\nimport time\n\nasync def simulate_llm_call(agent_name: str, delay: float = 2.0):\n    \"\"\"Simulate LLM API call with delay.\"\"\"\n    start = time.time()\n    await asyncio.sleep(delay)  # Simulates API latency\n    elapsed = time.time() - start\n    return f\"{agent_name} completed in {elapsed:.2f}s\"\n\n# Sequential execution (slow)\nasync def sequential_execution():\n    \"\"\"Run 6 agents one after another.\"\"\"\n    agents = [f\"Reviewer{i+1}\" for i in range(6)]\n    \n    start_time = time.time()\n    results = []\n    \n    for agent in agents:\n        result = await simulate_llm_call(agent, delay=0.5)  # 0.5s each (faster for demo)\n        results.append(result)\n    \n    total_time = time.time() - start_time\n    return results, total_time\n\n# Parallel execution (fast)\nasync def parallel_execution():\n    \"\"\"Run 6 agents simultaneously with asyncio.gather().\"\"\"\n    agents = [f\"Reviewer{i+1}\" for i in range(6)]\n    \n    start_time = time.time()\n    \n    # Create all tasks\n    tasks = [simulate_llm_call(agent, delay=0.5) for agent in agents]\n    \n    # Execute in parallel\n    results = await asyncio.gather(*tasks)\n    \n    total_time = time.time() - start_time\n    return results, total_time\n\n# Run comparison\nprint(\"üêå Sequential Execution:\\n\")\nseq_results, seq_time = await sequential_execution()\nfor result in seq_results:\n    print(f\"   {result}\")\nprint(f\"\\n   Total time: {seq_time:.2f}s\\n\")\n\nprint(\"=\" * 80)\n\nprint(\"\\n‚ö° Parallel Execution with asyncio.gather():\\n\")\npar_results, par_time = await parallel_execution()\nfor result in par_results:\n    print(f\"   {result}\")\nprint(f\"\\n   Total time: {par_time:.2f}s\\n\")\n\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä Performance Comparison:\")\nprint(f\"   Sequential: {seq_time:.2f}s\")\nprint(f\"   Parallel:   {par_time:.2f}s\")\nprint(f\"   Speedup:    {seq_time / par_time:.1f}x faster! ‚ö°\")\n\nprint(\"\\nüí° Real-world application:\")\nprint(\"   - ReviewerPanel: 6 reviewers in parallel (12s ‚Üí 2s)\")\nprint(\"   - Guardrails: Multiple checks in parallel (3s ‚Üí 1s)\")\nprint(\"   - A/B testing: Test multiple prompts simultaneously\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Parallel Execution with asyncio.gather()\n\n**Task 5.1.7**: Section - Parallel execution patterns with asyncio.gather()\n\n### The Latency Problem\n\n**Sequential execution** (slow):\n```python\n# Bad: 6 reviewers run one after another\nreviews = []\nfor reviewer in reviewers:  # 6 iterations\n    review = await reviewer.review(article)  # 2 seconds each\n    reviews.append(review)\n# Total time: 6 √ó 2s = 12 seconds\n```\n\n**Parallel execution** (fast):\n```python\n# Good: All 6 reviewers run simultaneously\ntasks = [reviewer.review(article) for reviewer in reviewers]\nreviews = await asyncio.gather(*tasks)\n# Total time: max(2s, 2s, 2s, 2s, 2s, 2s) = 2 seconds\n```\n\n**Speedup**: 6√ó faster!\n\n### How asyncio.gather() Works\n\n```python\nasync def task1():\n    await asyncio.sleep(2)\n    return \"Task 1 done\"\n\nasync def task2():\n    await asyncio.sleep(2)\n    return \"Task 2 done\"\n\n# Sequential (4 seconds total)\nresult1 = await task1()  # Wait 2s\nresult2 = await task2()  # Wait 2s more\n\n# Parallel (2 seconds total)\nresult1, result2 = await asyncio.gather(task1(), task2())  # Wait 2s total\n```\n\n**Key insight**: While waiting for API response, Python can start other tasks\n\n### Composable App Example: ReviewerPanel\n\n```python\n# From agents/reviewer_panel.py (simplified)\nasync def review_article(self, article: Article) -> str:\n    # Create 6 review tasks (all different personas)\n    tasks = [\n        self.grammar_reviewer.review(article),\n        self.math_reviewer.review(article),\n        self.conservative_parent.review(article),\n        self.liberal_parent.review(article),\n        self.school_admin.review(article),\n        self.district_rep.review(article),\n    ]\n    \n    # Execute all 6 in parallel\n    reviews = await asyncio.gather(*tasks)\n    \n    # Consolidate feedback\n    consolidated = await self.secretary.consolidate(reviews)\n    return consolidated\n```\n\n**Performance**: 2-3 seconds (parallel) vs. 12-18 seconds (sequential)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Comparison Across Providers\n",
    "\n",
    "**Task 5.1.6**: Section - Cost comparison across providers (table with pricing)\n",
    "\n",
    "### Pricing Breakdown (January 2025)\n",
    "\n",
    "**Note**: Prices change frequently. Always check provider websites for latest rates.\n",
    "\n",
    "| Provider | Model | Input (per 1M tokens) | Output (per 1M tokens) | Context Window | Speed |\n",
    "|----------|-------|----------------------|------------------------|----------------|-------|\n",
    "| **Gemini** | `gemini-2.0-flash` | $0.075 | $0.30 | 1M tokens | Very Fast |\n",
    "| | `gemini-2.5-flash-lite` | $0.04 | $0.15 | 128K tokens | Fastest |\n",
    "| **OpenAI** | `gpt-4o` | $2.50 | $10.00 | 128K tokens | Fast |\n",
    "| | `gpt-4o-mini` | $0.15 | $0.60 | 128K tokens | Fast |\n",
    "| **Anthropic** | `claude-3-5-sonnet` | $3.00 | $15.00 | 200K tokens | Medium |\n",
    "| | `claude-3-5-haiku` | $0.80 | $4.00 | 200K tokens | Fast |\n",
    "| **Ollama** | `llama3:8b` (local) | $0.00 | $0.00 | 8K tokens | Slow (CPU) |\n",
    "\n",
    "### Real-World Cost Analysis: Composable App Workflow\n",
    "\n",
    "**Scenario**: Generate 1 article\n",
    "- TaskAssigner: 100 tokens in, 50 tokens out\n",
    "- Writer (initial draft): 500 tokens in, 1000 tokens out\n",
    "- ReviewerPanel (6 reviewers): 1500 tokens in √ó 6, 300 tokens out √ó 6\n",
    "- Writer (revision): 2500 tokens in, 1200 tokens out\n",
    "\n",
    "**Total tokens**: ~13K input, ~3.8K output\n",
    "\n",
    "#### Cost per Article by Provider\n",
    "\n",
    "| Provider | Model | Input Cost | Output Cost | **Total** |\n",
    "|----------|-------|-----------|-------------|----------|\n",
    "| **Gemini** | `gemini-2.0-flash` | $0.00098 | $0.00114 | **$0.0021** |\n",
    "| **OpenAI** | `gpt-4o-mini` | $0.00195 | $0.00228 | **$0.0042** |\n",
    "| **OpenAI** | `gpt-4o` | $0.0325 | $0.038 | **$0.0705** |\n",
    "| **Anthropic** | `claude-3-5-haiku` | $0.0104 | $0.0152 | **$0.0256** |\n",
    "| **Anthropic** | `claude-3-5-sonnet` | $0.039 | $0.057 | **$0.096** |\n",
    "\n",
    "### Cost Optimization Strategies\n",
    "\n",
    "#### 1. **Model Cascades** (Pattern 8)\n",
    "Try cheap model first, escalate if needed:\n",
    "```python\n",
    "# Attempt 1: Try SMALL_MODEL (fast, cheap)\n",
    "result = await small_agent.run(prompt)\n",
    "\n",
    "# Attempt 2: If quality insufficient, escalate to BEST_MODEL\n",
    "if quality_check(result) < 0.7:\n",
    "    result = await best_agent.run(prompt)\n",
    "```\n",
    "\n",
    "**Savings**: 70-90% when small model succeeds\n",
    "\n",
    "#### 2. **Smart Routing**\n",
    "Route tasks to appropriate model tier:\n",
    "```python\n",
    "# Simple tasks ‚Üí SMALL_MODEL\n",
    "if task.type == \"guardrail_check\":\n",
    "    agent = Agent(llms.SMALL_MODEL)\n",
    "\n",
    "# Complex tasks ‚Üí BEST_MODEL\n",
    "elif task.type == \"initial_draft\":\n",
    "    agent = Agent(llms.BEST_MODEL)\n",
    "```\n",
    "\n",
    "#### 3. **Prompt Caching** (Pattern 25)\n",
    "Reuse expensive context across calls:\n",
    "```python\n",
    "# Large context (e.g., book chapter) cached by provider\n",
    "# First call: Pay for full context\n",
    "# Subsequent calls: Only pay for new content\n",
    "\n",
    "# Supported by: Anthropic Claude (90% cache discount), Google Gemini (context caching API)\n",
    "```\n",
    "\n",
    "#### 4. **Batch Processing**\n",
    "Some providers offer batch API (slower, 50% cheaper):\n",
    "```python\n",
    "# OpenAI Batch API: 50% discount, 24hr turnaround\n",
    "# Good for: Evaluation datasets, overnight processing\n",
    "# Bad for: Real-time user-facing workflows\n",
    "```\n",
    "\n",
    "### When to Optimize for Cost vs. Quality\n",
    "\n",
    "**Optimize for cost**:\n",
    "- ‚úÖ Internal tools and automation\n",
    "- ‚úÖ High-volume simple tasks (guardrails, classification)\n",
    "- ‚úÖ Development and testing\n",
    "\n",
    "**Optimize for quality**:\n",
    "- ‚úÖ User-facing outputs\n",
    "- ‚úÖ High-stakes decisions (medical, legal, financial)\n",
    "- ‚úÖ Complex reasoning tasks\n",
    "\n",
    "**Balance both**:\n",
    "- ‚úÖ Educational content (composable app)\n",
    "- ‚úÖ Most production applications\n",
    "- ‚úÖ Use model cascades for best of both worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Next Steps\n\n### Continue Learning\n1. **[Evaluation Tutorial](evaluation_tutorial.ipynb)** - Measure quality and iterate\n2. **[Horizontal Services](../concepts/horizontal_services.md)** - Composable design patterns\n3. **[Multi-Agent Pattern](multi_agent_pattern.ipynb)** - ReviewerPanel deep-dive\n\n### Hands-On Practice\n1. **Provider comparison**: Test all 3 providers (Gemini, OpenAI, Anthropic) on same task\n2. **Cost optimization**: Implement model cascade (SMALL_MODEL ‚Üí BEST_MODEL)\n3. **Parallel execution**: Convert a sequential workflow to parallel with asyncio.gather()\n4. **Temperature tuning**: Find optimal temperature for your use case\n\n### Advanced Challenges\n1. **Implement failover logic**: Automatic provider switching on errors\n2. **Cost tracking**: Log API costs per provider, visualize with Streamlit\n3. **A/B testing framework**: Compare providers systematically with metrics\n4. **Dynamic routing**: Route queries to providers based on complexity\n\n### Production Considerations\n1. **Monitoring**: Track latency, error rates, costs per provider\n2. **Rate limiting**: Implement exponential backoff for API errors\n3. **Caching**: Use provider-specific caching (Anthropic prompt caching, Gemini context caching)\n4. **Security**: Store API keys in secrets manager, not .env files\n\n---\n\n**Congratulations!** You've learned advanced patterns for LLM optimization:\n- Provider abstraction with Pydantic AI\n- Cost optimization strategies (model cascades, smart routing)\n- Performance optimization (parallel execution, temperature tuning)\n- Resilience patterns (failover, retry logic)\n\n**Tutorial Version**: 1.0  \n**Last Updated**: 2025-11-05  \n**Estimated Time**: 40-45 minutes  \n**API Cost**: $0.05-0.50 (depending on providers tested)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Self-Assessment\n\n**Task 5.1.13**: Self-assessment questions with answers\n\n### Question 1: Concept Check\n**What is the main benefit of Pydantic AI's provider abstraction?**\n\n<details>\n<summary>Click to reveal answer</summary>\n\n**Answer**: **Portability and flexibility** - Change LLM providers by modifying a single string, without rewriting code.\n\n**Example**:\n```python\n# Switch from Gemini to OpenAI - same API\nagent = Agent('gemini-2.0-flash')        # Google\nagent = Agent('gpt-4o')                   # OpenAI  \nagent = Agent('claude-3-5-sonnet-20241022')  # Anthropic\n```\n\n**Benefits**:\n1. **No vendor lock-in**: Easy migration between providers\n2. **Resilience**: Implement fallback logic if primary provider fails\n3. **Cost optimization**: Route tasks to cheapest suitable model\n4. **A/B testing**: Compare providers in production\n\n**Real-world scenario**: If OpenAI raises prices, switch entire app to Gemini in minutes, not weeks.\n</details>\n\n---\n\n### Question 2: Implementation\n**Why does the composable app use `temperature=0.25` for educational content?**\n\n<details>\n<summary>Click to reveal answer</summary>\n\n**Answer**: **Consistency** - Low temperature ensures similar explanations for same topic.\n\n**Temperature effects**:\n- **0.0**: Deterministic, almost identical outputs (too robotic)\n- **0.2-0.3**: Consistent with natural variation (ideal for education)\n- **0.7-1.0**: Creative, diverse outputs (good for brainstorming, bad for facts)\n- **1.5+**: Chaotic, may hallucinate (avoid for production)\n\n**Why it matters for K-12**:\n- Students asking same question should get consistent answer\n- Factual accuracy more important than creativity\n- Parents/teachers expect predictable, reliable content\n\n**Exception**: Use higher temperature (0.7-0.9) for creative writing assignments, poetry, or open-ended exploration.\n</details>\n\n---\n\n### Question 3: Optimization\n**You have a workflow with 6 parallel LLM calls (ReviewerPanel). Each call takes 2 seconds. What's the total time for sequential vs. parallel execution?**\n\n<details>\n<summary>Click to reveal answer</summary>\n\n**Answer**:\n- **Sequential**: 6 √ó 2s = **12 seconds**\n- **Parallel** (with asyncio.gather): max(2s, 2s, 2s, 2s, 2s, 2s) = **2 seconds**\n- **Speedup**: 6√ó faster ‚ö°\n\n**Code**:\n```python\n# Sequential (slow)\nreviews = []\nfor reviewer in reviewers:\n    review = await reviewer.review(article)  # Wait for each\n    reviews.append(review)\n# Total: 12s\n\n# Parallel (fast)\ntasks = [reviewer.review(article) for reviewer in reviewers]\nreviews = await asyncio.gather(*tasks)  # All at once\n# Total: 2s\n```\n\n**Why it works**: While waiting for API response from Reviewer1, Python starts requests for Reviewer2-6 simultaneously.\n\n**Limitation**: Speedup limited by slowest task (if one reviewer takes 5s, total = 5s, not 2s).\n</details>\n\n---\n\n### Question 4: Cost Analysis\n**Scenario**: Generate 1000 articles using composable app workflow (13K input tokens, 3.8K output tokens per article). Compare costs for Gemini Flash vs. GPT-4o.\n\n<details>\n<summary>Click to reveal answer</summary>\n\n**Calculation**:\n\n**Gemini 2.0 Flash**:\n- Input: 1000 √ó 13K tokens √ó $0.075/1M = $0.975\n- Output: 1000 √ó 3.8K tokens √ó $0.30/1M = $1.14\n- **Total**: **$2.12**\n\n**GPT-4o**:\n- Input: 1000 √ó 13K tokens √ó $2.50/1M = $32.50\n- Output: 1000 √ó 3.8K tokens √ó $10.00/1M = $38.00\n- **Total**: **$70.50**\n\n**Analysis**:\n- GPT-4o is **33√ó more expensive** ($70.50 vs $2.12)\n- For educational content, Gemini provides good quality at fraction of cost\n- **Recommendation**: Use Gemini Flash for development/testing, evaluate GPT-4o only if quality insufficient\n\n**Optimization**: Use model cascades\n```python\n# Try Gemini first (cheap)\nresult = await gemini_agent.run(prompt)\n\n# Escalate to GPT-4 only if quality check fails\nif quality_score(result) < 0.8:\n    result = await gpt4_agent.run(prompt)\n```\n\n**Savings**: 70-90% when Gemini succeeds most of the time.\n</details>\n\n---\n\n### Question 5: Advanced\n**How would you implement automatic failover if the primary LLM provider is down?**\n\n<details>\n<summary>Click to reveal answer</summary>\n\n**Approach: Try-Except with Provider Fallback**\n\n```python\nasync def resilient_llm_call(prompt: str, max_retries: int = 2):\n    \\\"\\\"\\\"Call LLM with automatic failover to backup provider.\\\"\\\"\\\"\n    \n    # Provider priority list\n    providers = [\n        ('gemini-2.0-flash', 'primary'),\n        ('gpt-4o-mini', 'secondary'),\n        ('claude-3-5-haiku-20241022', 'tertiary')\n    ]\n    \n    last_error = None\n    \n    for model, tier in providers:\n        for attempt in range(max_retries):\n            try:\n                agent = Agent(model)\n                result = await agent.run(prompt)\n                \n                logger.info(f\\\"‚úÖ Success with {tier} provider: {model}\")\n                return result.data\n                \n            except Exception as e:\n                last_error = e\n                logger.warning(f\\\"‚ö†Ô∏è {tier} provider failed (attempt {attempt+1}): {e}\")\n                \n                # Exponential backoff\n                if attempt < max_retries - 1:\n                    await asyncio.sleep(2 ** attempt)\n    \n    # All providers failed\n    logger.error(f\\\"‚ùå All providers failed. Last error: {last_error}\")\n    raise Exception(f\\\"LLM call failed after trying all providers: {last_error}\\\")\n\n# Usage\nresult = await resilient_llm_call(\"Explain photosynthesis\")\n```\n\n**Key features**:\n1. **Priority list**: Try cheapest/fastest first\n2. **Retry logic**: 2 attempts per provider\n3. **Exponential backoff**: Wait 1s, then 2s between retries\n4. **Logging**: Track which provider succeeded for monitoring\n5. **Graceful degradation**: Falls back to slower/more expensive providers\n\n**Production enhancements**:\n- **Circuit breaker**: Temporarily skip failing providers\n- **Health checks**: Monitor provider uptime separately\n- **Cost tracking**: Log which provider was used for billing\n- **Quality monitoring**: Compare output quality across providers\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Common Pitfalls\n\n**Task 5.1.12**: Common Pitfalls section\n\n### ‚ùå Error: \"Model not found\" when switching providers\n\n**Cause**: Typo in model name or using provider-specific model with wrong API key\n\n**Example**:\n```python\n# Wrong: Gemini model name with OpenAI key\nos.environ['OPENAI_API_KEY'] = \"...\"\nagent = Agent('gemini-2.0-flash')  # Will fail!\n```\n\n**Solution**: Verify model name matches provider\n```python\n# Correct: Use OpenAI model with OpenAI key\nagent = Agent('gpt-4o')  # ‚úÖ\n```\n\n---\n\n### ‚ùå Error: \"asyncio.gather() got multiple values for argument 'return_exceptions'\"\n\n**Cause**: Incorrect unpacking of tasks\n\n**Example**:\n```python\n# Wrong\ntasks = [task1(), task2()]\nresults = await asyncio.gather(tasks)  # Missing *\n```\n\n**Solution**: Use * to unpack list\n```python\n# Correct\ntasks = [task1(), task2()]\nresults = await asyncio.gather(*tasks)  # ‚úÖ\n```\n\n---\n\n### ‚ö†Ô∏è Warning: High costs from accidental BEST_MODEL usage\n\n**Cause**: Using expensive model for simple tasks\n\n**Example**:\n```python\n# Expensive: GPT-4 for simple yes/no check\nagent = Agent('gpt-4o')  # $10/1M output tokens\nfor item in 10000_items:\n    result = await agent.run(f\"Is this spam? {item}\")\n# Cost: ~$100!\n```\n\n**Solution**: Use tiered model selection\n```python\n# Cheap: Use small model for classification\nagent = Agent(llms.SMALL_MODEL)  # $0.15/1M output tokens\n# Cost: ~$1.50 for same task\n```\n\n---\n\n### ‚ö†Ô∏è Warning: Temperature too high causes inconsistent outputs\n\n**Cause**: Temperature > 0.5 for factual content\n\n**Example**:\n```python\n# Bad: High temperature for educational content\nsettings = GeminiModelSettings(temperature=1.5)\n# Result: Same question gives wildly different answers\n```\n\n**Solution**: Use low temperature (0.2-0.3) for consistency\n```python\n# Good: Low temperature for consistent educational content\nsettings = GeminiModelSettings(temperature=0.25)\n```\n\n---\n\n### üí° Tip: Cache agent instances for better performance\n\n**Problem**: Creating new Agent() for every request is slow\n\n**Bad**:\n```python\nfor topic in topics:\n    agent = Agent(llms.BEST_MODEL)  # Recreated each time!\n    result = await agent.run(f\"Write about {topic}\")\n```\n\n**Better**:\n```python\n# Create once, reuse many times\nagent = Agent(llms.BEST_MODEL)\nfor topic in topics:\n    result = await agent.run(f\"Write about {topic}\")  # ‚úÖ Faster\n```\n\n---\n\n### üí° Tip: Use environment variables for API keys, not hardcoded strings\n\n**Bad**:\n```python\nos.environ['GEMINI_API_KEY'] = \"AIza...\"  # Hardcoded! Security risk\n```\n\n**Good**:\n```python\n# Store in keys.env file (git-ignored)\nload_dotenv('keys.env')\n# Keys loaded from file, not in code ‚úÖ\n```\n\n---\n\n### üí° Tip: Monitor API quotas and rate limits\n\n**Problem**: Hitting rate limits during high-volume processing\n\n**Solutions**:\n1. **Exponential backoff**: Retry with increasing delays\n2. **Rate limiting**: Add `asyncio.sleep()` between batches\n3. **Provider diversity**: Distribute load across multiple providers\n\n```python\n# Example: Rate limiting\nfor batch in chunks(items, batch_size=100):\n    results = await asyncio.gather(*[process(item) for item in batch])\n    await asyncio.sleep(1)  # Wait 1s between batches\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demo: Profile real LLM calls\n\nimport time\nfrom typing import Dict\n\nasync def profile_model(model_name: str, prompt: str) -> Dict[str, float]:\n    \"\"\"Profile a single model call.\"\"\"\n    agent = Agent(model_name)\n    \n    start_time = time.time()\n    result = await agent.run(prompt)\n    end_time = time.time()\n    \n    latency = end_time - start_time\n    return {\n        \"model\": model_name,\n        \"latency_seconds\": latency,\n        \"response_length\": len(result.data)\n    }\n\n# Test different models\nprompt = \"Explain photosynthesis in 1 sentence.\"\n\nprint(\"‚è±Ô∏è Profiling Model Performance:\\n\")\nprint(f\"Prompt: '{prompt}'\\n\")\nprint(\"=\" * 80)\n\n# Profile BEST_MODEL\nbest_profile = await profile_model(llms.BEST_MODEL, prompt)\nprint(f\"\\nüèÜ {best_profile['model']}:\")\nprint(f\"   Latency: {best_profile['latency_seconds']:.2f}s\")\nprint(f\"   Response length: {best_profile['response_length']} chars\")\n\n# Profile SMALL_MODEL\nsmall_profile = await profile_model(llms.SMALL_MODEL, prompt)\nprint(f\"\\n‚ö° {small_profile['model']}:\")\nprint(f\"   Latency: {small_profile['latency_seconds']:.2f}s\")\nprint(f\"   Response length: {small_profile['response_length']} chars\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Compare\nspeedup = best_profile['latency_seconds'] / small_profile['latency_seconds']\nprint(f\"\\nüìä Analysis:\")\nprint(f\"   SMALL_MODEL is {speedup:.1f}x faster\")\nprint(f\"   Trade-off: Speed vs. quality (both produce valid answers)\")\n\nprint(\"\\nüí° Recommendation:\")\nprint(\"   - Use SMALL_MODEL for time-sensitive tasks (guardrails, classification)\")\nprint(\"   - Use BEST_MODEL for quality-critical tasks (user-facing content)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Performance Profiling\n\n**Task 5.1.8**: Code section - Performance profiling with %%time magic\n\n### Measuring Latency\n\n**Why profile?**\n- Identify bottlenecks in multi-agent workflows\n- Compare provider performance\n- Validate parallel execution benefits\n\n**Tools**:\n1. **Time module**: `time.time()` for programmatic timing\n2. **Jupyter %%time**: Cell magic for quick profiling\n3. **Profilers**: cProfile, py-spy for deep analysis",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}