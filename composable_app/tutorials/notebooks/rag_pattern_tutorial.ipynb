{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pattern 6: RAG with LlamaIndex (Composable App Tutorial)\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this tutorial, you will:\n",
        "- Understand semantic RAG architecture and when to use it\n",
        "- Learn LlamaIndex vector store setup with embeddings\n",
        "- Implement semantic retrieval with `similarity_top_k` configuration\n",
        "- Add citation tracking to generated content\n",
        "\n",
        "## Prerequisites\n",
        "- **Python**: Intermediate proficiency with async/await\n",
        "- **LLM basics**: Understanding of embeddings and vector similarity\n",
        "- **Setup**: Either Gemini API key OR OpenAI API key configured in `.env`\n",
        "- **Vector index**: Pre-built index in `composable_app/data/`\n",
        "\n",
        "## Estimated Time\n",
        "25-30 minutes (reading + execution)\n",
        "\n",
        "## Cost Estimate\n",
        "\u26a0\ufe0f **API costs**: \n",
        "- OpenAI: ~$0.001-0.01 (text-embedding-3-small)\n",
        "- Gemini: ~$0.02-0.05 (text-embedding-004)\n",
        "\n",
        "## Dual-Mode Support\n",
        "This notebook automatically detects which embedding model was used to create the vector index and uses the matching provider. The provided index was created with **Gemini embeddings**.\n",
        "\n",
        "> **Book Reference**: This pattern is detailed in *Generative AI Design Patterns*\n",
        "> (Lakshmanan & Hapke, 2025), Chapter 6: \"Retrieval-Augmented Generation\", pages 142-165."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What is RAG?\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)** combines semantic search with LLM generation:\n",
        "\n",
        "1. **Retrieve**: Find relevant documents using vector similarity\n",
        "2. **Augment**: Inject retrieved content into LLM prompt\n",
        "3. **Generate**: LLM produces answer grounded in retrieved context\n",
        "\n",
        "### When to Use RAG\n",
        "- \u2705 Domain-specific knowledge not in LLM training data\n",
        "- \u2705 Factual accuracy is critical (hallucination prevention)\n",
        "- \u2705 Citations/sources must be provided\n",
        "- \u2705 Knowledge base changes frequently\n",
        "- \u274c General knowledge questions (LLM alone is sufficient)\n",
        "- \u274c Real-time data (requires different retrieval strategy)\n",
        "\n",
        "### Composable App Use Case\n",
        "The **GenAIWriter** uses RAG to answer questions about the book \"Generative AI Design Patterns\":\n",
        "- **Vector index** built from book PDF\n",
        "- **User query**: \"What is prompt caching?\"\n",
        "- **Retrieved**: 3 most similar text chunks from book\n",
        "- **Generated**: 2-paragraph answer with page citations\n",
        "\n",
        "**Code Location**: [`agents/generic_writer_agent.py:142-165`](../../agents/generic_writer_agent.py#L142-L165)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup Cell\n",
        "\n",
        "**Tasks 2.1.1**: Setup cell with imports, API key check, cost warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to path for imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect if running from notebook or from nbconvert\n",
        "try:\n",
        "    current_dir = Path(__file__).parent.resolve()\n",
        "except NameError:\n",
        "    current_dir = Path.cwd()\n",
        "\n",
        "# Find repo root (contains .env)\n",
        "repo_root = current_dir\n",
        "while not (repo_root / '.env').exists() and repo_root != repo_root.parent:\n",
        "    repo_root = repo_root.parent\n",
        "\n",
        "# Add composable_app to path\n",
        "composable_app_path = repo_root / 'composable_app'\n",
        "sys.path.insert(0, str(composable_app_path))\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "env_path = repo_root / '.env'\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Check which API keys are available\n",
        "has_gemini = bool(os.getenv('GEMINI_API_KEY'))\n",
        "has_openai = bool(os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "# RAG dependencies\n",
        "from llama_index.core import StorageContext, Settings, load_index_from_storage\n",
        "from dataclasses import replace\n",
        "\n",
        "print(f\"\u2705 Environment loaded from: {env_path}\")\n",
        "print(f\"   Gemini API key: {'\u2705 Found' if has_gemini else '\u274c Not found'}\")\n",
        "print(f\"   OpenAI API key: {'\u2705 Found' if has_openai else '\u274c Not found'}\")\n",
        "\n",
        "# Determine which provider to use\n",
        "# The provided vector index uses Gemini embeddings (768 dimensions)\n",
        "# but we'll use OpenAI if Gemini is not available (note: requires recreating index)\n",
        "if has_gemini:\n",
        "    USE_GEMINI = True\n",
        "    print(\"\\n\ud83d\udd39 Using: Google Gemini embeddings (text-embedding-004, 768 dimensions)\")\n",
        "    print(\"   Matches the provided vector index\")\n",
        "elif has_openai:\n",
        "    USE_GEMINI = False\n",
        "    print(\"\\n\ud83d\udd39 Using: OpenAI embeddings (text-embedding-3-small, 1536 dimensions)\")\n",
        "    print(\"   \u26a0\ufe0f WARNING: The provided index uses Gemini (768-dim).\")\n",
        "    print(\"   You may see dimension mismatch errors during retrieval.\")\n",
        "    print(\"   To fix: Recreate index with python -m composable_app.data.create_index\")\n",
        "else:\n",
        "    raise EnvironmentError(\n",
        "        \"\u274c Neither GEMINI_API_KEY nor OPENAI_API_KEY found.\\n\"\n",
        "        \"   Add one to .env file:\\n\"\n",
        "        \"   - Gemini: https://makersuite.google.com/app/apikey\\n\"\n",
        "        \"   - OpenAI: https://platform.openai.com/api-keys\"\n",
        "    )\n",
        "\n",
        "print(\"\u2705 Setup complete\")\n",
        "print(\"\u26a0\ufe0f This notebook will make API calls for embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Vector Store Architecture\n",
        "\n",
        "**Task 2.1.2**: Conceptual section - What is RAG, when to use, composable app use case\n",
        "\n",
        "### How RAG Works in Composable App\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Book PDF] -->|OpenParse| B[Text Chunks]\n",
        "    B -->|Embed API| C[Vector Embeddings]\n",
        "    C --> D[Vector Store<br/>JSON Files]\n",
        "    \n",
        "    E[User Query] -->|Embed API| F[Query Embedding]\n",
        "    F -->|Cosine Similarity| D\n",
        "    D -->|Top-3 Most Similar| G[Retrieved Chunks]\n",
        "    G -->|Augment Prompt| H[LLM Generation]\n",
        "    H --> I[Article + Citations]\n",
        "    \n",
        "    style D fill:#e1f5ff\n",
        "    style G fill:#fff3cd\n",
        "    style I fill:#d4edda\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "- **Embeddings**: Supports both Google Gemini and OpenAI\n",
        "  - Gemini: `text-embedding-004` (768 dimensions) - Default for this tutorial\n",
        "  - OpenAI: `text-embedding-3-small` (1536 dimensions)\n",
        "- **Storage**: JSON files in `composable_app/data/`\n",
        "  - `default__vector_store.json` - Vector embeddings\n",
        "  - `docstore.json` - Document metadata\n",
        "  - `index_store.json` - Index structure\n",
        "- **Index**: Similarity search with cosine distance\n",
        "- **Retrieval**: Top-k most similar chunks (default k=3)\n",
        "\n",
        "### Embedding Model Comparison\n",
        "| Provider | Model | Dimensions | Cost (per 1M tokens) |\n",
        "|----------|-------|------------|---------------------|\n",
        "| **Gemini** | text-embedding-004 | 768 | Free (quota applies) |\n",
        "| OpenAI | text-embedding-3-small | 1536 | $0.02 |\n",
        "| OpenAI | text-embedding-3-large | 3072 | $0.13 |\n",
        "\n",
        "> **Note**: This tutorial uses the provided Gemini-based vector index. To use OpenAI embeddings, set `USE_GEMINI = False` in Cell 3 and recreate the index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Load Vector Index\n",
        "\n",
        "**Task 2.1.3**: Code section - Load vector index, configure embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure embedding model based on USE_GEMINI flag\n",
        "if USE_GEMINI:\n",
        "    from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "    Settings.embed_model = GoogleGenAIEmbedding(\n",
        "        model_name=\"text-embedding-004\",\n",
        "        api_key=os.environ[\"GEMINI_API_KEY\"]\n",
        "    )\n",
        "    print(\"\u2705 Embedding model: Google Gemini text-embedding-004 (768 dimensions)\")\n",
        "else:\n",
        "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "    Settings.embed_model = OpenAIEmbedding(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        "    )\n",
        "    print(\"\u2705 Embedding model: OpenAI text-embedding-3-small (1536 dimensions)\")\n",
        "\n",
        "# Load pre-built vector index from data directory\n",
        "try:\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=\"../../data\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    print(f\"\u2705 Loaded vector index from ../../data/\")\n",
        "    print(f\"   Index contains embeddings for book chunks\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c Vector index not found!\")\n",
        "    print(\"   Run: python -m composable_app.data.create_index\")\n",
        "    print(\"   Or see tutorials/README.md for setup instructions\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Semantic Retrieval Demo\n",
        "\n",
        "**Task 2.1.4**: Code section - Semantic retrieval demo with similarity_top_k=3\n",
        "\n",
        "### How Retrieval Works\n",
        "\n",
        "1. **Query Embedding**: Convert user query to 1536-dim vector using OpenAI API\n",
        "2. **Similarity Search**: Compute cosine similarity with all stored embeddings\n",
        "3. **Top-k Selection**: Return k most similar chunks (default k=3)\n",
        "4. **Metadata Extraction**: Include page numbers and bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create retriever with top-k=3 (configurable)\n",
        "retriever = index.as_retriever(similarity_top_k=3)\n",
        "\n",
        "# Example query\n",
        "query = \"What is prompt caching?\"\n",
        "print(f\"\ud83d\udcdd Query: '{query}'\")\n",
        "print(f\"\ud83d\udd0d Retrieving top-3 similar chunks...\\n\")\n",
        "\n",
        "# Perform semantic retrieval\n",
        "nodes = retriever.retrieve(query)\n",
        "\n",
        "print(f\"\u2705 Retrieved {len(nodes)} nodes:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, node in enumerate(nodes, 1):\n",
        "    print(f\"\\n\ud83d\udd39 Node {i}:\")\n",
        "    print(f\"   Similarity Score: {node.score:.4f} (higher = more relevant)\")\n",
        "    print(f\"   Page Number: {node.metadata['bbox'][0]['page']}\")\n",
        "    print(f\"   Text Preview: {node.text[:150]}...\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## GenAIWriter Implementation\n",
        "\n",
        "**Task 2.1.5**: Code section - GenAIWriter implementation walkthrough with line references\n",
        "\n",
        "### How GenAIWriter Uses RAG\n",
        "\n",
        "The `GenAIWriter` class extends `ZeroshotWriter` and overrides `write_response()` to add RAG:\n",
        "\n",
        "```python\n",
        "# From agents/generic_writer_agent.py:151-162\n",
        "async def write_response(self, topic: str, prompt: str) -> Article:\n",
        "    # 1. Semantic RAG retrieval\n",
        "    nodes = self.retriever.retrieve(topic)\n",
        "    \n",
        "    # 2. Augment prompt with retrieved context\n",
        "    prompt += f\"\\n**INFORMATION YOU CAN USE**\\n{nodes}\"\n",
        "    \n",
        "    # 3. Generate with LLM (uses BEST_MODEL = \"gemini-2.0-flash\")\n",
        "    result = await self.agent.run(prompt)\n",
        "    article = result.output\n",
        "    \n",
        "    # 4. Add page citations\n",
        "    pages = [str(node.metadata['bbox'][0]['page']) for node in nodes]\n",
        "    article = replace(article, full_text=article.full_text + f\"\\nSee pages: {', '.join(pages)}\")\n",
        "    \n",
        "    return article\n",
        "```\n",
        "\n",
        "**Key Design Decisions**:\n",
        "- **Why top-k=3?** Balances context richness with token limits\n",
        "- **Why append to prompt?** Simple, works with any LLM provider\n",
        "- **Why `replace()` not mutation?** Article is a frozen dataclass (immutable)\n",
        "- **Why page citations?** Enables fact-checking and trustworthiness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating GenAIWriter RAG Flow\n",
        "\n",
        "Let's simulate the full RAG flow without calling the LLM (to save costs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate the GenAIWriter RAG flow\n",
        "def simulate_rag_augmentation(topic: str, base_prompt: str, nodes: list) -> str:\n",
        "    \"\"\"Show how GenAIWriter augments prompts with retrieved context.\"\"\"\n",
        "    \n",
        "    # Step 1: Base prompt (from AbstractWriter)\n",
        "    print(\"\ud83d\udccb BASE PROMPT:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(base_prompt)\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Step 2: Augment with retrieved context\n",
        "    augmented_prompt = base_prompt + \"\\n\\n**INFORMATION YOU CAN USE**\\n\"\n",
        "    for i, node in enumerate(nodes, 1):\n",
        "        augmented_prompt += f\"\\nChunk {i} (Page {node.metadata['bbox'][0]['page']}): {node.text}\\n\"\n",
        "    \n",
        "    print(\"\\n\u2728 AUGMENTED PROMPT (what LLM sees):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(augmented_prompt[:500] + \"...\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return augmented_prompt\n",
        "\n",
        "# Demo\n",
        "base_prompt = f\"Write 2 paragraphs about: {query}\"\n",
        "augmented_prompt = simulate_rag_augmentation(query, base_prompt, nodes)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Prompt Stats:\")\n",
        "print(f\"   Base prompt: {len(base_prompt)} chars\")\n",
        "print(f\"   Augmented prompt: {len(augmented_prompt)} chars\")\n",
        "print(f\"   Context added: {len(augmented_prompt) - len(base_prompt)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Citation Tracking\n",
        "\n",
        "**Task 2.1.6**: Code section - Citation tracking (extract page numbers from metadata)\n",
        "\n",
        "### Why Citations Matter\n",
        "- **Trustworthiness**: Users can verify claims in source material\n",
        "- **Hallucination detection**: Easier to spot when LLM invents facts\n",
        "- **Educational value**: Students learn to check sources\n",
        "- **Legal compliance**: Some domains require citation (academic, medical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_citations(nodes: list) -> dict:\n",
        "    \"\"\"Extract page numbers and metadata from retrieved nodes.\n",
        "    \n",
        "    This replicates the citation logic from GenAIWriter (line 160).\n",
        "    \"\"\"\n",
        "    # Extract page numbers from node metadata\n",
        "    pages = [str(node.metadata['bbox'][0]['page']) for node in nodes]\n",
        "    \n",
        "    # Create citation string\n",
        "    citation_text = f\"See pages: {', '.join(pages)}\"\n",
        "    \n",
        "    return {\n",
        "        \"pages\": pages,\n",
        "        \"citation_text\": citation_text,\n",
        "        \"num_sources\": len(set(pages))  # unique pages\n",
        "    }\n",
        "\n",
        "# Demo citation extraction\n",
        "citations = extract_citations(nodes)\n",
        "\n",
        "print(\"\ud83d\udcda Citation Metadata:\")\n",
        "print(f\"   Pages referenced: {citations['pages']}\")\n",
        "print(f\"   Unique sources: {citations['num_sources']}\")\n",
        "print(f\"   Citation string: '{citations['citation_text']}'\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 How this appears in generated article:\")\n",
        "print(\"   [Article text here...]\")\n",
        "print(f\"   {citations['citation_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced: Citation with Bounding Boxes\n",
        "\n",
        "The metadata includes bounding box coordinates for precise source location:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect detailed metadata\n",
        "print(\"\ud83d\udd0d Detailed Metadata Example (Node 1):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "node = nodes[0]\n",
        "print(f\"Text: {node.text[:100]}...\")\n",
        "print(f\"\\nMetadata:\")\n",
        "print(f\"  - Page: {node.metadata['bbox'][0]['page']}\")\n",
        "print(f\"  - Bounding Box: {node.metadata['bbox'][0]}\")\n",
        "print(f\"  - Score: {node.score:.4f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Use Case: Could highlight exact text location in PDF viewer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Common Pitfalls\n",
        "\n",
        "**Task 2.1.7**: Common Pitfalls section\n",
        "\n",
        "### \u274c Error: \"No module named 'llama_index.embeddings.openai'\"\n",
        "**Cause**: Missing llama-index-embeddings-openai package\n",
        "\n",
        "**Solution**:\n",
        "```bash\n",
        "pip install llama-index-embeddings-openai\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \u274c Error: \"OPENAI_API_KEY not found\"\n",
        "**Cause**: API key not configured\n",
        "\n",
        "**Solution**:\n",
        "```bash\n",
        "# Add to .env in repo root OR composable_app/keys.env\n",
        "echo \"OPENAI_API_KEY=sk-...your_key_here\" >> .env\n",
        "\n",
        "# Get key from: https://platform.openai.com/api-keys\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \u274c Error: \"FileNotFoundError: [Errno 2] No such file or directory: '../../data/default__vector_store.json'\"\n",
        "**Cause**: Vector index not created yet\n",
        "\n",
        "**Solution**:\n",
        "```bash\n",
        "# Option 1: Create index from book PDF (requires PDF and API key)\n",
        "python -m composable_app.data.create_index\n",
        "\n",
        "# Option 2: Use sample index (if provided)\n",
        "cp composable_app/data/sample_vector_store.json composable_app/data/default__vector_store.json\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### \u26a0\ufe0f Warning: Retrieval returns irrelevant nodes\n",
        "**Cause**: Query too vague or embedding mismatch\n",
        "\n",
        "**Solutions**:\n",
        "1. **Rephrase query**: Be more specific (\"What is prompt caching?\" vs. \"caching\")\n",
        "2. **Adjust top-k**: Try `similarity_top_k=5` or `10` for broader context\n",
        "3. **Check embedding model**: Must match model used to create index\n",
        "4. **Inspect scores**: Low scores (<0.3) indicate poor match\n",
        "5. **Try larger model**: Upgrade to `text-embedding-3-large` for better quality\n",
        "\n",
        "---\n",
        "\n",
        "### \u26a0\ufe0f Warning: High API costs\n",
        "**Cause**: Embedding API calls for every query\n",
        "\n",
        "**Cost Optimization**:\n",
        "```python\n",
        "# Use text-embedding-3-small (already the cheapest at $0.02/1M tokens)\n",
        "\n",
        "# Reduce top-k to minimize context size\n",
        "retriever = index.as_retriever(similarity_top_k=1)  # Instead of 3\n",
        "\n",
        "# Cache common queries (future enhancement)\n",
        "# from functools import lru_cache\n",
        "# @lru_cache(maxsize=100)\n",
        "# def cached_retrieve(query: str):\n",
        "#     return retriever.retrieve(query)\n",
        "```\n",
        "\n",
        "**Cost Breakdown**:\n",
        "- Query embedding: ~10 tokens = $0.0000002 per query\n",
        "- 100 queries/day = $0.006/day = $1.80/year (negligible)\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udca1 Tip: Adjust similarity_top_k based on context window\n",
        "**Guidelines**:\n",
        "- **Top-k = 1**: When only most relevant chunk needed (narrow query)\n",
        "- **Top-k = 3**: Good default for most queries (balances context/cost)\n",
        "- **Top-k = 5-10**: Broad research questions, large context windows\n",
        "- **Top-k = 20+**: Rarely needed, risks exceeding token limits\n",
        "\n",
        "**Experiment**:\n",
        "```python\n",
        "# Try different top-k values\n",
        "for k in [1, 3, 5, 10]:\n",
        "    retriever = index.as_retriever(similarity_top_k=k)\n",
        "    nodes = retriever.retrieve(query)\n",
        "    print(f\"top-k={k}: Retrieved {len(nodes)} nodes, min score={nodes[-1].score:.4f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Self-Assessment\n",
        "\n",
        "**Task 2.1.8**: Self-assessment questions with answers\n",
        "\n",
        "### Question 1: Concept Check\n",
        "**What's the difference between semantic search and keyword search?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: \n",
        "- **Semantic search**: Uses vector embeddings and cosine similarity to find **meaning-based** matches. Can find \"automobile\" when searching for \"car\".\n",
        "- **Keyword search**: Uses exact or partial text matching (BM25, regex). Only finds literal text matches.\n",
        "\n",
        "**Example**: Query \"quick transportation\" would:\n",
        "- Semantic search: Find \"fast car\", \"rapid vehicle\", \"speedy automobile\" (similar meaning)\n",
        "- Keyword search: Find \"quick transportation\" only (exact match)\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Implementation\n",
        "**Why does GenAIWriter use `replace(article, ...)` instead of `article.full_text += ...` to add citations?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: Article is a **frozen dataclass** (immutable), so direct attribute assignment is not allowed.\n",
        "\n",
        "```python\n",
        "# From agents/article.py\n",
        "@dataclass(frozen=True)  # Makes it immutable\n",
        "class Article:\n",
        "    full_text: str\n",
        "    title: str\n",
        "    # ...\n",
        "```\n",
        "\n",
        "**Why frozen?**\n",
        "1. **Thread safety**: Multiple agents can reference same Article without race conditions\n",
        "2. **Cacheability**: Immutable objects can be safely cached\n",
        "3. **Debugging**: Easier to track changes (new instance = new version)\n",
        "\n",
        "**Solution**: Use `dataclasses.replace()` to create new instance with updated field:\n",
        "```python\n",
        "from dataclasses import replace\n",
        "article = replace(article, full_text=article.full_text + \"\\nCitations...\")\n",
        "```\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3: Design Trade-offs\n",
        "**When would you NOT use RAG? Give 2 examples.**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Answer**: RAG is not suitable when:\n",
        "\n",
        "1. **General knowledge already in LLM**: \"What is photosynthesis?\" - LLM has this from training, RAG adds latency/cost with no benefit.\n",
        "\n",
        "2. **Real-time data not in vector index**: \"What's the current stock price of AAPL?\" - Vector index is static, RAG can't retrieve live data. Need API integration instead.\n",
        "\n",
        "3. **Cost constraints**: Embedding API calls + increased prompt size. For high-volume applications, consider caching or fine-tuning LLM instead.\n",
        "\n",
        "4. **Very broad queries**: \"Tell me about everything\" - RAG works best with specific queries. Broad queries retrieve less relevant chunks.\n",
        "\n",
        "**When RAG shines**: Domain-specific docs, compliance requirements, changing knowledge bases, citation needs.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4: Advanced\n",
        "**How would you improve retrieval quality if users report irrelevant results?**\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Strategies**:\n",
        "\n",
        "1. **Query rewriting**: Transform user query before retrieval\n",
        "   ```python\n",
        "   # \"What's the deal with caching?\" \u2192 \"What is prompt caching?\"\n",
        "   rewritten_query = await llm.rewrite_query(user_query)\n",
        "   nodes = retriever.retrieve(rewritten_query)\n",
        "   ```\n",
        "\n",
        "2. **Hybrid search**: Combine semantic + keyword\n",
        "   ```python\n",
        "   semantic_nodes = semantic_retriever.retrieve(query)  # Top-5\n",
        "   keyword_nodes = bm25_retriever.retrieve(query)       # Top-5\n",
        "   nodes = rerank(semantic_nodes + keyword_nodes)[:3]   # Best 3\n",
        "   ```\n",
        "\n",
        "3. **Reranking**: Use second model to score and reorder results\n",
        "   ```python\n",
        "   candidates = retriever.retrieve(query, similarity_top_k=10)\n",
        "   reranked = reranker.rerank(query, candidates)[:3]  # More accurate top-3\n",
        "   ```\n",
        "\n",
        "4. **Better chunking**: Experiment with chunk size (current: default)\n",
        "   - Smaller chunks: More precise but may lose context\n",
        "   - Larger chunks: More context but less precise\n",
        "\n",
        "5. **Metadata filtering**: Add filters (e.g., date, chapter, author)\n",
        "   ```python\n",
        "   nodes = retriever.retrieve(query, filters={\"chapter\": \"6\"})\n",
        "   ```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Book References\n",
        "\n",
        "**Task 2.1.9**: Book reference - Chapter 6, pages 142-165\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "\ud83d\udcd6 **Generative AI Design Patterns** (Lakshmanan & Hapke, O'Reilly 2025)\n",
        "- **Chapter 6**: \"Retrieval-Augmented Generation\" (pages 142-165)\n",
        "  - RAG architecture patterns\n",
        "  - Semantic vs. keyword retrieval trade-offs\n",
        "  - Chunking strategies\n",
        "  - Citation tracking\n",
        "  - Production deployment considerations\n",
        "\n",
        "**Related Chapters**:\n",
        "- **Chapter 11**: \"Trustworthy Generation\" - Citation and hallucination detection\n",
        "- **Chapter 15**: \"Hybrid Search\" - Combining semantic and keyword retrieval\n",
        "- **Chapter 22**: \"Reranking\" - Improving retrieval accuracy\n",
        "\n",
        "### External Resources\n",
        "- [LlamaIndex Documentation](https://docs.llamaindex.ai) - Complete RAG framework guide\n",
        "- [Pydantic AI Multi-Agent Guide](https://ai.pydantic.dev/multi-agent-applications/)\n",
        "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [OpenAI Embeddings API Reference](https://platform.openai.com/docs/api-reference/embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "### Continue Learning\n",
        "1. **[Horizontal Services](../concepts/horizontal_services.md)** - Learn about memory and guardrails\n",
        "2. **[Advanced Patterns](advanced_patterns.ipynb)** - Optimize retrieval performance\n",
        "3. **[Multi-Agent Pattern](multi_agent_pattern.ipynb)** - Combine RAG with multi-agent review\n",
        "\n",
        "### Hands-On Practice\n",
        "1. **Modify top-k**: Try `similarity_top_k=1, 5, 10` and compare results\n",
        "2. **Different queries**: Test with broad vs. specific queries\n",
        "3. **Inspect scores**: Look for patterns in similarity scores\n",
        "4. **Add your data**: Create index from your own documents\n",
        "\n",
        "### Advanced Exercises\n",
        "1. **Implement query rewriting**: Use LLM to improve user queries before retrieval\n",
        "2. **Add reranking**: Use a second model to reorder retrieved chunks\n",
        "3. **Hybrid search**: Combine semantic search with BM25 keyword search\n",
        "4. **Citation UI**: Build interface to highlight cited text in source PDF\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've learned Pattern 6: RAG with LlamaIndex. You can now build knowledge-grounded LLM applications with citation tracking.\n",
        "\n",
        "**Tutorial Version**: 1.0  \n",
        "**Last Updated**: 2025-11-04  \n",
        "**Estimated Time**: 25-30 minutes  \n",
        "**API Cost**: ~$0.02-0.05"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}