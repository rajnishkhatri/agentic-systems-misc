{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using evals.log for LLM Improvement (Composable App Tutorial)\n",
    "\n",
    "## Learning Objectives\n",
    "By completing this tutorial, you will:\n",
    "- Understand why systematic evaluation is critical for LLM applications\n",
    "- Learn to parse and analyze `evals.log` for quality insights\n",
    "- Implement quantitative metrics (ideal count, diversity scoring)\n",
    "- Use embedding variance to measure semantic diversity\n",
    "- Apply statistical analysis to identify improvement opportunities\n",
    "\n",
    "## Prerequisites\n",
    "- **Python**: Intermediate proficiency with JSON, dataclasses, NumPy\n",
    "- **Statistics**: Basic understanding of mean, variance, descriptive statistics\n",
    "- **Setup**: Composable app installed with sample `evals.log` (generated from running the app)\n",
    "\n",
    "## Estimated Time\n",
    "30-35 minutes (reading + execution)\n",
    "\n",
    "## Cost Estimate\n",
    "‚úÖ **Free** - This tutorial reads local logs, no API calls required\n",
    "\n",
    "> **Book Reference**: This pattern is detailed in *Generative AI Design Patterns*\n",
    "> (Lakshmanan & Hapke, 2025), Chapter 16: \"LLM Evaluation\" and Chapter 30: \"Observability\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Evaluation Matters\n",
    "\n",
    "**Task 4.2.2**: Conceptual section - Why evaluation matters, LLM-as-judge vs. metrics\n",
    "\n",
    "### The Evaluation Challenge\n",
    "\n",
    "LLM outputs are **non-deterministic** and **difficult to assess**:\n",
    "- Traditional unit tests (`assert output == expected`) don't work for generative AI\n",
    "- Human evaluation is expensive and doesn't scale\n",
    "- Quality metrics vary by use case (accuracy, helpfulness, tone, citations)\n",
    "\n",
    "### Two Evaluation Approaches\n",
    "\n",
    "#### 1. **Quantitative Metrics** (This Tutorial)\n",
    "- **Measurable**: Ideal count, length, diversity, response time\n",
    "- **Fast**: Run on every output, no LLM calls needed\n",
    "- **Objective**: Same input always produces same score\n",
    "- **Limitations**: May miss semantic quality issues\n",
    "\n",
    "**Example**: Keyword quality evaluation\n",
    "```python\n",
    "# Ideal: 5 keywords per article\n",
    "score = 1.0 - abs(len(keywords) - 5) / 5.0\n",
    "# 5 keywords ‚Üí score = 1.0 (perfect)\n",
    "# 3 keywords ‚Üí score = 0.6 (penalty for too few)\n",
    "```\n",
    "\n",
    "#### 2. **LLM-as-Judge** (Covered in llm_as_judge_tutorial.ipynb)\n",
    "- **Semantic**: Evaluates meaning, correctness, helpfulness\n",
    "- **Flexible**: Can assess subjective qualities (tone, creativity)\n",
    "- **Costly**: Requires LLM API calls for each evaluation\n",
    "- **Bias risk**: Judge LLM may have preferences or blind spots\n",
    "\n",
    "**Example**: Correctness evaluation\n",
    "```python\n",
    "judge_prompt = f\"\"\"Rate the accuracy of this answer (0-10):\n",
    "Question: {question}\n",
    "Answer: {llm_answer}\n",
    "Ground Truth: {reference_answer}\n",
    "\"\"\"\n",
    "score = await judge_llm.evaluate(judge_prompt)\n",
    "```\n",
    "\n",
    "### Composable App Use Case\n",
    "\n",
    "The composable app evaluates **keyword quality** for generated articles:\n",
    "- **Metric 1**: Ideal count (5 keywords is optimal for search indexing)\n",
    "- **Metric 2**: Diversity (keywords should cover different aspects)\n",
    "- **Source**: `evals.log` records every AI-generated draft\n",
    "- **Goal**: Identify systematic issues (e.g., GenAIWriter always generates 8 keywords)\n",
    "\n",
    "**Code Location**: [`evals/evaluate_keywords.py`](../../evals/evaluate_keywords.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup Cell\n",
    "\n",
    "**Task 4.2.1**: Setup cell with imports, no API calls needed (reads logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')  # Navigate to composable_app/ root\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Data science imports\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Project imports\n",
    "from agents.article import Article  # Needed for eval() to reconstruct Article objects\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(\"‚úÖ No API calls needed - this tutorial reads local logs\")\n",
    "print(\"\\n‚ö†Ô∏è Note: On first run, SentenceTransformer will download ~90MB model from HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reading evals.log\n",
    "\n",
    "**Task 4.2.3**: Code section - Reading logs/evals.log (JSON lines format)\n",
    "\n",
    "### evals.log Format\n",
    "\n",
    "The `utils/save_for_eval.py` module records every AI response:\n",
    "\n",
    "```python\n",
    "# From save_for_eval.py:5-10\n",
    "async def record_ai_response(target, ai_input, ai_response):\n",
    "    logger.info(f\"AI Response\", extra={\n",
    "        \"target\": target,        # What was evaluated (e.g., \"initial_draft\")\n",
    "        \"ai_input\": ai_input,    # Input to LLM (e.g., topic)\n",
    "        \"ai_response\": ai_response,  # Output from LLM (Article object)\n",
    "    })\n",
    "```\n",
    "\n",
    "**Log Format**: JSON Lines (one JSON object per line)\n",
    "```json\n",
    "{\"target\": \"initial_draft\", \"ai_input\": \"Photosynthesis\", \"ai_response\": \"Article(title='...')\"}\n",
    "{\"target\": \"initial_draft\", \"ai_input\": \"Cell Division\", \"ai_response\": \"Article(...)\"}\n",
    "```\n",
    "\n",
    "### Why JSON Lines?\n",
    "- **Append-friendly**: Each run adds lines, no need to rewrite file\n",
    "- **Streaming**: Can process large logs without loading everything into memory\n",
    "- **Resilient**: Corrupted line doesn't break entire file\n",
    "- **Standard**: Widely supported by log analysis tools (jq, Logfire, Splunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(target: str = \"initial_draft\", log_path: str = \"../../evals.log\") -> List[Article]:\n",
    "    \"\"\"Parse evals.log and extract Article objects for given target.\n",
    "    \n",
    "    Args:\n",
    "        target: Filter for specific evaluation target (e.g., \"initial_draft\", \"revised_draft\")\n",
    "        log_path: Path to evals.log file\n",
    "    \n",
    "    Returns:\n",
    "        List of Article objects recorded in log\n",
    "    \n",
    "    Note:\n",
    "        Uses eval() to reconstruct Article objects from string representation.\n",
    "        This works because Article is a dataclass with a proper __repr__.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Check if log file exists\n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"‚ö†Ô∏è Log file not found: {log_path}\")\n",
    "        print(\"   Run the composable app to generate evals.log\")\n",
    "        return records\n",
    "    \n",
    "    # Parse JSON lines\n",
    "    with open(log_path) as ifp:\n",
    "        for line_num, line in enumerate(ifp.readlines(), 1):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                \n",
    "                # Filter by target\n",
    "                if obj.get('target') == target:\n",
    "                    # Reconstruct Article object from string representation\n",
    "                    article = eval(obj['ai_response'])\n",
    "                    records.append(article)\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping malformed JSON at line {line_num}: {e}\")\n",
    "            except (KeyError, NameError, SyntaxError) as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid record at line {line_num}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(records)} records for target='{target}'\")\n",
    "    return records\n",
    "\n",
    "# Demo: Load initial drafts from evals.log\n",
    "articles = get_records(target=\"initial_draft\")\n",
    "\n",
    "if articles:\n",
    "    print(f\"\\nüìã Sample Article:\")\n",
    "    print(f\"   Title: {articles[0].title}\")\n",
    "    print(f\"   Keywords: {articles[0].index_keywords}\")\n",
    "    print(f\"   Text length: {len(articles[0].full_text)} chars\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No articles found. Generate sample data by running the app.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Walkthrough: evaluate_keywords.py\n",
    "\n",
    "**Task 4.2.4**: Code section - Walkthrough of evaluate_keywords.py (line-by-line)\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "The `evaluate_keywords.py` script implements a **composite metric** for keyword quality:\n",
    "\n",
    "```python\n",
    "# From evals/evaluate_keywords.py:29-39\n",
    "def evaluate(keywords: List[str], embedding_model) -> float:\n",
    "    # Metric 1: Ideal count penalty\n",
    "    # If we have 5 keywords, it is ideal. Anything more or less is penalized\n",
    "    score = 1.0 - (np.abs(len(keywords) - 5) / 5.0)\n",
    "    score = np.clip(score, 0.0, 1.0)  # Clamp to [0, 1]\n",
    "    \n",
    "    # Metric 2: Diversity bonus\n",
    "    # The more diverse the set of keywords, the better\n",
    "    # We calculate diversity as variance of the embeddings\n",
    "    embeds = [np.mean(embedding_model.encode(keyword)) for keyword in keywords]\n",
    "    score += np.var(embeds)\n",
    "    \n",
    "    return score\n",
    "```\n",
    "\n",
    "### Metric 1: Ideal Count Penalty\n",
    "\n",
    "**Goal**: Prefer exactly 5 keywords (common best practice for search indexing)\n",
    "\n",
    "**Formula**: `penalty = abs(len(keywords) - 5) / 5.0`\n",
    "\n",
    "**Examples**:\n",
    "- `5 keywords`: penalty = 0.0 ‚Üí score = 1.0 ‚úÖ (perfect)\n",
    "- `3 keywords`: penalty = 0.4 ‚Üí score = 0.6 (too few)\n",
    "- `8 keywords`: penalty = 0.6 ‚Üí score = 0.4 (too many)\n",
    "- `0 keywords`: penalty = 1.0 ‚Üí score = 0.0 ‚ùå (worst)\n",
    "\n",
    "**Why penalize?**\n",
    "- **Too few**: Under-indexed, poor search discoverability\n",
    "- **Too many**: Keyword stuffing, dilutes relevance signals\n",
    "\n",
    "### Metric 2: Diversity Bonus\n",
    "\n",
    "**Goal**: Keywords should cover different semantic aspects, not duplicates\n",
    "\n",
    "**Formula**: `diversity = variance(keyword_embeddings)`\n",
    "\n",
    "**Why variance?**\n",
    "- **High variance**: Keywords are semantically distant (good diversity)\n",
    "  - Example: `[\"photosynthesis\", \"chloroplast\", \"light\", \"oxygen\", \"glucose\"]`\n",
    "  - Covers: process, organelle, input, outputs\n",
    "- **Low variance**: Keywords are semantically similar (redundant)\n",
    "  - Example: `[\"photosynthesis\", \"photosynthetic\", \"photosynthesize\", \"photosystem\"]`\n",
    "  - All variations of same concept\n",
    "\n",
    "**Implementation Detail**: Uses `np.mean(embedding)` to collapse 768-dim vector to scalar\n",
    "- This is a **simplification** for demonstration\n",
    "- Production version should use full embedding variance or pairwise cosine distance\n",
    "\n",
    "### Composite Score\n",
    "\n",
    "**Final score** = `ideal_count_score + diversity_bonus`\n",
    "\n",
    "**Range**: ~0.0 to ~2.0 (not normalized)\n",
    "- Higher is better\n",
    "- Allows comparing relative quality across articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Metric 1: Keyword Count Evaluation\n",
    "\n",
    "**Task 4.2.5**: Code section - Keyword quality evaluation (ideal count, diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_keyword_count(keywords: List[str], ideal_count: int = 5) -> float:\n",
    "    \"\"\"Evaluate keyword list based on ideal count.\n",
    "    \n",
    "    Args:\n",
    "        keywords: List of keywords from Article.index_keywords\n",
    "        ideal_count: Target number of keywords (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Score from 0.0 (worst) to 1.0 (perfect)\n",
    "    \"\"\"\n",
    "    # Calculate penalty for deviation from ideal count\n",
    "    penalty = np.abs(len(keywords) - ideal_count) / ideal_count\n",
    "    score = 1.0 - penalty\n",
    "    \n",
    "    # Clamp to valid range [0.0, 1.0]\n",
    "    score = np.clip(score, 0.0, 1.0)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Demo: Evaluate keyword counts\n",
    "if articles:\n",
    "    print(\"üìä Keyword Count Analysis:\\n\")\n",
    "    print(f\"{'Title':<30} {'Count':<8} {'Score':<8} {'Status'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    count_scores = []\n",
    "    for article in articles[:10]:  # Show first 10\n",
    "        count = len(article.index_keywords)\n",
    "        score = evaluate_keyword_count(article.index_keywords)\n",
    "        count_scores.append(score)\n",
    "        \n",
    "        # Status indicator\n",
    "        if score >= 0.8:\n",
    "            status = \"‚úÖ Good\"\n",
    "        elif score >= 0.5:\n",
    "            status = \"‚ö†Ô∏è Fair\"\n",
    "        else:\n",
    "            status = \"‚ùå Poor\"\n",
    "        \n",
    "        # Truncate title for display\n",
    "        title = article.title[:28] + \"..\" if len(article.title) > 30 else article.title\n",
    "        print(f\"{title:<30} {count:<8} {score:<8.2f} {status}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìà Summary Statistics:\")\n",
    "    print(f\"   Mean score: {np.mean(count_scores):.2f}\")\n",
    "    print(f\"   Std dev: {np.std(count_scores):.2f}\")\n",
    "    print(f\"   Min: {np.min(count_scores):.2f}, Max: {np.max(count_scores):.2f}\")\n",
    "    \n",
    "    # Insight\n",
    "    avg_count = np.mean([len(a.index_keywords) for a in articles])\n",
    "    print(f\"\\nüí° Insight: Average keyword count is {avg_count:.1f}\")\n",
    "    if avg_count > 6:\n",
    "        print(\"   ‚ö†Ô∏è Consider tuning prompts to reduce keyword count\")\n",
    "    elif avg_count < 4:\n",
    "        print(\"   ‚ö†Ô∏è Consider tuning prompts to increase keyword count\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Keyword count is well-calibrated\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No articles to evaluate. Run the app to generate evals.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Metric 2: Keyword Diversity with Embeddings\n",
    "\n",
    "**Task 4.2.6**: Code section - Embedding variance with SentenceTransformer\n",
    "\n",
    "### Why Use Embeddings for Diversity?\n",
    "\n",
    "**Problem**: How to measure if keywords are \"different\"?\n",
    "- **Lexical distance** (edit distance, n-grams): Misses semantic similarity\n",
    "  - \"car\" and \"automobile\" are lexically different but semantically identical\n",
    "- **Embedding distance**: Captures semantic meaning\n",
    "  - \"photosynthesis\" and \"cellular respiration\" have moderate distance (related processes)\n",
    "  - \"photosynthesis\" and \"economics\" have large distance (unrelated)\n",
    "\n",
    "**Approach**: Calculate variance of keyword embeddings\n",
    "- **High variance**: Keywords are semantically diverse (good)\n",
    "- **Low variance**: Keywords are semantically similar (redundant)\n",
    "\n",
    "### SentenceTransformer Model\n",
    "\n",
    "**Model**: `all-MiniLM-L6-v2`\n",
    "- **Size**: ~90MB download (first run only)\n",
    "- **Dimensions**: 384\n",
    "- **Speed**: ~1000 sentences/second on CPU\n",
    "- **Quality**: Good for keyword similarity tasks\n",
    "\n",
    "**Alternative models**:\n",
    "- `all-mpnet-base-v2`: Higher quality, slower (768 dims)\n",
    "- `paraphrase-MiniLM-L3-v2`: Faster, lower quality (384 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model (downloads ~90MB on first run)\n",
    "print(\"üì• Loading SentenceTransformer model...\")\n",
    "print(\"   (First run: downloads ~90MB from HuggingFace)\")\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"‚úÖ Model loaded: all-MiniLM-L6-v2 (384 dimensions)\")\n",
    "\n",
    "def evaluate_keyword_diversity(keywords: List[str], embedding_model) -> float:\n",
    "    \"\"\"Evaluate keyword diversity using embedding variance.\n",
    "    \n",
    "    Args:\n",
    "        keywords: List of keywords from Article.index_keywords\n",
    "        embedding_model: SentenceTransformer model for encoding\n",
    "    \n",
    "    Returns:\n",
    "        Diversity score (higher = more diverse)\n",
    "        \n",
    "    Note:\n",
    "        This is a simplified metric. Production version should:\n",
    "        - Use full embedding variance (not just mean)\n",
    "        - Normalize by number of keywords\n",
    "        - Consider pairwise cosine distances\n",
    "    \"\"\"\n",
    "    if len(keywords) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Encode keywords to embeddings\n",
    "    embeddings = embedding_model.encode(keywords)  # Shape: (num_keywords, 384)\n",
    "    \n",
    "    # Simplified diversity: variance of mean embeddings\n",
    "    # (Original evaluate_keywords.py uses this for demonstration)\n",
    "    mean_embeddings = [np.mean(emb) for emb in embeddings]\n",
    "    diversity = np.var(mean_embeddings)\n",
    "    \n",
    "    return diversity\n",
    "\n",
    "def evaluate_keyword_diversity_full(keywords: List[str], embedding_model) -> Dict[str, float]:\n",
    "    \"\"\"Advanced diversity metrics using full embeddings.\n",
    "    \n",
    "    Returns multiple diversity measures for comparison.\n",
    "    \"\"\"\n",
    "    if len(keywords) <= 1:\n",
    "        return {\"mean_variance\": 0.0, \"pairwise_distance\": 0.0, \"full_variance\": 0.0}\n",
    "    \n",
    "    # Encode keywords\n",
    "    embeddings = embedding_model.encode(keywords)  # Shape: (num_keywords, 384)\n",
    "    \n",
    "    # Metric 1: Mean embedding variance (original)\n",
    "    mean_embeddings = [np.mean(emb) for emb in embeddings]\n",
    "    mean_variance = np.var(mean_embeddings)\n",
    "    \n",
    "    # Metric 2: Full embedding variance (better)\n",
    "    full_variance = np.mean(np.var(embeddings, axis=0))\n",
    "    \n",
    "    # Metric 3: Average pairwise cosine distance (best)\n",
    "    from sklearn.metrics.pairwise import cosine_distances\n",
    "    distances = cosine_distances(embeddings)\n",
    "    # Get upper triangle (avoid diagonal and duplicates)\n",
    "    triu_indices = np.triu_indices_from(distances, k=1)\n",
    "    pairwise_distance = np.mean(distances[triu_indices])\n",
    "    \n",
    "    return {\n",
    "        \"mean_variance\": mean_variance,\n",
    "        \"full_variance\": full_variance,\n",
    "        \"pairwise_distance\": pairwise_distance\n",
    "    }\n",
    "\n",
    "# Demo: Evaluate diversity for sample articles\n",
    "if articles:\n",
    "    print(\"\\nüìä Keyword Diversity Analysis:\\n\")\n",
    "    print(f\"{'Title':<30} {'Simple':<10} {'Full':<10} {'Pairwise':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for article in articles[:5]:  # Show first 5\n",
    "        metrics = evaluate_keyword_diversity_full(article.index_keywords, embedding_model)\n",
    "        \n",
    "        title = article.title[:28] + \"..\" if len(article.title) > 30 else article.title\n",
    "        print(f\"{title:<30} {metrics['mean_variance']:<10.4f} {metrics['full_variance']:<10.4f} {metrics['pairwise_distance']:<10.4f}\")\n",
    "    \n",
    "    print(\"\\nüí° Metric Comparison:\")\n",
    "    print(\"   - Simple: Original evaluate_keywords.py (variance of mean embeddings)\")\n",
    "    print(\"   - Full: Variance across all embedding dimensions (more accurate)\")\n",
    "    print(\"   - Pairwise: Average cosine distance between all keyword pairs (best)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No articles to evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical Analysis with scipy.stats\n",
    "\n",
    "**Task 4.2.7**: Code section - Statistical analysis with scipy.stats\n",
    "\n",
    "### Descriptive Statistics\n",
    "\n",
    "Use `scipy.stats.describe()` to get comprehensive summary:\n",
    "- **nobs**: Number of observations\n",
    "- **minmax**: Range of scores\n",
    "- **mean**: Average quality\n",
    "- **variance**: Spread of scores (consistency)\n",
    "- **skewness**: Distribution symmetry (negative = left-skewed)\n",
    "- **kurtosis**: Tail heaviness (outliers)\n",
    "\n",
    "### Interpretation Guide\n",
    "\n",
    "**Mean score**:\n",
    "- `> 1.5`: Excellent keyword quality ‚úÖ\n",
    "- `1.0 - 1.5`: Good quality, room for improvement\n",
    "- `< 1.0`: Poor quality, needs prompt tuning ‚ùå\n",
    "\n",
    "**Variance**:\n",
    "- **Low variance** (< 0.1): Consistent quality (good for production)\n",
    "- **High variance** (> 0.3): Inconsistent (indicates prompt sensitivity)\n",
    "\n",
    "**Skewness**:\n",
    "- **Negative skew**: Most outputs good, few poor outliers\n",
    "- **Positive skew**: Most outputs poor, few good outliers\n",
    "- **Near zero**: Symmetric distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_composite(keywords: List[str], embedding_model) -> float:\n",
    "    \"\"\"Composite evaluation combining count and diversity.\n",
    "    \n",
    "    This replicates the exact logic from evaluate_keywords.py:29-39\n",
    "    \"\"\"\n",
    "    # Metric 1: Ideal count penalty\n",
    "    score = 1.0 - (np.abs(len(keywords) - 5) / 5.0)\n",
    "    score = np.clip(score, 0.0, 1.0)\n",
    "    \n",
    "    # Metric 2: Diversity bonus\n",
    "    embeds = [np.mean(embedding_model.encode(keyword)) for keyword in keywords]\n",
    "    score += np.var(embeds)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Evaluate all articles\n",
    "if articles:\n",
    "    print(\"üî¨ Evaluating all articles...\\n\")\n",
    "    \n",
    "    scores = []\n",
    "    for article in articles:\n",
    "        score = evaluate_composite(article.index_keywords, embedding_model)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Statistical analysis\n",
    "    stats_result = stats.describe(scores)\n",
    "    \n",
    "    print(\"üìä Statistical Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Number of articles: {stats_result.nobs}\")\n",
    "    print(f\"Mean score: {stats_result.mean:.4f}\")\n",
    "    print(f\"Variance: {stats_result.variance:.4f}\")\n",
    "    print(f\"Std deviation: {np.sqrt(stats_result.variance):.4f}\")\n",
    "    print(f\"Min score: {stats_result.minmax[0]:.4f}\")\n",
    "    print(f\"Max score: {stats_result.minmax[1]:.4f}\")\n",
    "    print(f\"Skewness: {stats_result.skewness:.4f}\")\n",
    "    print(f\"Kurtosis: {stats_result.kurtosis:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nüí° Interpretation:\")\n",
    "    \n",
    "    # Mean score interpretation\n",
    "    if stats_result.mean > 1.5:\n",
    "        print(\"   ‚úÖ Excellent keyword quality (mean > 1.5)\")\n",
    "    elif stats_result.mean > 1.0:\n",
    "        print(\"   ‚ö†Ô∏è Good quality with room for improvement (1.0 < mean < 1.5)\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Poor keyword quality (mean < 1.0) - tune prompts!\")\n",
    "    \n",
    "    # Variance interpretation\n",
    "    if stats_result.variance < 0.1:\n",
    "        print(\"   ‚úÖ Low variance - consistent quality across articles\")\n",
    "    elif stats_result.variance < 0.3:\n",
    "        print(\"   ‚ö†Ô∏è Moderate variance - some inconsistency\")\n",
    "    else:\n",
    "        print(\"   ‚ùå High variance - inconsistent quality (prompt sensitivity)\")\n",
    "    \n",
    "    # Skewness interpretation\n",
    "    if abs(stats_result.skewness) < 0.5:\n",
    "        print(\"   ‚úÖ Symmetric distribution (balanced quality)\")\n",
    "    elif stats_result.skewness < -0.5:\n",
    "        print(\"   üìä Negative skew - mostly good with few poor outliers\")\n",
    "    else:\n",
    "        print(\"   üìä Positive skew - mostly poor with few good outliers\")\n",
    "    \n",
    "    # Identify outliers (scores > 2 std devs from mean)\n",
    "    mean = stats_result.mean\n",
    "    std = np.sqrt(stats_result.variance)\n",
    "    outliers = [(i, s, articles[i].title) for i, s in enumerate(scores) \n",
    "                if abs(s - mean) > 2 * std]\n",
    "    \n",
    "    if outliers:\n",
    "        print(f\"\\n‚ö†Ô∏è Outliers detected ({len(outliers)} articles > 2 std devs):\")\n",
    "        for idx, score, title in outliers[:5]:  # Show first 5\n",
    "            deviation = (score - mean) / std\n",
    "            print(f\"   - '{title[:40]}': {score:.2f} ({deviation:+.1f}œÉ)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No outliers detected (all scores within 2 std devs)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No articles to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Design a New Evaluation Metric\n",
    "\n",
    "**Task 4.2.8**: Exercise - Design a new evaluation metric (response length, citation count)\n",
    "\n",
    "### Challenge\n",
    "\n",
    "Design and implement a new evaluation metric for the composable app. Choose one:\n",
    "\n",
    "1. **Response Length Evaluation**\n",
    "   - Goal: Prefer articles between 300-500 words\n",
    "   - Penalty for too short (<200) or too long (>800)\n",
    "   - Normalize score to [0, 1]\n",
    "\n",
    "2. **Citation Count Evaluation** (for GenAIWriter)\n",
    "   - Goal: Ensure articles have page citations\n",
    "   - Parse `article.full_text` for \"See pages:\" pattern\n",
    "   - Score based on number of unique pages cited\n",
    "\n",
    "3. **Key Lesson Quality**\n",
    "   - Goal: Evaluate clarity and conciseness of `article.key_lesson`\n",
    "   - Metrics: Length (ideal 10-20 words), starts with action verb\n",
    "\n",
    "### Template Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_length(article: Article, ideal_min: int = 300, ideal_max: int = 500) -> float:\n",
    "    \"\"\"Evaluate article based on word count.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    \n",
    "    Args:\n",
    "        article: Article object with full_text\n",
    "        ideal_min: Minimum ideal word count\n",
    "        ideal_max: Maximum ideal word count\n",
    "    \n",
    "    Returns:\n",
    "        Score from 0.0 to 1.0\n",
    "        - 1.0: Word count in [ideal_min, ideal_max]\n",
    "        - 0.0: Word count < ideal_min/2 or > ideal_max*2\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def evaluate_citations(article: Article) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate citation quality for GenAIWriter articles.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    \n",
    "    Args:\n",
    "        article: Article object with full_text containing citations\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - has_citations: bool\n",
    "        - num_pages: int (number of unique pages cited)\n",
    "        - citation_text: str (extracted citation string)\n",
    "    \n",
    "    Hint: Look for pattern \"See pages: 42, 87, 103\"\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "if articles:\n",
    "    print(\"üß™ Testing custom evaluation metrics:\\n\")\n",
    "    \n",
    "    for article in articles[:3]:\n",
    "        print(f\"Article: {article.title}\")\n",
    "        \n",
    "        # Test response length (uncomment after implementing)\n",
    "        # length_score = evaluate_response_length(article)\n",
    "        # print(f\"  Length score: {length_score:.2f}\")\n",
    "        \n",
    "        # Test citations (uncomment after implementing)\n",
    "        # citation_metrics = evaluate_citations(article)\n",
    "        # print(f\"  Citations: {citation_metrics}\")\n",
    "        \n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No articles to test\")\n",
    "\n",
    "print(\"üí° Bonus Challenge: Combine multiple metrics into a composite score\")\n",
    "print(\"   Example: 0.4*keyword_quality + 0.3*length_score + 0.3*citation_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**Task 4.2.9**: Common Pitfalls section\n",
    "\n",
    "### ‚ùå Error: \"FileNotFoundError: evals.log\"\n",
    "**Cause**: Log file doesn't exist yet\n",
    "\n",
    "**Solution**: Run the composable app to generate logs\n",
    "```bash\n",
    "cd composable_app\n",
    "streamlit run Main.py\n",
    "# Generate at least 3-5 articles to have meaningful data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Error: \"JSONDecodeError: Expecting value\"\n",
    "**Cause**: Malformed JSON line in evals.log (corrupted write)\n",
    "\n",
    "**Solution**: Gracefully skip bad lines\n",
    "```python\n",
    "try:\n",
    "    obj = json.loads(line)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Skipping malformed line {line_num}: {e}\")\n",
    "    continue\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Error: \"NameError: name 'Article' is not defined\"\n",
    "**Cause**: Using `eval()` without importing Article class\n",
    "\n",
    "**Solution**: Import Article before calling `eval()`\n",
    "```python\n",
    "from agents.article import Article  # Required for eval() to work\n",
    "article = eval(obj['ai_response'])\n",
    "```\n",
    "\n",
    "**Security Note**: `eval()` is dangerous with untrusted input. Only use on logs you generated.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Warning: SentenceTransformer downloads 90MB on first run\n",
    "**Cause**: Model files cached in `~/.cache/torch/sentence_transformers/`\n",
    "\n",
    "**Solutions**:\n",
    "1. **Wait for download** (one-time, ~30 seconds on broadband)\n",
    "2. **Use smaller model**: `paraphrase-MiniLM-L3-v2` (33MB)\n",
    "3. **Offline mode**: Pre-download model to shared cache\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Warning: \"Mean variance is very small (< 0.01)\"\n",
    "**Cause**: Using `np.mean(embedding)` collapses 384-dim vector to scalar, losing information\n",
    "\n",
    "**Solution**: Use full embedding variance or pairwise distances\n",
    "```python\n",
    "# Better: Full variance across all dimensions\n",
    "full_variance = np.mean(np.var(embeddings, axis=0))\n",
    "\n",
    "# Best: Average pairwise cosine distance\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "distances = cosine_distances(embeddings)\n",
    "diversity = np.mean(distances[np.triu_indices_from(distances, k=1)])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Tip: Incremental evaluation during development\n",
    "\n",
    "**Problem**: Running full evaluation on every prompt change is slow\n",
    "\n",
    "**Solution**: Evaluate on sample during dev, full dataset before deploy\n",
    "```python\n",
    "# Development: Quick iteration\n",
    "sample_articles = articles[:10]  # Evaluate on 10 articles\n",
    "\n",
    "# Pre-deployment: Comprehensive\n",
    "all_articles = get_records()  # Evaluate on all articles\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Tip: Version your evaluation metrics\n",
    "\n",
    "**Problem**: Hard to compare evaluations across time if metrics change\n",
    "\n",
    "**Solution**: Tag metrics with version in output\n",
    "```python\n",
    "result = {\n",
    "    \"metric_version\": \"v1.0\",\n",
    "    \"timestamp\": \"2025-11-05T10:00:00Z\",\n",
    "    \"mean_score\": 1.42,\n",
    "    \"variance\": 0.18\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Self-Assessment\n",
    "\n",
    "**Task 4.2.10**: Self-assessment questions with answers\n",
    "\n",
    "### Question 1: Concept Check\n",
    "**What's the difference between quantitative metrics and LLM-as-judge evaluation?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Quantitative Metrics**:\n",
    "- **What**: Measurable properties (count, length, diversity, latency)\n",
    "- **Cost**: Free (no LLM calls)\n",
    "- **Speed**: Fast (can run on every output)\n",
    "- **Limitations**: May miss semantic quality issues\n",
    "- **Example**: `score = 1.0 - abs(len(keywords) - 5) / 5.0`\n",
    "\n",
    "**LLM-as-Judge**:\n",
    "- **What**: Semantic evaluation (correctness, helpfulness, tone)\n",
    "- **Cost**: Expensive (requires LLM API calls)\n",
    "- **Speed**: Slow (latency per evaluation)\n",
    "- **Limitations**: Bias, inconsistency, cost\n",
    "- **Example**: `judge_llm.evaluate(\"Rate the accuracy of this answer (0-10): ...\")`\n",
    "\n",
    "**Best Practice**: Use quantitative metrics for continuous monitoring, LLM-as-judge for spot checks and deep analysis.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: Implementation\n",
    "**Why does `evaluate_keywords.py` use `eval()` to reconstruct Article objects? Is this safe?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Why `eval()`?**\n",
    "\n",
    "The log stores Article objects as their `__repr__` string:\n",
    "```python\n",
    "# In evals.log\n",
    "{\"ai_response\": \"Article(title='Photosynthesis', full_text='...', ...)\"}\n",
    "```\n",
    "\n",
    "Using `eval()` reconstructs the Article from this string:\n",
    "```python\n",
    "article = eval(\"Article(title='Photosynthesis', ...)\")  # Creates Article object\n",
    "```\n",
    "\n",
    "**Is it safe?**\n",
    "\n",
    "‚ùå **No, `eval()` is dangerous with untrusted input** - can execute arbitrary code\n",
    "\n",
    "‚úÖ **Acceptable here** because:\n",
    "1. Logs are self-generated (not user input)\n",
    "2. Running locally (not production service)\n",
    "3. Requires Article class import (limits scope)\n",
    "\n",
    "**Better alternatives for production**:\n",
    "```python\n",
    "# Option 1: Store as JSON dict, reconstruct with dataclass\n",
    "obj = json.loads(line)\n",
    "article = Article(**obj['ai_response'])  # Safer\n",
    "\n",
    "# Option 2: Use ast.literal_eval (safer than eval)\n",
    "import ast\n",
    "article = ast.literal_eval(obj['ai_response'])  # Only literals, no functions\n",
    "\n",
    "# Option 3: Use pickle (binary, efficient)\n",
    "import pickle\n",
    "article = pickle.loads(obj['ai_response'])  # Type-safe\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: Metrics Design\n",
    "**The diversity metric uses `np.mean(embedding)` which collapses 384 dimensions to 1 scalar. Why is this problematic?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Problem**: Information loss\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Keyword 1: \"photosynthesis\"\n",
    "emb1 = [0.5, -0.3, 0.8, ..., 0.1]  # 384 dims\n",
    "mean1 = np.mean(emb1)  # Collapses to ~0.15\n",
    "\n",
    "# Keyword 2: \"chloroplast\" (semantically related)\n",
    "emb2 = [0.4, -0.2, 0.7, ..., 0.0]  # 384 dims\n",
    "mean2 = np.mean(emb2)  # Collapses to ~0.12\n",
    "```\n",
    "\n",
    "**Issue**: Two very different 384-dim vectors collapse to similar scalars (0.15 vs 0.12)\n",
    "- **Result**: Diversity appears low even if keywords are actually diverse\n",
    "- **Variance of means**: `var([0.15, 0.12]) = 0.0009` (tiny!)\n",
    "\n",
    "**Better approaches**:\n",
    "\n",
    "1. **Full embedding variance**:\n",
    "```python\n",
    "# Variance across all 384 dimensions\n",
    "full_variance = np.mean(np.var(embeddings, axis=0))\n",
    "```\n",
    "\n",
    "2. **Pairwise cosine distance** (best):\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "distances = cosine_distances(embeddings)\n",
    "# Average distance between all keyword pairs\n",
    "diversity = np.mean(distances[np.triu_indices_from(distances, k=1)])\n",
    "```\n",
    "\n",
    "**Why original code uses mean**: Simplicity for educational demo. Production code should use pairwise distances.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4: Interpretation\n",
    "**You run evaluation and get: mean=1.2, variance=0.5, skewness=+1.8. What does this tell you?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Analysis**:\n",
    "\n",
    "1. **Mean = 1.2** (moderate quality)\n",
    "   - Above 1.0 (passing) but below 1.5 (excellent)\n",
    "   - Interpretation: System is functional but needs improvement\n",
    "\n",
    "2. **Variance = 0.5** (high variance)\n",
    "   - Std dev = ‚àö0.5 ‚âà 0.71\n",
    "   - Large spread: Some outputs score ~0.5, others score ~1.9\n",
    "   - Interpretation: **Inconsistent quality** - prompt is sensitive to input\n",
    "\n",
    "3. **Skewness = +1.8** (strong positive skew)\n",
    "   - Distribution has long tail to the right\n",
    "   - **Most outputs are low-scoring** with a few high-scoring outliers\n",
    "   - Interpretation: System **usually fails** but occasionally produces good results\n",
    "\n",
    "**Diagnosis**:\n",
    "```\n",
    "Score Distribution:\n",
    "\n",
    "       |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     ‚Üê Most articles (poor quality)\n",
    "Count  |‚ñà‚ñà‚ñà\n",
    "       |‚ñà‚ñà\n",
    "       |‚ñà\n",
    "       |      ‚ñà   ‚ñà   ‚ñà‚ñà           ‚Üê Few articles (good quality)\n",
    "       +---------------------------\n",
    "        0.5  1.0  1.5  2.0  2.5    Score\n",
    "```\n",
    "\n",
    "**Action Items**:\n",
    "1. **Investigate outliers**: What makes the high-scoring articles good? Can we replicate?\n",
    "2. **Reduce variance**: Add few-shot examples to prompt for consistency\n",
    "3. **Shift distribution left**: Improve base prompt to raise mean score\n",
    "4. **Root cause**: Check if certain topics/writers perform worse (use metadata filtering)\n",
    "\n",
    "**Compare to ideal stats**:\n",
    "- Ideal: `mean=1.8, variance=0.05, skewness=0.0` (high, consistent, symmetric)\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: Advanced\n",
    "**How would you detect if prompt changes improved or degraded quality?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Approach: A/B Testing with Statistical Significance**\n",
    "\n",
    "**Step 1: Baseline evaluation**\n",
    "```python\n",
    "# Before prompt change\n",
    "baseline_scores = [evaluate(a) for a in baseline_articles]\n",
    "baseline_mean = np.mean(baseline_scores)  # e.g., 1.2\n",
    "```\n",
    "\n",
    "**Step 2: Experiment evaluation**\n",
    "```python\n",
    "# After prompt change\n",
    "experiment_scores = [evaluate(a) for a in experiment_articles]\n",
    "experiment_mean = np.mean(experiment_scores)  # e.g., 1.5\n",
    "```\n",
    "\n",
    "**Step 3: Statistical test** (t-test for mean difference)\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t_stat, p_value = ttest_ind(baseline_scores, experiment_scores)\n",
    "\n",
    "if p_value < 0.05:  # Statistically significant\n",
    "    if experiment_mean > baseline_mean:\n",
    "        print(\"‚úÖ Prompt change IMPROVED quality (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"‚ùå Prompt change DEGRADED quality (p < 0.05)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No significant difference (p ‚â• 0.05) - need more data\")\n",
    "```\n",
    "\n",
    "**Step 4: Effect size** (how much better?)\n",
    "```python\n",
    "# Cohen's d: Standardized mean difference\n",
    "pooled_std = np.sqrt((np.var(baseline_scores) + np.var(experiment_scores)) / 2)\n",
    "cohens_d = (experiment_mean - baseline_mean) / pooled_std\n",
    "\n",
    "# Interpretation:\n",
    "# |d| < 0.2: Negligible effect\n",
    "# |d| = 0.5: Medium effect\n",
    "# |d| > 0.8: Large effect\n",
    "\n",
    "print(f\"Effect size (Cohen's d): {cohens_d:.2f}\")\n",
    "```\n",
    "\n",
    "**Best Practices**:\n",
    "1. **Same topics**: Evaluate both prompts on identical set of topics (controlled)\n",
    "2. **Sufficient sample**: Need ‚â•30 articles for reliable t-test\n",
    "3. **Multiple metrics**: Don't just compare means - check variance, outliers, failure modes\n",
    "4. **Version control**: Tag evals.log entries with prompt version for retrospective analysis\n",
    "\n",
    "**Example versioning**:\n",
    "```python\n",
    "# In save_for_eval.py\n",
    "record_ai_response(target, ai_input, ai_response, metadata={\n",
    "    \"prompt_version\": \"v2.1\",\n",
    "    \"timestamp\": \"2025-11-05T10:00:00Z\"\n",
    "})\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Avoiding Judge Bias\n\n**Task 4.3.3**: Avoiding judge bias (randomization, calibration)\n\n**Problem**: LLM judges have systematic biases that can skew evaluations\n\n### Common Biases\n\n#### 1. **Position Bias**\n**Issue**: Judge prefers first or last option in comparisons\n\n**Example**:\n```python\n# Bad: Position influences score\nprompt = f\"Rate A vs B: A={article1.text}, B={article2.text}\"\n# Judge tends to favor A (primacy bias) or B (recency bias)\n```\n\n**Solution**: Randomize presentation order\n```python\nimport random\n\n# Randomly swap order\nif random.random() < 0.5:\n    prompt = f\"Rate A vs B: A={article1.text}, B={article2.text}\"\n    swap = False\nelse:\n    prompt = f\"Rate A vs B: A={article2.text}, B={article1.text}\"\n    swap = True\n\nresult = await judge.run(prompt)\n\n# Unswap scores if needed\nif swap:\n    result.score_A, result.score_B = result.score_B, result.score_A\n```\n\n#### 2. **Length Bias**  \n**Issue**: Judge favors longer responses (assumes more detail = better quality)\n\n**Example**: \n- Article A: 200 words, high quality\n- Article B: 800 words, low quality (verbose)\n- Judge gives B higher score due to length alone\n\n**Solution**: \n1. **Blind evaluation**: Don't show judge the full text length\n2. **Explicit instructions**: \"Judge quality, not quantity. Penalize verbosity.\"\n3. **Control group**: Include known reference articles of varying lengths\n\n#### 3. **Self-Preference Bias**\n**Issue**: Judge (same model as generator) favors its own outputs\n\n**Example**:\n- Generator: Gemini 2.0\n- Judge: Gemini 2.0 (same model)\n- Result: Gemini judges its own style favorably, penalizes other models\n\n**Solution**: Use different model for judge\n```python\n# Generator\nwriter_agent = Agent('gemini-2.0-flash', ...)\n\n# Judge: Use different model family\njudge_agent = Agent('claude-3-5-sonnet-20241022', ...)  # Different model\n```\n\n#### 4. **Prompt Sensitivity**\n**Issue**: Small wording changes in judge prompt cause large score swings\n\n**Example**:\n- Prompt A: \"Rate quality (0-10)\" ‚Üí Mean score: 6.5\n- Prompt B: \"Rate excellence (0-10)\" ‚Üí Mean score: 4.2 (stricter interpretation)\n\n**Solution**: Calibrate prompts with ground truth dataset\n```python\n# Step 1: Create calibration set with human ratings\ncalibration = [\n    {\"article\": article1, \"human_score\": 8},\n    {\"article\": article2, \"human_score\": 5},\n    # ... 20-50 examples\n]\n\n# Step 2: Test prompt variants\nfor prompt_version in [\"v1\", \"v2\", \"v3\"]:\n    judge_scores = [await judge.evaluate(item[\"article\"], prompt_version) \n                    for item in calibration]\n    human_scores = [item[\"human_score\"] for item in calibration]\n    \n    # Step 3: Measure correlation\n    from scipy.stats import pearsonr\n    correlation, p_value = pearsonr(judge_scores, human_scores)\n    \n    print(f\"{prompt_version}: r={correlation:.3f} (p={p_value:.3f})\")\n    # Choose prompt with highest correlation to human judgment\n```\n\n### Best Practices for Reducing Bias\n\n1. **Randomization**: Shuffle presentation order, use multiple judges\n2. **Calibration**: Validate against human ratings on sample dataset\n3. **Cross-validation**: Use multiple judge models, average scores\n4. **Explicit criteria**: Provide concrete rubric (see structured outputs above)\n5. **Anchoring**: Include reference examples (\"Score 5 looks like..., Score 10 looks like...\")\n6. **Blind evaluation**: Remove identifying information (author, model name)\n\n### Example: Multi-Judge Consensus\n\n**Reduce bias by averaging scores from multiple judge models:**\n\n```python\n# Use 3 different judge models\njudges = [\n    Agent('gemini-2.0-flash'),\n    Agent('gpt-4o'),\n    Agent('claude-3-5-sonnet-20241022')\n]\n\n# Get scores from all judges\nscores = []\nfor judge in judges:\n    result = await judge.evaluate(article)\n    scores.append(result.overall_score)\n\n# Average scores (reduces individual model bias)\nconsensus_score = np.mean(scores)\nvariance = np.var(scores)\n\nif variance > 2.0:\n    print(f\"‚ö†Ô∏è High variance ({variance:.1f}) - judges disagree!\")\nelse:\n    print(f\"‚úÖ Consensus score: {consensus_score:.1f} (low variance)\")\n```\n\n**When to use multi-judge**:\n- ‚úÖ High-stakes evaluations (production deployment decisions)\n- ‚úÖ Subjective qualities (tone, creativity, helpfulness)\n- ‚ùå Cost-sensitive scenarios (3x API costs)\n- ‚ùå Simple objective metrics (use quantitative instead)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pydantic import BaseModel, Field\nfrom typing import List\n\nclass ArticleEvaluation(BaseModel):\n    \"\"\"Structured evaluation result for article quality.\"\"\"\n    \n    correctness_score: int = Field(..., ge=0, le=10, description=\"Factual accuracy (0-10)\")\n    helpfulness_score: int = Field(..., ge=0, le=10, description=\"Usefulness for target audience (0-10)\")\n    tone_score: int = Field(..., ge=0, le=10, description=\"Tone appropriateness (0-10)\")\n    \n    strengths: List[str] = Field(..., min_length=1, description=\"What the article does well\")\n    weaknesses: List[str] = Field(default_factory=list, description=\"Areas for improvement\")\n    \n    overall_score: int = Field(..., ge=0, le=10, description=\"Combined quality score (0-10)\")\n    reasoning: str = Field(..., min_length=10, description=\"Brief explanation of scores\")\n\n# Example: How to use with Pydantic AI (pseudocode - requires API key)\n\"\"\"\nfrom pydantic_ai import Agent\n\njudge_agent = Agent(\n    'gemini-2.0-flash',\n    result_type=ArticleEvaluation,  # Enforces structured output\n    system_prompt='''\n    You are an expert evaluator for K-12 educational content.\n    Rate articles on correctness, helpfulness, and tone.\n    Provide specific, actionable feedback.\n    '''\n)\n\n# Evaluate an article\nresult = await judge_agent.run(f'''\nEvaluate this article about \"{article.title}\":\n\n{article.full_text}\n\nRate on three dimensions (0-10 each):\n1. Correctness: Factual accuracy\n2. Helpfulness: Usefulness for K-12 students  \n3. Tone: Age-appropriate and engaging\n\nProvide strengths, weaknesses, overall score, and reasoning.\n''')\n\nevaluation: ArticleEvaluation = result.output\n\nprint(f\"Correctness: {evaluation.correctness_score}/10\")\nprint(f\"Helpfulness: {evaluation.helpfulness_score}/10\")\nprint(f\"Tone: {evaluation.tone_score}/10\")\nprint(f\"Overall: {evaluation.overall_score}/10\")\nprint(f\"\\\\nStrengths: {', '.join(evaluation.strengths)}\")\nprint(f\"Weaknesses: {', '.join(evaluation.weaknesses)}\")\nprint(f\"\\\\nReasoning: {evaluation.reasoning}\")\n\"\"\"\n\n# Demo: Manual structured evaluation (no API call)\nprint(\"üìã Example Structured Evaluation:\\n\")\n\nsample_eval = ArticleEvaluation(\n    correctness_score=9,\n    helpfulness_score=7,\n    tone_score=8,\n    strengths=[\n        \"Factually accurate with good citations\",\n        \"Clear explanations of complex concepts\",\n        \"Engaging examples (photosynthesis in everyday plants)\"\n    ],\n    weaknesses=[\n        \"Could use more visual aids or diagrams\",\n        \"Conclusion feels rushed\",\n        \"Missing connection to real-world applications\"\n    ],\n    overall_score=8,\n    reasoning=\"Strong factual foundation with accessible tone. Main weakness is lack of visual aids and deeper real-world connections, which would enhance student engagement.\"\n)\n\nprint(f\"Correctness: {sample_eval.correctness_score}/10\")\nprint(f\"Helpfulness: {sample_eval.helpfulness_score}/10\")\nprint(f\"Tone: {sample_eval.tone_score}/10\")\nprint(f\"Overall: {sample_eval.overall_score}/10\")\nprint(f\"\\nStrengths:\")\nfor s in sample_eval.strengths:\n    print(f\"  ‚úÖ {s}\")\nprint(f\"\\nWeaknesses:\")\nfor w in sample_eval.weaknesses:\n    print(f\"  ‚ö†Ô∏è {w}\")\nprint(f\"\\nReasoning: {sample_eval.reasoning}\")\n\nprint(\"\\nüí° Benefits of Structured Outputs:\")\nprint(\"  1. Machine-readable: Easy to aggregate scores across evaluations\")\nprint(\"  2. Consistent format: Always get same fields, no parsing errors\")\nprint(\"  3. Type-safe: Pydantic validates ranges (0-10), required fields\")\nprint(\"  4. Actionable: Separate strengths/weaknesses guide improvements\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Structured Judge Outputs\n\n**Task 4.3.2**: Structured judge outputs (scores + reasoning)\n\n**Problem**: Free-text judge responses are hard to parse and analyze\n\n**Bad Example** (unstructured):\n```\nJudge output: \"This article is pretty good, I'd give it an 8 out of 10.\nThe tone is appropriate and it covers the main points, but it could\nuse more examples. Also the conclusion feels rushed.\"\n```\n\nProblems:\n- Score buried in text (hard to extract)\n- No machine-readable reasoning\n- Inconsistent format across evaluations\n\n**Better: Structured Output with Pydantic**\n\nUse Pydantic AI's result type to enforce structure:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Metric 2: Helpfulness Evaluation\n\n**Goal**: Evaluate if article addresses the user's actual information need\n\n**Judge Prompt**:\n```python\njudge_prompt = f\"\"\"\nRate how helpful this Article is for a K-12 student learning about \"{topic}\" (0-10):\n\nArticle: {article.full_text}\n\nCriteria:\n- Answers the core question about {topic}: +3 points\n- Provides practical examples or analogies: +3 points\n- Appropriate depth for K-12 level (not too simple, not too advanced): +2 points\n- Well-structured (intro, body, conclusion): +2 points\n\nScore (0-10):\nBrief reasoning:\n\"\"\"\n```\n\n**Key insight**: Helpfulness depends on **user context** (K-12 student vs. researcher vs. general public)\n- Always specify target audience in judge prompt\n- Consider creating persona-specific judges (e.g., \"conservative parent\" perspective)\n\n### Metric 3: Tone Evaluation\n\n**Goal**: Evaluate if tone is appropriate for target audience (K-12 educational)\n\n**Judge Prompt**:\n```python\njudge_prompt = f\"\"\"\nRate the tone appropriateness of this Article for K-12 students (0-10):\n\nArticle: {article.full_text}\n\nCriteria:\n- Engaging and accessible (not dry or overly academic): +3 points\n- Age-appropriate vocabulary (explains technical terms): +2 points\n- Respectful and inclusive language: +2 points\n- Encouraging learning (not condescending): +3 points\n\nScore (0-10):\nIssues found (if any):\n\"\"\"\n```\n\n**Multi-dimensional tone**:\n- **Formality**: Casual ‚Üî Academic\n- **Complexity**: Simple ‚Üî Technical\n- **Emotion**: Neutral ‚Üî Enthusiastic\n- **Respect**: Condescending ‚Üî Empowering\n\n**Tip**: Use multiple judges with different persona prompts to catch tone issues from diverse perspectives (see ReviewerPanel)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Metric 1: Correctness Evaluation\n\n**Task 4.3.1**: LLM-as-judge metrics (correctness, helpfulness, tone)\n\n**Goal**: Evaluate if article content is factually accurate\n\n**Challenge**: Need ground truth or reference answer for comparison\n\n**Approaches**:\n\n1. **Reference-based**: Compare to known correct answer\n```python\njudge_prompt = f\"\"\"\nRate the factual correctness of the Answer compared to the Reference (0-10):\n\nQuestion: {topic}\nAnswer: {article.full_text}\nReference: {ground_truth_content}\n\nCriteria:\n- All facts in Answer are present in Reference: +4 points\n- No hallucinations or false claims in Answer: +4 points  \n- Answer includes key details from Reference: +2 points\n\nScore (0-10): \n\"\"\"\n```\n\n2. **Self-consistency**: Generate multiple answers, check agreement\n```python\n# Generate 5 answers to same question\nanswers = [await writer.write_about(topic) for _ in range(5)]\n\n# Judge: Do all 5 answers agree on key facts?\nconsistency_score = judge_llm.check_consistency(answers)\n# High consistency ‚Üí likely correct\n# Low consistency ‚Üí prompt ambiguity or knowledge gap\n```\n\n3. **Retrieval-augmented judge**: Give judge access to source documents\n```python\njudge_prompt = f\"\"\"\nCheck if this Article is grounded in the Source Documents:\n\nArticle: {article.full_text}\nSource Documents: {retrieved_chunks}\n\nFor each claim in Article:\n1. Is it supported by Source Documents? (yes/no/partial)\n2. If no, mark as potential hallucination\n\nHallucination count: \nGrounding score (0-10):\n```\n\n**Production Tip**: Use retrieval-augmented judge for RAG applications (like GenAIWriter)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Advanced: LLM-as-Judge Evaluation\n\n**Task 4.3**: Evaluation methodologies - LLM-as-judge, structured outputs, bias avoidance\n\n### When to Use LLM-as-Judge\n\n**Quantitative metrics** (covered above) work well for measurable properties, but fail for semantic quality:\n- ‚ùå **Correctness**: Is the answer factually accurate?\n- ‚ùå **Helpfulness**: Does it address the user's actual question?\n- ‚ùå **Tone**: Is it appropriate for K-12 audience?\n- ‚ùå **Creativity**: Is the explanation engaging?\n\n**LLM-as-Judge** uses an LLM to evaluate another LLM's output:\n\n```python\n# Pseudocode\njudge_prompt = f\"\"\"\nRate the {quality} of this response (0-10):\nInput: {user_query}\nOutput: {llm_response}\n\"\"\"\nscore = await judge_llm.run(judge_prompt)\n```\n\n### Cost-Quality Trade-off\n\n| Evaluation Method | Cost | Speed | Semantic Quality | Best For |\n|-------------------|------|-------|------------------|----------|\n| **Quantitative** | $0 | Fast | ‚ùå Misses meaning | Continuous monitoring |\n| **LLM-as-Judge** | $0.01-0.10/eval | Slow | ‚úÖ Understands meaning | Spot checks, deep analysis |\n| **Human** | $5-20/eval | Very slow | ‚úÖ‚úÖ Gold standard | Final validation |\n\n**Best Practice**: Combine approaches\n1. **Quantitative**: Run on every output (fast feedback loop)\n2. **LLM-as-Judge**: Sample 10% of outputs (quality assurance)\n3. **Human**: Review edge cases and failures (ground truth)\n\n**Code Location**: See [`llm_as_judge_tutorial.ipynb`](llm_as_judge_tutorial.ipynb) for guardrails implementation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue Learning\n",
    "1. **[LLM-as-Judge Tutorial](llm_as_judge_tutorial.ipynb)** - Semantic evaluation with guardrails\n",
    "2. **[Horizontal Services](../concepts/horizontal_services.md)** - Learn about evaluation recording architecture\n",
    "3. **[Advanced Patterns](advanced_patterns.ipynb)** - Cost and latency optimization\n",
    "\n",
    "### Hands-On Practice\n",
    "1. **Complete Exercise**: Implement response length and citation evaluation metrics\n",
    "2. **Custom metric**: Design evaluation for your specific use case\n",
    "3. **A/B testing**: Compare two prompt versions using t-test\n",
    "4. **Visualization**: Plot score distributions with matplotlib/seaborn\n",
    "\n",
    "### Advanced Challenges\n",
    "1. **Real-time dashboard**: Stream evals.log to Streamlit dashboard with live metrics\n",
    "2. **Regression detection**: Alert when mean score drops below threshold\n",
    "3. **Multi-dimensional analysis**: Cluster articles by topic and compare quality across clusters\n",
    "4. **Combine metrics**: Create composite score (e.g., 0.4*keywords + 0.3*length + 0.3*citations)\n",
    "\n",
    "### Production Considerations\n",
    "1. **Sampling**: Evaluate on random sample during high traffic (cost/latency)\n",
    "2. **Versioning**: Tag all logs with model version, prompt version, timestamp\n",
    "3. **Storage**: Rotate logs, archive to S3/GCS for historical analysis\n",
    "4. **Alerting**: Set up alerts for quality degradation (PagerDuty, Slack)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've learned how to systematically evaluate LLM outputs using quantitative metrics. You can now:\n",
    "- Parse and analyze `evals.log` for quality insights\n",
    "- Implement composite metrics (count + diversity)\n",
    "- Use statistical analysis to detect improvements or regressions\n",
    "- Design custom evaluation metrics for your use case\n",
    "\n",
    "**Tutorial Version**: 1.0  \n",
    "**Last Updated**: 2025-11-05  \n",
    "**Estimated Time**: 30-35 minutes  \n",
    "**Cost**: $0.00 (reads local logs, downloads ~90MB model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}