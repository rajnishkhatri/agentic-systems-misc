{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Pattern Comparison Tutorial\n",
    "\n",
    "**Lesson 14 - Task 5.9**: Interactive tutorial comparing 5 multi-agent coordination patterns.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "1. Understand 5 multi-agent coordination patterns through simplified simulations\n",
    "2. Compare trade-offs between patterns (latency vs. quality vs. cost)\n",
    "3. Learn when to use each pattern based on task characteristics\n",
    "4. Visualize pattern performance using radar charts\n",
    "\n",
    "## Pattern Overview\n",
    "\n",
    "| Pattern | Description | Best For | Trade-offs |\n",
    "|---------|-------------|----------|------------|\n",
    "| **Hierarchical** | Central orchestrator delegates to specialists | Complex tasks with clear subtasks | Higher latency, better quality |\n",
    "| **Diamond** | Broadcast ‚Üí Generate ‚Üí Select best | Optimization problems | Higher cost (parallel LLM calls) |\n",
    "| **P2P** | Peer-to-peer handoff with context | Sequential pipelines | Context loss risk |\n",
    "| **Collaborative** | Shared workspace with peer review | Creative tasks | Coordination overhead |\n",
    "| **Adaptive Loop** | Iterative refinement until quality threshold | Quality-critical tasks | Variable latency |\n",
    "\n",
    "## Execution Modes\n",
    "\n",
    "- **DEMO Mode**: 5 scenarios, simulated execution (~$0, <2 min)\n",
    "- **FULL Mode**: 30 scenarios, simulated execution (~$0, <5 min)\n",
    "\n",
    "**Note**: This notebook uses *simulated* pattern execution to demonstrate concepts. For production implementation, see `backend/multi_agent_patterns.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Select Execution Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION MODE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Change this to \"FULL\" for comprehensive evaluation\n",
    "MODE = \"DEMO\"  # Options: \"DEMO\" or \"FULL\"\n",
    "\n",
    "# Mode-specific configuration\n",
    "MODE_CONFIG = {\n",
    "    \"DEMO\": {\n",
    "        \"num_scenarios\": 5,\n",
    "        \"estimated_time\": \"<2 min\",\n",
    "    },\n",
    "    \"FULL\": {\n",
    "        \"num_scenarios\": 30,\n",
    "        \"estimated_time\": \"<5 min\",\n",
    "    },\n",
    "}\n",
    "\n",
    "config = MODE_CONFIG[MODE]\n",
    "print(f\"üîß Mode: {MODE}\")\n",
    "print(f\"üìä Scenarios: {config['num_scenarios']}\")\n",
    "print(f\"‚è±Ô∏è  Estimated Time: {config['estimated_time']}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: This notebook uses SIMULATED execution to demonstrate pattern concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test scenarios\n",
    "data_path = Path(\"data/multi_agent_scenarios.json\")\n",
    "assert data_path.exists(), f\"Data file not found: {data_path}\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Select scenarios based on mode\n",
    "all_scenarios = dataset[\"scenarios\"]\n",
    "scenarios = all_scenarios[:config[\"num_scenarios\"]]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(scenarios)} test scenarios\")\n",
    "print(f\"\\nScenario Preview:\")\n",
    "for i, scenario in enumerate(scenarios[:3], 1):\n",
    "    print(f\"  {i}. [{scenario['expected_pattern'].upper()}] {scenario['query'][:60]}...\")\n",
    "if len(scenarios) > 3:\n",
    "    print(f\"  ... and {len(scenarios) - 3} more scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Pattern Simulation\n",
    "\n",
    "Instead of running actual LLM calls, we'll simulate pattern behavior based on theoretical characteristics.\n",
    "This allows us to understand pattern trade-offs without API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_hierarchical(scenario: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Simulate hierarchical pattern execution.\n",
    "    \n",
    "    Characteristics:\n",
    "    - Higher latency due to orchestration overhead\n",
    "    - Higher quality due to specialist coordination\n",
    "    - Moderate cost (multiple agents, but sequential)\n",
    "    \"\"\"\n",
    "    base_latency = scenario[\"evaluation_criteria\"].get(\"latency_target\", 5.0)\n",
    "    num_workers = scenario[\"evaluation_criteria\"].get(\"agent_utilization\", 3)\n",
    "    \n",
    "    # Hierarchical adds orchestration overhead but improves quality\n",
    "    latency = base_latency * 1.2  # 20% overhead for coordination\n",
    "    cost = num_workers * 0.05  # Cost per worker\n",
    "    quality = min(1.0, scenario[\"evaluation_criteria\"].get(\"quality_threshold\", 0.85) + 0.05)\n",
    "    \n",
    "    return {\"latency\": latency, \"cost\": cost, \"quality\": quality, \"agent_count\": num_workers}\n",
    "\n",
    "\n",
    "def simulate_diamond(scenario: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Simulate diamond (competitive) pattern execution.\n",
    "    \n",
    "    Characteristics:\n",
    "    - Lower latency (parallel execution)\n",
    "    - Higher cost (N competing agents)\n",
    "    - High quality (best-of-N selection)\n",
    "    \"\"\"\n",
    "    base_latency = scenario[\"evaluation_criteria\"].get(\"latency_target\", 5.0)\n",
    "    num_agents = 3  # Typically 3-5 competing agents\n",
    "    \n",
    "    latency = base_latency * 0.6  # Parallel execution is faster\n",
    "    cost = num_agents * 0.08  # Pay for all competing agents\n",
    "    quality = min(1.0, scenario[\"evaluation_criteria\"].get(\"quality_threshold\", 0.85) + 0.08)\n",
    "    \n",
    "    return {\"latency\": latency, \"cost\": cost, \"quality\": quality, \"agent_count\": num_agents}\n",
    "\n",
    "\n",
    "def simulate_p2p(scenario: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Simulate peer-to-peer handoff pattern execution.\n",
    "    \n",
    "    Characteristics:\n",
    "    - Moderate latency (sequential pipeline)\n",
    "    - Lower cost (minimal overhead)\n",
    "    - Quality depends on context preservation\n",
    "    \"\"\"\n",
    "    base_latency = scenario[\"evaluation_criteria\"].get(\"latency_target\", 5.0)\n",
    "    num_steps = len(scenario.get(\"reference_trajectory\", []))\n",
    "    \n",
    "    latency = base_latency * 0.9  # Efficient handoffs\n",
    "    cost = num_steps * 0.04  # Lower cost per step\n",
    "    quality = scenario[\"evaluation_criteria\"].get(\"quality_threshold\", 0.85) - 0.03  # Context loss risk\n",
    "    \n",
    "    return {\"latency\": latency, \"cost\": cost, \"quality\": quality, \"agent_count\": num_steps}\n",
    "\n",
    "\n",
    "def simulate_collaborative(scenario: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Simulate collaborative pattern execution.\n",
    "    \n",
    "    Characteristics:\n",
    "    - Moderate latency (peer review adds time)\n",
    "    - Moderate cost (shared workspace)\n",
    "    - High quality for creative tasks\n",
    "    \"\"\"\n",
    "    base_latency = scenario[\"evaluation_criteria\"].get(\"latency_target\", 5.0)\n",
    "    num_peers = 3\n",
    "    \n",
    "    # Collaborative pattern benefits creative tasks\n",
    "    is_creative = scenario[\"query_type\"] in [\"creative_writing\", \"creative_design\", \"brainstorming\"]\n",
    "    quality_boost = 0.10 if is_creative else 0.03\n",
    "    \n",
    "    latency = base_latency * 1.1  # Peer review overhead\n",
    "    cost = num_peers * 0.06\n",
    "    quality = min(1.0, scenario[\"evaluation_criteria\"].get(\"quality_threshold\", 0.85) + quality_boost)\n",
    "    \n",
    "    return {\"latency\": latency, \"cost\": cost, \"quality\": quality, \"agent_count\": num_peers}\n",
    "\n",
    "\n",
    "def simulate_adaptive_loop(scenario: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Simulate adaptive loop pattern execution.\n",
    "    \n",
    "    Characteristics:\n",
    "    - Variable latency (depends on iterations to converge)\n",
    "    - Moderate cost (iterative refinement)\n",
    "    - Highest quality (meets threshold)\n",
    "    \"\"\"\n",
    "    base_latency = scenario[\"evaluation_criteria\"].get(\"latency_target\", 5.0)\n",
    "    quality_target = scenario[\"evaluation_criteria\"].get(\"quality_threshold\", 0.85)\n",
    "    \n",
    "    # Simulate 2-4 iterations based on difficulty\n",
    "    difficulty = scenario.get(\"difficulty\", \"medium\")\n",
    "    iterations = {\"easy\": 2, \"medium\": 3, \"hard\": 4}[difficulty]\n",
    "    \n",
    "    latency = base_latency * (1 + iterations * 0.3)  # Each iteration adds time\n",
    "    cost = iterations * 0.06\n",
    "    quality = min(1.0, quality_target + 0.10)  # Meets or exceeds target\n",
    "    \n",
    "    return {\"latency\": latency, \"cost\": cost, \"quality\": quality, \"agent_count\": iterations}\n",
    "\n",
    "\n",
    "# Pattern simulation functions\n",
    "pattern_simulators = {\n",
    "    \"hierarchical\": simulate_hierarchical,\n",
    "    \"diamond\": simulate_diamond,\n",
    "    \"p2p\": simulate_p2p,\n",
    "    \"collaborative\": simulate_collaborative,\n",
    "    \"adaptive_loop\": simulate_adaptive_loop,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Initialized 5 pattern simulators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pattern Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "results = []\n",
    "total_experiments = len(scenarios) * len(pattern_simulators)\n",
    "\n",
    "print(f\"üöÄ Starting simulation: {len(scenarios)} scenarios √ó {len(pattern_simulators)} patterns = {total_experiments} executions\\n\")\n",
    "\n",
    "with tqdm(total=total_experiments, desc=\"Running simulations\") as pbar:\n",
    "    for scenario in scenarios:\n",
    "        scenario_id = scenario[\"scenario_id\"]\n",
    "        query = scenario[\"query\"]\n",
    "        \n",
    "        for pattern_name, simulator in pattern_simulators.items():\n",
    "            # Simulate pattern execution\n",
    "            metrics = simulator(scenario)\n",
    "            \n",
    "            # Add small random variation (¬±10%) to make it realistic\n",
    "            noise = np.random.uniform(0.9, 1.1)\n",
    "            metrics[\"latency\"] *= noise\n",
    "            metrics[\"cost\"] *= noise\n",
    "            \n",
    "            # Record result\n",
    "            results.append({\n",
    "                \"scenario_id\": scenario_id,\n",
    "                \"query\": query,\n",
    "                \"pattern\": pattern_name,\n",
    "                \"latency\": metrics[\"latency\"],\n",
    "                \"cost\": metrics[\"cost\"],\n",
    "                \"quality\": metrics[\"quality\"],\n",
    "                \"agent_count\": metrics[\"agent_count\"],\n",
    "                \"success\": True,\n",
    "            })\n",
    "            \n",
    "            pbar.update(1)\n",
    "            time.sleep(0.01)  # Small delay for visualization\n",
    "\n",
    "print(f\"\\n‚úÖ Simulation complete: {len(results)} results collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Results by Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by pattern\n",
    "pattern_metrics = {}\n",
    "\n",
    "for pattern_name in pattern_simulators.keys():\n",
    "    pattern_results = [r for r in results if r[\"pattern\"] == pattern_name]\n",
    "    \n",
    "    pattern_metrics[pattern_name] = {\n",
    "        \"avg_latency\": np.mean([r[\"latency\"] for r in pattern_results]),\n",
    "        \"std_latency\": np.std([r[\"latency\"] for r in pattern_results]),\n",
    "        \"avg_cost\": np.mean([r[\"cost\"] for r in pattern_results]),\n",
    "        \"avg_quality\": np.mean([r[\"quality\"] for r in pattern_results]),\n",
    "        \"avg_agents\": np.mean([r[\"agent_count\"] for r in pattern_results]),\n",
    "    }\n",
    "\n",
    "# Display aggregated metrics\n",
    "print(\"\\nüìä Pattern Performance Summary (Simulated)\\n\")\n",
    "print(f\"{'Pattern':<20} {'Latency (s)':<18} {'Cost ($)':<15} {'Quality':<15} {'Avg Agents'}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for pattern_name, metrics in pattern_metrics.items():\n",
    "    print(\n",
    "        f\"{pattern_name.upper():<20} \"\n",
    "        f\"{metrics['avg_latency']:<6.2f} ¬±{metrics['std_latency']:<9.2f} \"\n",
    "        f\"{metrics['avg_cost']:<15.4f} \"\n",
    "        f\"{metrics['avg_quality']:<15.2f} \"\n",
    "        f\"{metrics['avg_agents']:.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Radar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart\n",
    "def create_radar_chart(pattern_metrics: dict[str, dict[str, float]]) -> None:\n",
    "    \"\"\"Create radar chart comparing patterns across metrics.\"\"\"\n",
    "    # Prepare data\n",
    "    metrics_names = [\"Speed\\n(1/latency)\", \"Efficiency\\n(1/cost)\", \"Quality\"]\n",
    "    \n",
    "    # Normalize metrics to 0-1 scale (higher is better)\n",
    "    max_latency = max(m[\"avg_latency\"] for m in pattern_metrics.values())\n",
    "    max_cost = max(m[\"avg_cost\"] for m in pattern_metrics.values())\n",
    "    \n",
    "    values_by_pattern = {}\n",
    "    for pattern_name, metrics in pattern_metrics.items():\n",
    "        # Invert latency and cost (lower is better ‚Üí higher score)\n",
    "        speed = 1 - (metrics[\"avg_latency\"] / max_latency)\n",
    "        efficiency = 1 - (metrics[\"avg_cost\"] / max_cost)\n",
    "        quality = metrics[\"avg_quality\"]\n",
    "        \n",
    "        values_by_pattern[pattern_name] = [speed, efficiency, quality]\n",
    "    \n",
    "    # Setup radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 9), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each pattern\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "    for (pattern_name, values), color in zip(values_by_pattern.items(), colors):\n",
    "        values += values[:1]  # Complete the circle\n",
    "        ax.plot(angles, values, 'o-', linewidth=2.5, label=pattern_name.upper(), color=color, markersize=8)\n",
    "        ax.fill(angles, values, alpha=0.2, color=color)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_names, size=11, weight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=9)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7, linewidth=1.2)\n",
    "    \n",
    "    # Title and legend\n",
    "    plt.title('Multi-Agent Pattern Comparison (Simulated)\\nHigher values = Better performance', \n",
    "              size=16, weight='bold', pad=25)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.35, 1.15), fontsize=11, framealpha=0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_radar_chart(pattern_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pattern recommendations\n",
    "fastest = min(pattern_metrics.items(), key=lambda x: x[1][\"avg_latency\"])[0]\n",
    "cheapest = min(pattern_metrics.items(), key=lambda x: x[1][\"avg_cost\"])[0]\n",
    "highest_quality = max(pattern_metrics.items(), key=lambda x: x[1][\"avg_quality\"])[0]\n",
    "\n",
    "recommendations = {\n",
    "    \"hierarchical\": {\n",
    "        \"description\": \"Complex tasks with clear subtask decomposition. Trade higher latency for better quality and coordination.\",\n",
    "        \"use_when\": [\"Task can be broken into independent subtasks\", \"Need specialist expertise\", \"Quality matters more than speed\"],\n",
    "        \"avoid_when\": [\"Real-time response required\", \"Simple tasks\", \"Limited budget\"],\n",
    "    },\n",
    "    \"diamond\": {\n",
    "        \"description\": \"Optimization problems where you need the best of multiple approaches. Higher cost due to parallel execution.\",\n",
    "        \"use_when\": [\"Need highest quality output\", \"Multiple valid approaches exist\", \"Budget allows parallel calls\"],\n",
    "        \"avoid_when\": [\"Cost-sensitive application\", \"Single obvious solution\", \"Time is critical\"],\n",
    "    },\n",
    "    \"p2p\": {\n",
    "        \"description\": \"Sequential pipelines with clear handoff points. Lower coordination overhead but context transfer risk.\",\n",
    "        \"use_when\": [\"Clear processing pipeline\", \"Each step depends on previous\", \"Cost optimization important\"],\n",
    "        \"avoid_when\": [\"Complex context needed throughout\", \"Steps can be parallelized\", \"High accuracy required\"],\n",
    "    },\n",
    "    \"collaborative\": {\n",
    "        \"description\": \"Creative tasks requiring multiple perspectives. Good balance of quality and agent utilization.\",\n",
    "        \"use_when\": [\"Creative or brainstorming tasks\", \"Multiple viewpoints add value\", \"Moderate complexity\"],\n",
    "        \"avoid_when\": [\"Single correct answer\", \"Coordination overhead too high\", \"Simple factual queries\"],\n",
    "    },\n",
    "    \"adaptive_loop\": {\n",
    "        \"description\": \"Quality-critical tasks where iterative refinement is acceptable. Variable latency based on convergence.\",\n",
    "        \"use_when\": [\"Quality threshold must be met\", \"Iterative improvement possible\", \"Time is flexible\"],\n",
    "        \"avoid_when\": [\"Fixed latency requirement\", \"First attempt usually sufficient\", \"High cost sensitivity\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nüéØ Pattern Selection Guide\\n\")\n",
    "print(\"=\" * 90)\n",
    "for pattern_name, rec in recommendations.items():\n",
    "    badges = []\n",
    "    if pattern_name == fastest:\n",
    "        badges.append(\"‚ö° FASTEST\")\n",
    "    if pattern_name == cheapest:\n",
    "        badges.append(\"üí∞ CHEAPEST\")\n",
    "    if pattern_name == highest_quality:\n",
    "        badges.append(\"üèÜ HIGHEST QUALITY\")\n",
    "    \n",
    "    badge_str = \" \".join(badges)\n",
    "    \n",
    "    print(f\"\\n{pattern_name.upper()}: {badge_str}\")\n",
    "    print(f\"{rec['description']}\")\n",
    "    print(f\"\\n‚úÖ Use when:\")\n",
    "    for point in rec[\"use_when\"]:\n",
    "        print(f\"   - {point}\")\n",
    "    print(f\"\\n‚ùå Avoid when:\")\n",
    "    for point in rec[\"avoid_when\"]:\n",
    "        print(f\"   - {point}\")\n",
    "    print(\"-\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results JSON for dashboard integration\n",
    "output_data = {\n",
    "    \"experiment_metadata\": {\n",
    "        \"mode\": MODE,\n",
    "        \"num_scenarios\": len(scenarios),\n",
    "        \"num_patterns\": len(pattern_simulators),\n",
    "        \"total_executions\": len(results),\n",
    "        \"simulation_type\": \"theoretical\",\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    },\n",
    "    \"pattern_metrics\": {\n",
    "        name: {\n",
    "            \"avg_latency\": float(m[\"avg_latency\"]),\n",
    "            \"avg_cost\": float(m[\"avg_cost\"]),\n",
    "            \"avg_quality\": float(m[\"avg_quality\"]),\n",
    "            \"avg_agents\": float(m[\"avg_agents\"]),\n",
    "        }\n",
    "        for name, m in pattern_metrics.items()\n",
    "    },\n",
    "    \"detailed_results\": results,\n",
    "    \"pattern_rankings\": {\n",
    "        \"fastest\": fastest,\n",
    "        \"cheapest\": cheapest,\n",
    "        \"highest_quality\": highest_quality,\n",
    "    },\n",
    "    \"recommendations\": recommendations,\n",
    "}\n",
    "\n",
    "# Save to results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "output_path = results_dir / \"multi_agent_pattern_comparison.json\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_path}\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  - Fastest Pattern: {fastest.upper()}\")\n",
    "print(f\"  - Cheapest Pattern: {cheapest.upper()}\")\n",
    "print(f\"  - Highest Quality: {highest_quality.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Pattern Trade-offs (Based on Simulation)\n",
    "\n",
    "1. **Hierarchical Pattern** (Manager-Worker)\n",
    "   - ‚úÖ Best for: Complex tasks with clear decomposition\n",
    "   - ‚ö†Ô∏è Trade-off: Higher latency due to sequential orchestration  \n",
    "   - üí° Use when: Quality and coordination matter more than speed\n",
    "   - üîß Real implementation: `HierarchicalAgent` in `backend/multi_agent_patterns.py:156`\n",
    "\n",
    "2. **Diamond Pattern** (Competitive)\n",
    "   - ‚úÖ Best for: Finding optimal solution among alternatives\n",
    "   - ‚ö†Ô∏è Trade-off: Higher cost due to parallel LLM calls\n",
    "   - üí° Use when: Budget allows and you need best-of-N selection\n",
    "   - üîß Real implementation: `DiamondAgent` in `backend/multi_agent_patterns.py:336`\n",
    "\n",
    "3. **P2P Pattern** (Peer-to-Peer Handoff)\n",
    "   - ‚úÖ Best for: Sequential pipelines with specialized agents\n",
    "   - ‚ö†Ô∏è Trade-off: Context loss risk during handoffs\n",
    "   - üí° Use when: Clear pipeline stages with minimal context dependency\n",
    "   - üîß Real implementation: `P2PAgent` in `backend/multi_agent_patterns.py:633`\n",
    "\n",
    "4. **Collaborative Pattern** (Shared Workspace)\n",
    "   - ‚úÖ Best for: Creative tasks requiring multiple perspectives\n",
    "   - ‚ö†Ô∏è Trade-off: Coordination overhead for merging contributions\n",
    "   - üí° Use when: Diverse viewpoints improve output quality\n",
    "   - üîß Real implementation: `CollaborativeAgent` in `backend/multi_agent_patterns.py:838`\n",
    "\n",
    "5. **Adaptive Loop Pattern** (Iterative Refinement)\n",
    "   - ‚úÖ Best for: Quality-critical tasks with refinement\n",
    "   - ‚ö†Ô∏è Trade-off: Variable latency (depends on convergence)\n",
    "   - üí° Use when: Quality threshold must be met, time is flexible\n",
    "   - üîß Real implementation: `AdaptiveLoopAgent` in `backend/multi_agent_patterns.py:1033`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Study real implementations**: Read `backend/multi_agent_patterns.py` for production code\n",
    "2. **Try different scenarios**: Edit `data/multi_agent_scenarios.json` to test your own tasks\n",
    "3. **Explore automotive case study**: See `automotive_ai_case_study.ipynb` for real-world application\n",
    "4. **Read concept tutorials**:\n",
    "   - `multi_agent_fundamentals.md` - 11 core components\n",
    "   - `multi_agent_design_patterns.md` - 5 coordination patterns in depth\n",
    "   - `multi_agent_challenges_evaluation.md` - 6 challenges and evaluation\n",
    "\n",
    "### Important Note\n",
    "\n",
    "This notebook uses **simulated execution** to demonstrate pattern concepts without API costs. For production use:\n",
    "- Use the real implementations in `backend/multi_agent_patterns.py`\n",
    "- Run unit tests: `pytest tests/test_multi_agent_patterns.py`\n",
    "- See `tests/test_multi_agent_patterns.py` for usage examples with real LLM calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
