```mermaid
%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#E8F4F8'}}}%%
graph TB
    subgraph "Agent Observability Architecture"
        Agent[AI Agent System] -->|Executes| Query[User Query]

        subgraph AgentComponents["Agent Components"]
            AC1[Query Classifier]
            AC2[Retrieval Agent]
            AC3[Tool Executor]
            AC4[LLM Synthesis]
            AC5[Validator Agent]
        end

        Agent --> AgentComponents

        subgraph Telemetry["Telemetry Collection Layer"]
            Tel1[OpenTelemetry Spans]
            Tel2[Structured Logs]
            Tel3[Metrics Emitter]
            Tel4[Trace Context Propagation]
        end

        AgentComponents -->|Instrument| Telemetry

        Telemetry -->|High-Level Metrics| Dashboard
        Telemetry -->|Detailed Traces| TraceStorage

        subgraph Dashboard["High-Level Observability Dashboards"]
            D1["üìä Business Metrics<br/>‚Ä¢ Revenue Impact<br/>‚Ä¢ User Engagement DAU/MAU<br/>‚Ä¢ Conversion Rates"]
            D2["üéØ Goal-Level Metrics<br/>‚Ä¢ Goal Completion Rate<br/>‚Ä¢ Critical Task Success<br/>‚Ä¢ User Journey Completion"]
            D3["‚ö° Application Telemetry<br/>‚Ä¢ P50/P95/P99 Latency<br/>‚Ä¢ Error Rate by Type<br/>‚Ä¢ Throughput RPS"]
            D4["üë§ Human Feedback<br/>‚Ä¢ Thumbs Up/Down Rate<br/>‚Ä¢ NPS Score<br/>‚Ä¢ User Satisfaction CSAT"]
        end

        Dashboard --> D1
        Dashboard --> D2
        Dashboard --> D3
        Dashboard --> D4

        subgraph TraceStorage["Detailed Trace Storage"]
            TS1[Trace Database]
            TS2[Log Aggregation]
            TS3[Span Analytics]
        end

        TraceStorage --> TS1
        TraceStorage --> TS2
        TraceStorage --> TS3

        subgraph AlertingMonitoring["Alerting & Monitoring"]
            AM1["üö® Real-Time Alerts<br/>‚Ä¢ Goal completion rate drop<br/>‚Ä¢ Latency spike P99 > threshold<br/>‚Ä¢ Error rate increase"]
            AM2["üìà Trend Analysis<br/>‚Ä¢ Week-over-week performance<br/>‚Ä¢ Model drift detection<br/>‚Ä¢ User feedback trends"]
            AM3["üîç Anomaly Detection<br/>‚Ä¢ Unexpected tool call patterns<br/>‚Ä¢ Novel failure modes<br/>‚Ä¢ Performance degradation"]
        end

        Dashboard --> AlertingMonitoring
        TraceStorage --> AlertingMonitoring

        AlertingMonitoring --> AM1
        AlertingMonitoring --> AM2
        AlertingMonitoring --> AM3

        AM1 -->|Trigger| Incident[Incident Response]
        AM2 -->|Identify| Analysis[Root Cause Analysis]
        AM3 -->|Flag| Investigation[Deep Dive Investigation]

        Incident --> DrillDown[Drill Down to Traces]
        Analysis --> DrillDown
        Investigation --> DrillDown

        DrillDown --> TraceStorage

        subgraph TraceDetail["Detailed Trace View"]
            TD1["üî¨ Single Query Trace<br/>‚Ä¢ Timestamp: 2025-01-15 10:23:45<br/>‚Ä¢ User Query: 'Book flight to Paris'<br/>‚Ä¢ Query Classifier: 0.5s ‚Üí 'flight_booking'<br/>‚Ä¢ Flight API: 1.2s ‚Üí 200 OK<br/>‚Ä¢ LLM Synthesis: 0.8s ‚Üí 500 tokens<br/>‚Ä¢ Total Latency: 5.0s<br/>‚Ä¢ Bottleneck: Flight API"]
            TD2["üìù Contextual Information<br/>‚Ä¢ User ID: user_12345<br/>‚Ä¢ Session ID: sess_abc123<br/>‚Ä¢ Agent Version: v2.3.1<br/>‚Ä¢ LLM Model: claude-3-5-sonnet<br/>‚Ä¢ Environment: production-us-west"]
            TD3["‚ùå Error Details<br/>‚Ä¢ Tool: check_availability<br/>‚Ä¢ Status: 500 Internal Server Error<br/>‚Ä¢ Retry Attempt: 1/3<br/>‚Ä¢ Fallback: Use cached results"]
        end

        DrillDown --> TraceDetail
        TraceDetail --> TD1
        TraceDetail --> TD2
        TraceDetail --> TD3

        subgraph FeedbackLoop["Feedback Loop"]
            FB1[Identify Issue in Traces]
            FB2{Root Cause?}
            FB3[Fix Prompt/Tool/Logic]
            FB4[Deploy Update]
            FB5[Verify Fix in Dashboard]
        end

        TD1 --> FeedbackLoop
        TD3 --> FeedbackLoop
        FeedbackLoop --> FB1
        FB1 --> FB2
        FB2 -->|Prompt Error| FB3
        FB2 -->|Tool Failure| FB3
        FB2 -->|Logic Bug| FB3
        FB3 --> FB4
        FB4 --> FB5
        FB5 --> Dashboard
    end

    style Agent fill:#FFE6CC,stroke:#FF8C00,stroke-width:3px
    style Dashboard fill:#90EE90,stroke:#228B22,stroke-width:2px
    style TraceStorage fill:#E6F3FF,stroke:#4A90E2,stroke-width:2px
    style AlertingMonitoring fill:#FFE6E6,stroke:#DC143C,stroke-width:2px
    style DrillDown fill:#FFF4E6,stroke:#FFA500,stroke-width:2px
    style TraceDetail fill:#F0E6FF,stroke:#9370DB,stroke-width:2px
    style FeedbackLoop fill:#E6FFE6,stroke:#20B2AA,stroke-width:2px
    style D1 fill:#FFF9E6
    style D2 fill:#E6F9FF
    style D3 fill:#FFE6F9
    style D4 fill:#F0FFE6
```

**Diagram Purpose:** Illustrate the two-tier agent observability architecture combining high-level KPI dashboards with detailed trace debugging.

**Architecture Layers:**

**1. Agent Components (Top):**
- Query Classifier, Retrieval Agent, Tool Executor, LLM Synthesis, Validator Agent
- Each component is instrumented with telemetry

**2. Telemetry Collection Layer:**
- **OpenTelemetry Spans**: Distributed tracing across agent components
- **Structured Logs**: JSON-formatted logs with context
- **Metrics Emitter**: Real-time metric publishing
- **Trace Context Propagation**: Maintains context across async operations

**3. High-Level Observability Dashboards:**
- **üìä Business Metrics**: Revenue, DAU, conversion rates
- **üéØ Goal-Level Metrics**: Goal completion rate, task success
- **‚ö° Application Telemetry**: Latency (P50/P95/P99), error rates, throughput
- **üë§ Human Feedback**: Thumbs up/down, NPS, CSAT

**4. Detailed Trace Storage:**
- Trace Database (e.g., Jaeger, Tempo)
- Log Aggregation (e.g., Elasticsearch, Loki)
- Span Analytics (query and analyze traces)

**5. Alerting & Monitoring:**
- **üö® Real-Time Alerts**: Goal completion drop, latency spike, error rate increase
- **üìà Trend Analysis**: Week-over-week performance, drift detection
- **üîç Anomaly Detection**: Unexpected patterns, novel failures

**6. Incident Response Workflow:**
- Dashboard alerts trigger incident response
- Root cause analysis identifies patterns
- Drill down to specific trace details
- View full context (timestamps, user info, errors)
- Fix issue (prompt/tool/logic)
- Deploy update and verify fix

**Key Principles:**
1. **High-Level for Trends**: Dashboards show aggregate metrics (always-on monitoring)
2. **Detailed for Debugging**: Traces provide granular context (drill-down when issues arise)
3. **Bidirectional Flow**: Alerts trigger trace investigation ‚Üí fixes ‚Üí dashboard validation

**Example Use Case:**
- Dashboard shows: "Goal completion rate dropped from 87% to 72% in last hour"
- Alert triggers investigation
- Drill down to traces reveals: "Flight API returning 500 errors"
- Fix: Add retry logic with exponential backoff
- Verify: Dashboard shows goal completion rate recovered to 85%

**Referenced In:**
- `agent_evaluation_fundamentals.md` (Section 4, Lines 250-370)
- `human_in_the_loop_evaluation.md` (Section 5, Lines 735-756)

**Rendering Notes:**
- Flow from top (agent) to bottom (feedback loop)
- Color-coded stages (green = healthy, red = alerts, purple = detailed analysis)
- Emoji icons for visual scanning
- Compatible with GitHub Markdown rendering
