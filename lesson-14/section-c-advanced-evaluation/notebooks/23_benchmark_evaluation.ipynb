{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Benchmark Evaluation for Agent Systems\n",
    "\n",
    "This notebook evaluates agent performance on public benchmarks:\n",
    "- **BFCL-style**: Tool calling accuracy (Berkeley Function-Calling Leaderboard)\n",
    "- **PlanBench-style**: Planning quality and multi-step reasoning\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand public benchmark evaluation methodologies\n",
    "2. Measure tool calling accuracy and argument validation\n",
    "3. Evaluate planning quality with trajectory metrics\n",
    "4. Compare agent performance across benchmark types\n",
    "\n",
    "## Execution Modes\n",
    "- **DEMO**: 10 examples (5 BFCL + 5 PlanBench), ~2-3 minutes, <$1 cost\n",
    "- **FULL**: 50 examples (25 BFCL + 25 PlanBench), ~8-10 minutes, <$3 cost\n",
    "\n",
    "## Prerequisites\n",
    "- OpenAI API key configured\n",
    "- Completed trajectory evaluation tutorial\n",
    "- Understanding of tool calling and planning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add backend to path\n",
    "backend_path = Path(\".\").resolve().parent / \"backend\"\n",
    "if str(backend_path) not in sys.path:\n",
    "    sys.path.insert(0, str(backend_path))\n",
    "\n",
    "# Import trajectory evaluation\n",
    "from trajectory_evaluation import TrajectoryEvaluator, TrajectoryVisualizer\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"Current directory: {Path.cwd()}\")\n",
    "print(f\"Backend path: {backend_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Mode Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODE SELECTION\n",
    "# ============================================\n",
    "# Change MODE to \"FULL\" for comprehensive evaluation\n",
    "MODE = \"DEMO\"  # Options: \"DEMO\" or \"FULL\"\n",
    "\n",
    "# Configure based on mode\n",
    "if MODE == \"DEMO\":\n",
    "    N_BFCL = 5  # BFCL-style tool calling examples\n",
    "    N_PLANBENCH = 5  # PlanBench-style planning examples\n",
    "    MODEL = \"gpt-3.5-turbo\"  # Cheaper model\n",
    "    ESTIMATED_COST = \"$0.50-$1.00\"\n",
    "    ESTIMATED_TIME = \"2-3 minutes\"\n",
    "elif MODE == \"FULL\":\n",
    "    N_BFCL = 25\n",
    "    N_PLANBENCH = 25\n",
    "    MODEL = \"gpt-4o-mini\"  # Better quality\n",
    "    ESTIMATED_COST = \"$2.00-$3.00\"\n",
    "    ESTIMATED_TIME = \"8-10 minutes\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid MODE: {MODE}. Must be 'DEMO' or 'FULL'\")\n",
    "\n",
    "# API Configuration\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found. Set it with: export OPENAI_API_KEY='your-key'\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Display configuration\n",
    "print(f\"üîß MODE: {MODE}\")\n",
    "print(f\"üìä Samples: {N_BFCL} BFCL + {N_PLANBENCH} PlanBench = {N_BFCL + N_PLANBENCH} total\")\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üí∞ Estimated cost: {ESTIMATED_COST}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {ESTIMATED_TIME}\")\n",
    "print(\"\\n‚ö†Ô∏è  WARNING: This will make API calls and incur costs.\")\n",
    "print(\"Continue only if you accept these charges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_data(benchmark_type: str, n_samples: int) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load benchmark data with stratified sampling.\n",
    "\n",
    "    Args:\n",
    "        benchmark_type: \"bfcl\" or \"planbench\"\n",
    "        n_samples: Number of samples to load\n",
    "\n",
    "    Returns:\n",
    "        List of benchmark test cases\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If inputs are invalid types\n",
    "        ValueError: If benchmark_type is invalid or data loading fails\n",
    "    \"\"\"\n",
    "    # Step 1: Type checking\n",
    "    if not isinstance(benchmark_type, str):\n",
    "        raise TypeError(\"benchmark_type must be a string\")\n",
    "    if not isinstance(n_samples, int):\n",
    "        raise TypeError(\"n_samples must be an int\")\n",
    "\n",
    "    # Step 2: Input validation\n",
    "    if benchmark_type not in [\"bfcl\", \"planbench\"]:\n",
    "        raise ValueError(\"benchmark_type must be 'bfcl' or 'planbench'\")\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(\"n_samples must be positive\")\n",
    "\n",
    "    # Step 3: Load data file\n",
    "    data_dir = Path(\"data\")\n",
    "    if benchmark_type == \"bfcl\":\n",
    "        filepath = data_dir / \"agent_tool_call_benchmark.json\"\n",
    "    else:\n",
    "        filepath = data_dir / \"agent_planning_benchmark.json\"\n",
    "\n",
    "    if not filepath.exists():\n",
    "        raise ValueError(f\"Benchmark file not found: {filepath}\")\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    test_cases = data.get(\"test_cases\", [])\n",
    "    if not test_cases:\n",
    "        raise ValueError(f\"No test cases found in {filepath}\")\n",
    "\n",
    "    # Step 4: Stratified sampling by difficulty\n",
    "    # Group by difficulty\n",
    "    by_difficulty = defaultdict(list)\n",
    "    for case in test_cases:\n",
    "        difficulty = case.get(\"difficulty\", \"unknown\")\n",
    "        by_difficulty[difficulty].append(case)\n",
    "\n",
    "    # Calculate samples per difficulty (proportional)\n",
    "    total_cases = len(test_cases)\n",
    "    samples = []\n",
    "    for difficulty, cases in by_difficulty.items():\n",
    "        proportion = len(cases) / total_cases\n",
    "        n_difficulty = max(1, int(n_samples * proportion))\n",
    "        samples.extend(cases[:n_difficulty])\n",
    "\n",
    "    # Step 5: Ensure we have exactly n_samples\n",
    "    if len(samples) > n_samples:\n",
    "        samples = samples[:n_samples]\n",
    "    elif len(samples) < n_samples:\n",
    "        # Add more from remaining cases\n",
    "        remaining = [c for c in test_cases if c not in samples]\n",
    "        samples.extend(remaining[: n_samples - len(samples)])\n",
    "\n",
    "    return samples[:n_samples]\n",
    "\n",
    "\n",
    "# Load benchmark data\n",
    "print(\"üì• Loading benchmark data...\")\n",
    "bfcl_data = load_benchmark_data(\"bfcl\", N_BFCL)\n",
    "planbench_data = load_benchmark_data(\"planbench\", N_PLANBENCH)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(bfcl_data)} BFCL test cases\")\n",
    "print(f\"‚úÖ Loaded {len(planbench_data)} PlanBench test cases\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìã BFCL Sample:\")\n",
    "print(json.dumps(bfcl_data[0], indent=2))\n",
    "print(\"\\nüìã PlanBench Sample:\")\n",
    "print(json.dumps(planbench_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BFCL Evaluation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bfcl_case(\n",
    "    test_case: dict[str, Any], client: OpenAI, model: str\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Evaluate a single BFCL tool calling test case.\n",
    "\n",
    "    Args:\n",
    "        test_case: BFCL benchmark test case\n",
    "        client: OpenAI client\n",
    "        model: Model name\n",
    "\n",
    "    Returns:\n",
    "        Evaluation results with accuracy scores\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If inputs are invalid types\n",
    "        ValueError: If test case is missing required fields\n",
    "    \"\"\"\n",
    "    # Step 1: Type checking\n",
    "    if not isinstance(test_case, dict):\n",
    "        raise TypeError(\"test_case must be a dict\")\n",
    "    if not isinstance(model, str):\n",
    "        raise TypeError(\"model must be a string\")\n",
    "\n",
    "    # Step 2: Extract test case data\n",
    "    task = test_case.get(\"task\")\n",
    "    expected_tool = test_case.get(\"tool_call\", {}).get(\"tool\")\n",
    "    expected_args = test_case.get(\"tool_call\", {}).get(\"args\", {})\n",
    "    labels = test_case.get(\"labels\", {})\n",
    "\n",
    "    if not task or not expected_tool:\n",
    "        raise ValueError(\"Test case missing required fields: task or tool_call\")\n",
    "\n",
    "    # Step 3: Generate agent response with tool calling\n",
    "    try:\n",
    "        # Define available tools (simplified recipe domain)\n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"search_recipes\",\n",
    "                    \"description\": \"Search for recipes based on criteria\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"ingredients\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                                \"description\": \"Ingredient filters\",\n",
    "                            },\n",
    "                            \"dietary_restrictions\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                                \"description\": \"Dietary restrictions (vegan, gluten-free, etc.)\",\n",
    "                            },\n",
    "                            \"max_cook_time\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Maximum cooking time in minutes\",\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": task}],\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Extract tool call\n",
    "        message = response.choices[0].message\n",
    "        tool_calls = message.tool_calls if hasattr(message, \"tool_calls\") else None\n",
    "\n",
    "        if not tool_calls or len(tool_calls) == 0:\n",
    "            # No tool call made\n",
    "            return {\n",
    "                \"id\": test_case.get(\"id\"),\n",
    "                \"task\": task,\n",
    "                \"tool_selection_correct\": False,\n",
    "                \"args_correct\": False,\n",
    "                \"overall_correct\": False,\n",
    "                \"error\": \"No tool call made\",\n",
    "                \"expected\": {\"tool\": expected_tool, \"args\": expected_args},\n",
    "                \"predicted\": None,\n",
    "            }\n",
    "\n",
    "        # Parse tool call\n",
    "        tool_call = tool_calls[0]\n",
    "        predicted_tool = tool_call.function.name\n",
    "        predicted_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # Step 4: Evaluate accuracy\n",
    "        tool_correct = predicted_tool == expected_tool\n",
    "        args_correct = predicted_args == expected_args\n",
    "        overall_correct = tool_correct and args_correct\n",
    "\n",
    "        # Step 5: Return results\n",
    "        return {\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"task\": task,\n",
    "            \"tool_selection_correct\": tool_correct,\n",
    "            \"args_correct\": args_correct,\n",
    "            \"overall_correct\": overall_correct,\n",
    "            \"expected\": {\"tool\": expected_tool, \"args\": expected_args},\n",
    "            \"predicted\": {\"tool\": predicted_tool, \"args\": predicted_args},\n",
    "            \"difficulty\": test_case.get(\"difficulty\"),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle API errors gracefully\n",
    "        return {\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"task\": task,\n",
    "            \"tool_selection_correct\": False,\n",
    "            \"args_correct\": False,\n",
    "            \"overall_correct\": False,\n",
    "            \"error\": str(e),\n",
    "            \"expected\": {\"tool\": expected_tool, \"args\": expected_args},\n",
    "            \"predicted\": None,\n",
    "        }\n",
    "\n",
    "\n",
    "# Run BFCL evaluation\n",
    "print(\"\\nüîß Running BFCL evaluation...\")\n",
    "bfcl_results = []\n",
    "for case in tqdm(bfcl_data, desc=\"BFCL\"):\n",
    "    result = evaluate_bfcl_case(case, client, MODEL)\n",
    "    bfcl_results.append(result)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "n_tool_correct = sum(r[\"tool_selection_correct\"] for r in bfcl_results)\n",
    "n_args_correct = sum(r[\"args_correct\"] for r in bfcl_results)\n",
    "n_overall_correct = sum(r[\"overall_correct\"] for r in bfcl_results)\n",
    "n_total = len(bfcl_results)\n",
    "\n",
    "print(f\"\\nüìä BFCL Results:\")\n",
    "print(f\"  Tool Selection Accuracy: {n_tool_correct}/{n_total} ({100*n_tool_correct/n_total:.1f}%)\")\n",
    "print(f\"  Args Validation Accuracy: {n_args_correct}/{n_total} ({100*n_args_correct/n_total:.1f}%)\")\n",
    "print(f\"  Overall Accuracy: {n_overall_correct}/{n_total} ({100*n_overall_correct/n_total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PlanBench Evaluation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_planbench_case(\n",
    "    test_case: dict[str, Any], client: OpenAI, model: str, evaluator: TrajectoryEvaluator\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Evaluate a single PlanBench planning test case.\n",
    "\n",
    "    Args:\n",
    "        test_case: PlanBench test case\n",
    "        client: OpenAI client\n",
    "        model: Model name\n",
    "        evaluator: TrajectoryEvaluator for metric calculation\n",
    "\n",
    "    Returns:\n",
    "        Evaluation results with trajectory metrics\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If inputs are invalid types\n",
    "        ValueError: If test case is missing required fields\n",
    "    \"\"\"\n",
    "    # Step 1: Type checking\n",
    "    if not isinstance(test_case, dict):\n",
    "        raise TypeError(\"test_case must be a dict\")\n",
    "    if not isinstance(model, str):\n",
    "        raise TypeError(\"model must be a string\")\n",
    "\n",
    "    # Step 2: Extract test case data\n",
    "    task = test_case.get(\"task\")\n",
    "    goal = test_case.get(\"goal\")\n",
    "    gold_plan = test_case.get(\"gold_plan\", {})\n",
    "    gold_steps = gold_plan.get(\"steps\", [])\n",
    "\n",
    "    if not task or not gold_steps:\n",
    "        raise ValueError(\"Test case missing required fields: task or gold_plan.steps\")\n",
    "\n",
    "    # Convert gold plan to trajectory (list of tool names)\n",
    "    reference_trajectory = [step[\"tool\"] for step in gold_steps]\n",
    "\n",
    "    # Step 3: Generate agent plan\n",
    "    try:\n",
    "        prompt = f\"\"\"Task: {task}\n",
    "Goal: {goal}\n",
    "\n",
    "Generate a step-by-step plan to accomplish this goal. For each step, specify:\n",
    "1. The tool to use (search_recipes, filter_results, etc.)\n",
    "2. The arguments for that tool\n",
    "3. The rationale for this step\n",
    "\n",
    "Format your response as a JSON array of steps:\n",
    "[{{\"tool\": \"tool_name\", \"args\": {{...}}, \"rationale\": \"...\"}}]\n",
    "\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.0\n",
    "        )\n",
    "\n",
    "        # Parse response\n",
    "        content = response.choices[0].message.content\n",
    "\n",
    "        # Extract JSON (handle markdown code blocks)\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        predicted_plan = json.loads(content)\n",
    "        predicted_trajectory = [step[\"tool\"] for step in predicted_plan]\n",
    "\n",
    "        # Step 4: Calculate trajectory metrics\n",
    "        metrics = {\n",
    "            \"exact_match\": evaluator.exact_match(reference_trajectory, predicted_trajectory),\n",
    "            \"in_order_match\": evaluator.in_order_match(\n",
    "                reference_trajectory, predicted_trajectory\n",
    "            ),\n",
    "            \"any_order_match\": evaluator.any_order_match(\n",
    "                reference_trajectory, predicted_trajectory\n",
    "            ),\n",
    "            \"precision\": evaluator.precision(reference_trajectory, predicted_trajectory),\n",
    "            \"recall\": evaluator.recall(reference_trajectory, predicted_trajectory),\n",
    "            \"single_tool_use\": evaluator.single_tool_use(\n",
    "                reference_trajectory, predicted_trajectory\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Step 5: Return results\n",
    "        return {\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"task\": task,\n",
    "            \"goal\": goal,\n",
    "            \"reference_trajectory\": reference_trajectory,\n",
    "            \"predicted_trajectory\": predicted_trajectory,\n",
    "            \"metrics\": metrics,\n",
    "            \"difficulty\": test_case.get(\"difficulty\"),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        return {\n",
    "            \"id\": test_case.get(\"id\"),\n",
    "            \"task\": task,\n",
    "            \"goal\": goal,\n",
    "            \"reference_trajectory\": reference_trajectory,\n",
    "            \"predicted_trajectory\": [],\n",
    "            \"metrics\": {\n",
    "                \"exact_match\": 0.0,\n",
    "                \"in_order_match\": 0.0,\n",
    "                \"any_order_match\": 0.0,\n",
    "                \"precision\": 0.0,\n",
    "                \"recall\": 0.0,\n",
    "                \"single_tool_use\": 0.0,\n",
    "            },\n",
    "            \"error\": str(e),\n",
    "            \"difficulty\": test_case.get(\"difficulty\"),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = TrajectoryEvaluator()\n",
    "\n",
    "# Run PlanBench evaluation\n",
    "print(\"\\nüìù Running PlanBench evaluation...\")\n",
    "planbench_results = []\n",
    "for case in tqdm(planbench_data, desc=\"PlanBench\"):\n",
    "    result = evaluate_planbench_case(case, client, MODEL, evaluator)\n",
    "    planbench_results.append(result)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "avg_metrics = defaultdict(float)\n",
    "for result in planbench_results:\n",
    "    for metric, value in result[\"metrics\"].items():\n",
    "        avg_metrics[metric] += value\n",
    "\n",
    "n_planbench = len(planbench_results)\n",
    "for metric in avg_metrics:\n",
    "    avg_metrics[metric] /= n_planbench\n",
    "\n",
    "print(f\"\\nüìä PlanBench Results (Average Trajectory Metrics):\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "results_summary = {\n",
    "    \"mode\": MODE,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL,\n",
    "    \"n_samples\": {\"bfcl\": N_BFCL, \"planbench\": N_PLANBENCH, \"total\": N_BFCL + N_PLANBENCH},\n",
    "    \"bfcl_results\": {\n",
    "        \"tool_selection_accuracy\": n_tool_correct / n_total,\n",
    "        \"args_validation_accuracy\": n_args_correct / n_total,\n",
    "        \"overall_accuracy\": n_overall_correct / n_total,\n",
    "        \"details\": bfcl_results,\n",
    "    },\n",
    "    \"planbench_results\": {\n",
    "        \"avg_trajectory_metrics\": dict(avg_metrics),\n",
    "        \"details\": planbench_results,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Benchmark\": [\"BFCL (Tool Calling)\", \"PlanBench (Planning)\"],\n",
    "        \"N_Samples\": [N_BFCL, N_PLANBENCH],\n",
    "        \"Primary Metric\": [\n",
    "            f\"{100*n_overall_correct/n_total:.1f}% Overall Accuracy\",\n",
    "            f\"{100*avg_metrics['any_order_match']:.1f}% Any-Order Match\",\n",
    "        ],\n",
    "        \"Tool Selection\": [\n",
    "            f\"{100*n_tool_correct/n_total:.1f}%\",\n",
    "            f\"{100*avg_metrics['precision']:.1f}% Precision\",\n",
    "        ],\n",
    "        \"Efficiency\": [\n",
    "            f\"{100*n_args_correct/n_total:.1f}% Args\",\n",
    "            f\"{100*avg_metrics['single_tool_use']:.1f}% Single-Tool\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä BENCHMARK EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 7.1 Benchmark Comparison Bar Chart\n",
    "ax1 = axes[0]\n",
    "benchmarks = [\"BFCL\\nOverall\", \"BFCL\\nTool\", \"BFCL\\nArgs\", \"PlanBench\\nAny-Order\", \"PlanBench\\nPrecision\", \"PlanBench\\nRecall\"]\n",
    "accuracies = [\n",
    "    100 * n_overall_correct / n_total,\n",
    "    100 * n_tool_correct / n_total,\n",
    "    100 * n_args_correct / n_total,\n",
    "    100 * avg_metrics[\"any_order_match\"],\n",
    "    100 * avg_metrics[\"precision\"],\n",
    "    100 * avg_metrics[\"recall\"],\n",
    "]\n",
    "colors = [\"#3498db\", \"#5dade2\", \"#85c1e9\", \"#e74c3c\", \"#ec7063\", \"#f1948a\"]\n",
    "bars = ax1.bar(benchmarks, accuracies, color=colors, alpha=0.8)\n",
    "ax1.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax1.set_title(f\"Benchmark Comparison ({MODE} Mode)\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.axhline(y=70, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"70% Threshold\")\n",
    "ax1.legend()\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 2,\n",
    "        f\"{acc:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "# 7.2 Trajectory Metrics Radar Chart\n",
    "ax2 = axes[1]\n",
    "visualizer = TrajectoryVisualizer()\n",
    "radar_data = visualizer.generate_radar_chart(avg_metrics)\n",
    "labels = [label.replace(\"_\", \"\\n\") for label in radar_data[\"labels\"]]\n",
    "values = radar_data[\"values\"]\n",
    "\n",
    "# Radar chart setup\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "values_plot = values + [values[0]]  # Close the polygon\n",
    "angles_plot = angles + [angles[0]]\n",
    "\n",
    "ax2 = plt.subplot(132, projection=\"polar\")\n",
    "ax2.plot(angles_plot, values_plot, \"o-\", linewidth=2, color=\"#e74c3c\")\n",
    "ax2.fill(angles_plot, values_plot, alpha=0.25, color=\"#e74c3c\")\n",
    "ax2.set_xticks(angles)\n",
    "ax2.set_xticklabels(labels, fontsize=9)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_yticks([0.25, 0.5, 0.75, 1.0])\n",
    "ax2.set_yticklabels([\"25%\", \"50%\", \"75%\", \"100%\"], fontsize=8)\n",
    "ax2.set_title(\"PlanBench Trajectory Metrics\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "ax2.grid(True)\n",
    "\n",
    "# 7.3 Error Type Distribution\n",
    "ax3 = axes[2]\n",
    "error_types = [\"BFCL\\nTool Error\", \"BFCL\\nArgs Error\", \"PlanBench\\nLow Recall\", \"PlanBench\\nLow Precision\"]\n",
    "error_counts = [\n",
    "    n_total - n_tool_correct,\n",
    "    n_total - n_args_correct,\n",
    "    int(n_planbench * (1 - avg_metrics[\"recall\"])),\n",
    "    int(n_planbench * (1 - avg_metrics[\"precision\"])),\n",
    "]\n",
    "colors_err = [\"#e74c3c\", \"#f39c12\", \"#9b59b6\", \"#3498db\"]\n",
    "ax3.barh(error_types, error_counts, color=colors_err, alpha=0.8)\n",
    "ax3.set_xlabel(\"Error Count\", fontsize=12)\n",
    "ax3.set_title(\"Error Type Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "ax3.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, count in enumerate(error_counts):\n",
    "    ax3.text(count + 0.2, i, str(count), va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/benchmark_evaluation_visualizations.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations saved to: results/benchmark_evaluation_visualizations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure results directory exists\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export to JSON\n",
    "output_path = results_dir / \"benchmark_results.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results exported to: {output_path}\")\n",
    "print(f\"üìÅ File size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "print(\"\\nüîç Running validation checks...\\n\")\n",
    "\n",
    "checks_passed = 0\n",
    "total_checks = 7\n",
    "\n",
    "# Check 1: Data completeness\n",
    "if len(bfcl_results) == N_BFCL and len(planbench_results) == N_PLANBENCH:\n",
    "    print(\"‚úÖ Check 1: Data completeness - All test cases evaluated\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(\n",
    "        f\"‚ùå Check 1: Data completeness - Missing results (BFCL: {len(bfcl_results)}/{N_BFCL}, PlanBench: {len(planbench_results)}/{N_PLANBENCH})\"\n",
    "    )\n",
    "\n",
    "# Check 2: BFCL accuracy threshold\n",
    "if n_overall_correct / n_total >= 0.5:  # 50% threshold for basic functionality\n",
    "    print(f\"‚úÖ Check 2: BFCL accuracy - Above 50% threshold ({100*n_overall_correct/n_total:.1f}%)\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  Check 2: BFCL accuracy - Below 50% threshold ({100*n_overall_correct/n_total:.1f}%)\"\n",
    "    )\n",
    "\n",
    "# Check 3: PlanBench any-order match\n",
    "if avg_metrics[\"any_order_match\"] >= 0.4:  # 40% threshold\n",
    "    print(\n",
    "        f\"‚úÖ Check 3: PlanBench any-order match - Above 40% threshold ({100*avg_metrics['any_order_match']:.1f}%)\"\n",
    "    )\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  Check 3: PlanBench any-order match - Below 40% threshold ({100*avg_metrics['any_order_match']:.1f}%)\"\n",
    "    )\n",
    "\n",
    "# Check 4: Results JSON schema\n",
    "required_keys = [\"mode\", \"timestamp\", \"model\", \"n_samples\", \"bfcl_results\", \"planbench_results\"]\n",
    "if all(key in results_summary for key in required_keys):\n",
    "    print(\"‚úÖ Check 4: Results JSON schema - All required fields present\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(\"‚ùå Check 4: Results JSON schema - Missing required fields\")\n",
    "\n",
    "# Check 5: No critical errors\n",
    "n_bfcl_errors = sum(1 for r in bfcl_results if \"error\" in r)\n",
    "n_planbench_errors = sum(1 for r in planbench_results if \"error\" in r)\n",
    "if n_bfcl_errors == 0 and n_planbench_errors == 0:\n",
    "    print(\"‚úÖ Check 5: Error rate - No critical errors\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  Check 5: Error rate - {n_bfcl_errors} BFCL errors, {n_planbench_errors} PlanBench errors\"\n",
    "    )\n",
    "\n",
    "# Check 6: Trajectory metrics valid range\n",
    "all_valid = all(0.0 <= v <= 1.0 for v in avg_metrics.values())\n",
    "if all_valid:\n",
    "    print(\"‚úÖ Check 6: Trajectory metrics - All values in valid range [0.0, 1.0]\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(\"‚ùå Check 6: Trajectory metrics - Some values out of range\")\n",
    "\n",
    "# Check 7: Output files exist\n",
    "json_exists = (results_dir / \"benchmark_results.json\").exists()\n",
    "viz_exists = (results_dir / \"benchmark_evaluation_visualizations.png\").exists()\n",
    "if json_exists and viz_exists:\n",
    "    print(\"‚úÖ Check 7: Output files - All files generated successfully\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"‚ùå Check 7: Output files - Missing files (JSON: {json_exists}, Viz: {viz_exists})\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Validation Summary: {checks_passed}/{total_checks} checks passed\")\n",
    "if checks_passed == total_checks:\n",
    "    print(\"‚úÖ All validation checks passed!\")\n",
    "elif checks_passed >= total_checks - 2:\n",
    "    print(\"‚ö†Ô∏è  Most checks passed with minor issues\")\n",
    "else:\n",
    "    print(\"‚ùå Multiple validation issues detected\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Recommendations\n",
    "\n",
    "### BFCL (Tool Calling) Insights:\n",
    "- **Tool Selection**: Measures if agent picks the right tool for the task\n",
    "- **Args Validation**: Checks if tool arguments are correct and complete\n",
    "- **Common Failures**:\n",
    "  - Missing required arguments\n",
    "  - Incorrect argument types\n",
    "  - Wrong tool selection for ambiguous tasks\n",
    "\n",
    "### PlanBench (Planning) Insights:\n",
    "- **Any-Order Match**: Most forgiving metric - checks if all steps are present\n",
    "- **In-Order Match**: Validates correct step sequencing\n",
    "- **Efficiency**: Single-tool-use metric penalizes redundant steps\n",
    "- **Common Failures**:\n",
    "  - Missing intermediate steps\n",
    "  - Incorrect step ordering\n",
    "  - Over-planning (too many redundant steps)\n",
    "\n",
    "### Production Recommendations:\n",
    "1. **Baseline**: Achieve >70% BFCL accuracy and >60% PlanBench any-order match\n",
    "2. **Model Selection**: Use GPT-4 for planning tasks, GPT-3.5 may struggle\n",
    "3. **Prompt Engineering**: Provide clear tool descriptions and examples\n",
    "4. **Fallback Strategy**: Implement retry logic for low-confidence predictions\n",
    "5. **Continuous Eval**: Re-run benchmarks after prompt/model changes\n",
    "\n",
    "### Next Steps:\n",
    "- Review error cases to identify patterns\n",
    "- Fine-tune prompts for low-performing categories\n",
    "- Implement autoraters for production monitoring\n",
    "- Expand benchmark coverage with domain-specific test cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}