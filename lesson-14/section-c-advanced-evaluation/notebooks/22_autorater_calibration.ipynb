{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autorater Calibration Tutorial\n",
    "\n",
    "**Lesson 14 - Agent Evaluation Methodology (Task 4.11)**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. Understand how to build LLM-as-Judge autoraters for agent evaluation\n",
    "2. Design effective autorater prompts with multi-dimensional scoring\n",
    "3. Calibrate autoraters against human annotations\n",
    "4. Measure agreement metrics (correlation, Cohen's kappa, MAE)\n",
    "5. Identify and analyze systematic biases in autorater evaluations\n",
    "6. Visualize autorater vs. human score comparisons\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Part 1**: Introduction & mode selection (DEMO/FULL)\n",
    "- **Part 2**: Load agent responses and human annotations\n",
    "- **Part 3**: Define evaluation criteria (accuracy, completeness, tool usage, reasoning quality)\n",
    "- **Part 4**: Build autorater prompts with scoring rubrics\n",
    "- **Part 5**: Run autorater evaluations\n",
    "- **Part 6**: Calibrate with human feedback\n",
    "- **Part 7**: Visualize correlation and bias\n",
    "- **Part 8**: Save results and validate\n",
    "\n",
    "## Execution Modes\n",
    "\n",
    "- **DEMO mode**: 10 responses √ó 4 criteria = 40 evaluations (~$0.60-$0.80, <3 min)\n",
    "- **FULL mode**: 50 responses √ó 4 criteria = 200 evaluations (~$2.50-$3.00, <10 min)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `agent_evaluation_fundamentals.md`\n",
    "- Completed `autorater_final_response_eval.md`\n",
    "- Anthropic API key configured\n",
    "- Basic understanding of LLM-as-Judge methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode selection\n",
    "MODE = \"DEMO\"  # Change to \"FULL\" for complete evaluation\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"DEMO\": {\n",
    "        \"num_responses\": 10,\n",
    "        \"estimated_cost\": \"$0.60-$0.80\",\n",
    "        \"estimated_time\": \"2-3 minutes\"\n",
    "    },\n",
    "    \"FULL\": {\n",
    "        \"num_responses\": 50,\n",
    "        \"estimated_cost\": \"$2.50-$3.00\",\n",
    "        \"estimated_time\": \"8-10 minutes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üéØ Mode: {MODE}\")\n",
    "print(f\"üìä Responses to evaluate: {CONFIG[MODE]['num_responses']}\")\n",
    "print(f\"üí∞ Estimated cost: {CONFIG[MODE]['estimated_cost']}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {CONFIG[MODE]['estimated_time']}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Anthropic API key\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"ANTHROPIC_API_KEY not found. Please set it in your environment:\\n\"\n",
    "        \"export ANTHROPIC_API_KEY='your-api-key-here'\"\n",
    "    )\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"‚úÖ Anthropic API client initialized\")\n",
    "\n",
    "# Test API connection\n",
    "try:\n",
    "    test_response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=10,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "    print(\"‚úÖ API connection successful\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Anthropic API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Load Agent Response Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent responses\n",
    "responses_path = Path(\"data/agent_responses_sample.json\")\n",
    "annotations_path = Path(\"data/human_annotations.json\")\n",
    "\n",
    "if not responses_path.exists():\n",
    "    raise FileNotFoundError(f\"Agent responses file not found: {responses_path}\")\n",
    "if not annotations_path.exists():\n",
    "    raise FileNotFoundError(f\"Human annotations file not found: {annotations_path}\")\n",
    "\n",
    "with open(responses_path, 'r') as f:\n",
    "    responses_data = json.load(f)\n",
    "\n",
    "with open(annotations_path, 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "# Extract responses and filter by mode\n",
    "all_responses = responses_data['responses']\n",
    "all_annotations = annotations_data['annotations']\n",
    "\n",
    "num_to_evaluate = CONFIG[MODE]['num_responses']\n",
    "responses = all_responses[:num_to_evaluate]\n",
    "annotations = all_annotations[:num_to_evaluate]\n",
    "\n",
    "print(f\"üì• Loaded {len(responses)} agent responses\")\n",
    "print(f\"üì• Loaded {len(annotations)} human annotations\")\n",
    "print(f\"\\nSample response:\")\n",
    "print(f\"  Query: {responses[0]['query']}\")\n",
    "print(f\"  Type: {responses[0]['query_type']}\")\n",
    "print(f\"  Tools used: {', '.join(responses[0]['tools_used'])}\")\n",
    "print(f\"  Response length: {len(responses[0]['agent_output'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data distribution\n",
    "query_types = [r['query_type'] for r in responses]\n",
    "from collections import Counter\n",
    "\n",
    "type_counts = Counter(query_types)\n",
    "\n",
    "print(\"üìä Query type distribution:\")\n",
    "for qtype, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {qtype}: {count} ({count/len(responses)*100:.1f}%)\")\n",
    "\n",
    "# Check annotation completeness\n",
    "assert len(responses) == len(annotations), \"Mismatch between responses and annotations\"\n",
    "for i, (resp, annot) in enumerate(zip(responses, annotations)):\n",
    "    assert resp['response_id'] == annot['response_id'], f\"ID mismatch at index {i}\"\n",
    "\n",
    "print(\"\\n‚úÖ Data validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Define Evaluation Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 4 evaluation criteria with detailed rubrics\n",
    "CRITERIA = {\n",
    "    \"accuracy\": {\n",
    "        \"name\": \"Accuracy\",\n",
    "        \"description\": \"Factual correctness of the information provided\",\n",
    "        \"rubric\": {\n",
    "            1: \"Multiple factual errors or completely incorrect information\",\n",
    "            2: \"Some factual errors or outdated information\",\n",
    "            3: \"Mostly accurate with minor errors or ambiguities\",\n",
    "            4: \"Accurate information with proper context\",\n",
    "            5: \"Completely accurate with excellent supporting details\"\n",
    "        }\n",
    "    },\n",
    "    \"completeness\": {\n",
    "        \"name\": \"Completeness\",\n",
    "        \"description\": \"How well the response addresses all aspects of the query\",\n",
    "        \"rubric\": {\n",
    "            1: \"Fails to address the main query or missing critical information\",\n",
    "            2: \"Addresses query partially, missing important aspects\",\n",
    "            3: \"Addresses main query but could include more relevant details\",\n",
    "            4: \"Thoroughly addresses query with good coverage\",\n",
    "            5: \"Comprehensively addresses all aspects with excellent depth\"\n",
    "        }\n",
    "    },\n",
    "    \"tool_usage\": {\n",
    "        \"name\": \"Tool Usage\",\n",
    "        \"description\": \"Appropriate selection and use of tools/functions\",\n",
    "        \"rubric\": {\n",
    "            1: \"Wrong tools used or critical tools missing\",\n",
    "            2: \"Suboptimal tool selection or execution\",\n",
    "            3: \"Appropriate tools used with minor inefficiencies\",\n",
    "            4: \"Well-chosen tools used effectively\",\n",
    "            5: \"Optimal tool selection and execution\"\n",
    "        }\n",
    "    },\n",
    "    \"reasoning_quality\": {\n",
    "        \"name\": \"Reasoning Quality\",\n",
    "        \"description\": \"Logical coherence and clarity of the reasoning process\",\n",
    "        \"rubric\": {\n",
    "            1: \"Illogical or incoherent reasoning\",\n",
    "            2: \"Weak reasoning with logical gaps\",\n",
    "            3: \"Generally sound reasoning with minor issues\",\n",
    "            4: \"Clear and logical reasoning throughout\",\n",
    "            5: \"Exceptional reasoning with excellent step-by-step clarity\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Evaluation Criteria:\")\n",
    "print(\"=\"*80)\n",
    "for criterion_key, criterion in CRITERIA.items():\n",
    "    print(f\"\\n{criterion['name'].upper()}\")\n",
    "    print(f\"Description: {criterion['description']}\")\n",
    "    print(\"\\nScoring Rubric (1-5):\")\n",
    "    for score, description in criterion['rubric'].items():\n",
    "        print(f\"  {score} - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Build Autorater Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autorater prompt template\n",
    "def build_autorater_prompt(query: str, response: str, tools: List[str], \n",
    "                          reasoning: List[str], criterion_key: str) -> str:\n",
    "    \"\"\"Build autorater prompt for a specific criterion.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        response: Agent response\n",
    "        tools: Tools used by agent\n",
    "        reasoning: Reasoning trace\n",
    "        criterion_key: Which criterion to evaluate (accuracy, completeness, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt for LLM-as-Judge\n",
    "    \"\"\"\n",
    "    criterion = CRITERIA[criterion_key]\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert evaluator assessing an AI agent's response to a user query.\n",
    "\n",
    "**USER QUERY:**\n",
    "{query}\n",
    "\n",
    "**AGENT RESPONSE:**\n",
    "{response}\n",
    "\n",
    "**TOOLS USED:**\n",
    "{', '.join(tools) if tools else 'None'}\n",
    "\n",
    "**REASONING TRACE:**\n",
    "{chr(10).join(f'{i+1}. {step}' for i, step in enumerate(reasoning))}\n",
    "\n",
    "---\n",
    "\n",
    "**EVALUATION CRITERION: {criterion['name'].upper()}**\n",
    "\n",
    "Definition: {criterion['description']}\n",
    "\n",
    "**SCORING RUBRIC (1-5):**\n",
    "{chr(10).join(f'{score}. {description}' for score, description in criterion['rubric'].items())}\n",
    "\n",
    "---\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "\n",
    "1. Carefully analyze the agent's response against the scoring rubric above\n",
    "2. Provide a score from 1-5 based on the rubric\n",
    "3. Explain your reasoning with specific evidence from the response\n",
    "4. Be objective and consistent in your evaluation\n",
    "\n",
    "**OUTPUT FORMAT (JSON):**\n",
    "\n",
    "{{\n",
    "  \"score\": <integer 1-5>,\n",
    "  \"reasoning\": \"<2-3 sentence explanation>\",\n",
    "  \"evidence\": \"<specific quote or example from the response>\"\n",
    "}}\n",
    "\n",
    "Provide ONLY the JSON output, no additional text.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test the prompt template\n",
    "test_prompt = build_autorater_prompt(\n",
    "    query=responses[0]['query'],\n",
    "    response=responses[0]['agent_output'],\n",
    "    tools=responses[0]['tools_used'],\n",
    "    reasoning=responses[0]['reasoning_trace'],\n",
    "    criterion_key='accuracy'\n",
    ")\n",
    "\n",
    "print(\"üìù Sample Autorater Prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(test_prompt[:500] + \"...\\n[truncated]\")\n",
    "print(\"\\n‚úÖ Prompt template created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Run Autorater Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_autorater(query: str, response: str, tools: List[str], \n",
    "                           reasoning: List[str], criterion_key: str) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a single response on a single criterion using LLM-as-Judge.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        response: Agent response\n",
    "        tools: Tools used\n",
    "        reasoning: Reasoning trace\n",
    "        criterion_key: Criterion to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation result with score, reasoning, evidence\n",
    "    \"\"\"\n",
    "    prompt = build_autorater_prompt(query, response, tools, reasoning, criterion_key)\n",
    "    \n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.0,  # Deterministic for consistency\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        content = message.content[0].text\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        # Validate score is in range\n",
    "        if not isinstance(result['score'], int) or not 1 <= result['score'] <= 5:\n",
    "            raise ValueError(f\"Invalid score: {result['score']}\")\n",
    "        \n",
    "        return {\n",
    "            \"score\": result['score'],\n",
    "            \"reasoning\": result.get('reasoning', ''),\n",
    "            \"evidence\": result.get('evidence', ''),\n",
    "            \"input_tokens\": message.usage.input_tokens,\n",
    "            \"output_tokens\": message.usage.output_tokens\n",
    "        }\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ö†Ô∏è  JSON parsing error: {e}\")\n",
    "        print(f\"Raw response: {content[:200]}\")\n",
    "        return {\"score\": 3, \"reasoning\": \"Error in parsing\", \"evidence\": \"\", \n",
    "                \"input_tokens\": 0, \"output_tokens\": 0}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Evaluation error: {e}\")\n",
    "        return {\"score\": 3, \"reasoning\": \"Error in evaluation\", \"evidence\": \"\",\n",
    "                \"input_tokens\": 0, \"output_tokens\": 0}\n",
    "\n",
    "print(\"‚úÖ Autorater evaluation function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run autorater evaluation on all responses\n",
    "print(f\"üöÄ Starting autorater evaluation ({MODE} mode)\")\n",
    "print(f\"Evaluating {len(responses)} responses √ó {len(CRITERIA)} criteria = {len(responses) * len(CRITERIA)} total evaluations\\n\")\n",
    "\n",
    "evaluation_results = []\n",
    "total_cost = 0.0\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "\n",
    "# Cost per 1M tokens (Claude 3.5 Sonnet)\n",
    "INPUT_COST_PER_1M = 3.00\n",
    "OUTPUT_COST_PER_1M = 15.00\n",
    "\n",
    "for i, (resp, annot) in enumerate(tqdm(zip(responses, annotations), \n",
    "                                       total=len(responses),\n",
    "                                       desc=\"Evaluating responses\")):\n",
    "    \n",
    "    response_eval = {\n",
    "        \"response_id\": resp['response_id'],\n",
    "        \"query\": resp['query'],\n",
    "        \"query_type\": resp['query_type'],\n",
    "        \"autorater_scores\": {},\n",
    "        \"human_scores\": {\n",
    "            \"accuracy\": annot['accuracy'],\n",
    "            \"completeness\": annot['completeness'],\n",
    "            \"tool_usage\": annot['tool_usage'],\n",
    "            \"reasoning_quality\": annot['reasoning_quality']\n",
    "        },\n",
    "        \"human_annotator\": annot['annotator_id'],\n",
    "        \"human_confidence\": annot['confidence']\n",
    "    }\n",
    "    \n",
    "    # Evaluate on each criterion\n",
    "    for criterion_key in CRITERIA.keys():\n",
    "        eval_result = evaluate_with_autorater(\n",
    "            query=resp['query'],\n",
    "            response=resp['agent_output'],\n",
    "            tools=resp['tools_used'],\n",
    "            reasoning=resp['reasoning_trace'],\n",
    "            criterion_key=criterion_key\n",
    "        )\n",
    "        \n",
    "        response_eval['autorater_scores'][criterion_key] = {\n",
    "            \"score\": eval_result['score'],\n",
    "            \"reasoning\": eval_result['reasoning'],\n",
    "            \"evidence\": eval_result['evidence']\n",
    "        }\n",
    "        \n",
    "        # Track token usage and cost\n",
    "        total_input_tokens += eval_result['input_tokens']\n",
    "        total_output_tokens += eval_result['output_tokens']\n",
    "    \n",
    "    evaluation_results.append(response_eval)\n",
    "\n",
    "# Calculate total cost\n",
    "total_cost = (\n",
    "    (total_input_tokens / 1_000_000) * INPUT_COST_PER_1M +\n",
    "    (total_output_tokens / 1_000_000) * OUTPUT_COST_PER_1M\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"üìä Total evaluations: {len(evaluation_results) * len(CRITERIA)}\")\n",
    "print(f\"üî¢ Input tokens: {total_input_tokens:,}\")\n",
    "print(f\"üî¢ Output tokens: {total_output_tokens:,}\")\n",
    "print(f\"üí∞ Total cost: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample evaluation\n",
    "sample_eval = evaluation_results[0]\n",
    "\n",
    "print(\"üìã Sample Evaluation Result:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Query: {sample_eval['query']}\")\n",
    "print(f\"Query Type: {sample_eval['query_type']}\")\n",
    "print(f\"\\nScores Comparison:\")\n",
    "print(f\"{'Criterion':<20} {'Autorater':<12} {'Human':<12} {'Difference'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for criterion_key in CRITERIA.keys():\n",
    "    auto_score = sample_eval['autorater_scores'][criterion_key]['score']\n",
    "    human_score = sample_eval['human_scores'][criterion_key]\n",
    "    diff = auto_score - human_score\n",
    "    \n",
    "    print(f\"{CRITERIA[criterion_key]['name']:<20} {auto_score:<12} {human_score:<12} {diff:+.1f}\")\n",
    "\n",
    "print(f\"\\nAutorater Reasoning (Accuracy):\")\n",
    "print(sample_eval['autorater_scores']['accuracy']['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Calibration with Human Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate agreement metrics\n",
    "def calculate_agreement_metrics(evaluation_results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate agreement metrics between autorater and human scores.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: List of evaluation results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of agreement metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"overall\": {},\n",
    "        \"by_criterion\": {}\n",
    "    }\n",
    "    \n",
    "    # Collect all scores\n",
    "    all_autorater_scores = []\n",
    "    all_human_scores = []\n",
    "    \n",
    "    criterion_scores = {crit: {\"autorater\": [], \"human\": []} for crit in CRITERIA.keys()}\n",
    "    \n",
    "    for eval_result in evaluation_results:\n",
    "        for criterion_key in CRITERIA.keys():\n",
    "            auto_score = eval_result['autorater_scores'][criterion_key]['score']\n",
    "            human_score = eval_result['human_scores'][criterion_key]\n",
    "            \n",
    "            all_autorater_scores.append(auto_score)\n",
    "            all_human_scores.append(human_score)\n",
    "            \n",
    "            criterion_scores[criterion_key]['autorater'].append(auto_score)\n",
    "            criterion_scores[criterion_key]['human'].append(human_score)\n",
    "    \n",
    "    # Overall metrics\n",
    "    metrics['overall']['pearson_correlation'] = stats.pearsonr(all_autorater_scores, all_human_scores)[0]\n",
    "    metrics['overall']['spearman_correlation'] = stats.spearmanr(all_autorater_scores, all_human_scores)[0]\n",
    "    metrics['overall']['mean_absolute_error'] = mean_absolute_error(all_human_scores, all_autorater_scores)\n",
    "    \n",
    "    # Convert to categorical for Cohen's kappa\n",
    "    metrics['overall']['cohens_kappa'] = cohen_kappa_score(all_human_scores, all_autorater_scores)\n",
    "    \n",
    "    # Mean difference (bias)\n",
    "    differences = np.array(all_autorater_scores) - np.array(all_human_scores)\n",
    "    metrics['overall']['mean_difference'] = float(np.mean(differences))\n",
    "    metrics['overall']['std_difference'] = float(np.std(differences))\n",
    "    \n",
    "    # Per-criterion metrics\n",
    "    for criterion_key, scores in criterion_scores.items():\n",
    "        auto = scores['autorater']\n",
    "        human = scores['human']\n",
    "        \n",
    "        metrics['by_criterion'][criterion_key] = {\n",
    "            \"pearson_correlation\": stats.pearsonr(auto, human)[0],\n",
    "            \"spearman_correlation\": stats.spearmanr(auto, human)[0],\n",
    "            \"mean_absolute_error\": mean_absolute_error(human, auto),\n",
    "            \"mean_difference\": float(np.mean(np.array(auto) - np.array(human))),\n",
    "            \"autorater_mean\": float(np.mean(auto)),\n",
    "            \"human_mean\": float(np.mean(human))\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "agreement_metrics = calculate_agreement_metrics(evaluation_results)\n",
    "\n",
    "print(\"üìä AGREEMENT METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nOVERALL METRICS:\")\n",
    "print(f\"  Pearson Correlation:  {agreement_metrics['overall']['pearson_correlation']:.3f}\")\n",
    "print(f\"  Spearman Correlation: {agreement_metrics['overall']['spearman_correlation']:.3f}\")\n",
    "print(f\"  Cohen's Kappa:        {agreement_metrics['overall']['cohens_kappa']:.3f}\")\n",
    "print(f\"  Mean Absolute Error:  {agreement_metrics['overall']['mean_absolute_error']:.3f}\")\n",
    "print(f\"  Mean Difference:      {agreement_metrics['overall']['mean_difference']:+.3f}\")\n",
    "print(f\"  Std Difference:       {agreement_metrics['overall']['std_difference']:.3f}\")\n",
    "\n",
    "print(\"\\nPER-CRITERION CORRELATIONS:\")\n",
    "print(f\"{'Criterion':<20} {'Pearson':<10} {'Spearman':<10} {'MAE':<10} {'Bias'}\")\n",
    "print(\"-\" * 70)\n",
    "for criterion_key, metrics in agreement_metrics['by_criterion'].items():\n",
    "    print(f\"{CRITERIA[criterion_key]['name']:<20} \"\n",
    "          f\"{metrics['pearson_correlation']:<10.3f} \"\n",
    "          f\"{metrics['spearman_correlation']:<10.3f} \"\n",
    "          f\"{metrics['mean_absolute_error']:<10.3f} \"\n",
    "          f\"{metrics['mean_difference']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bias patterns\n",
    "print(\"üîç BIAS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mean_diff = agreement_metrics['overall']['mean_difference']\n",
    "\n",
    "if abs(mean_diff) < 0.1:\n",
    "    bias_interpretation = \"No systematic bias detected\"\n",
    "elif mean_diff > 0:\n",
    "    bias_interpretation = f\"Autorater tends to score {mean_diff:.2f} points HIGHER than humans\"\n",
    "else:\n",
    "    bias_interpretation = f\"Autorater tends to score {abs(mean_diff):.2f} points LOWER than humans\"\n",
    "\n",
    "print(f\"\\nOverall Bias: {bias_interpretation}\")\n",
    "\n",
    "print(\"\\nPer-Criterion Bias:\")\n",
    "for criterion_key, metrics in agreement_metrics['by_criterion'].items():\n",
    "    diff = metrics['mean_difference']\n",
    "    if abs(diff) < 0.1:\n",
    "        bias = \"Neutral\"\n",
    "    elif diff > 0:\n",
    "        bias = f\"Over-scores by {diff:.2f}\"\n",
    "    else:\n",
    "        bias = f\"Under-scores by {abs(diff):.2f}\"\n",
    "    \n",
    "    print(f\"  {CRITERIA[criterion_key]['name']:<20}: {bias}\")\n",
    "\n",
    "# Identify disagreement cases\n",
    "large_disagreements = []\n",
    "for eval_result in evaluation_results:\n",
    "    for criterion_key in CRITERIA.keys():\n",
    "        auto_score = eval_result['autorater_scores'][criterion_key]['score']\n",
    "        human_score = eval_result['human_scores'][criterion_key]\n",
    "        diff = abs(auto_score - human_score)\n",
    "        \n",
    "        if diff >= 2:  # Disagreement of 2+ points\n",
    "            large_disagreements.append({\n",
    "                \"response_id\": eval_result['response_id'],\n",
    "                \"query\": eval_result['query'],\n",
    "                \"criterion\": CRITERIA[criterion_key]['name'],\n",
    "                \"autorater\": auto_score,\n",
    "                \"human\": human_score,\n",
    "                \"difference\": auto_score - human_score\n",
    "            })\n",
    "\n",
    "print(f\"\\nLarge Disagreements (‚â•2 points): {len(large_disagreements)}\")\n",
    "if large_disagreements:\n",
    "    print(\"\\nTop 3 Disagreements:\")\n",
    "    for i, disagreement in enumerate(sorted(large_disagreements, \n",
    "                                            key=lambda x: abs(x['difference']), \n",
    "                                            reverse=True)[:3], 1):\n",
    "        print(f\"\\n{i}. {disagreement['criterion']} - {disagreement['query'][:60]}...\")\n",
    "        print(f\"   Autorater: {disagreement['autorater']}, Human: {disagreement['human']} \"\n",
    "              f\"(Diff: {disagreement['difference']:+d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (criterion_key, criterion) in enumerate(CRITERIA.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Extract scores\n",
    "    auto_scores = [e['autorater_scores'][criterion_key]['score'] for e in evaluation_results]\n",
    "    human_scores = [e['human_scores'][criterion_key] for e in evaluation_results]\n",
    "    \n",
    "    # Scatter plot with jitter to show overlapping points\n",
    "    jitter = 0.1\n",
    "    auto_jittered = np.array(auto_scores) + np.random.uniform(-jitter, jitter, len(auto_scores))\n",
    "    human_jittered = np.array(human_scores) + np.random.uniform(-jitter, jitter, len(human_scores))\n",
    "    \n",
    "    ax.scatter(human_jittered, auto_jittered, alpha=0.6, s=80, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Perfect agreement line\n",
    "    ax.plot([1, 5], [1, 5], 'r--', linewidth=2, label='Perfect Agreement', alpha=0.7)\n",
    "    \n",
    "    # Regression line\n",
    "    z = np.polyfit(human_scores, auto_scores, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot([1, 5], [p(1), p(5)], 'b-', linewidth=2, label='Regression', alpha=0.7)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Human Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Autorater Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f\"{criterion['name']}\\n(r={agreement_metrics['by_criterion'][criterion_key]['pearson_correlation']:.3f})\",\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0.5, 5.5)\n",
    "    ax.set_ylim(0.5, 5.5)\n",
    "    ax.set_xticks(range(1, 6))\n",
    "    ax.set_yticks(range(1, 6))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper left', fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Autorater vs Human Score Correlation by Criterion', \n",
    "            fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Correlation plots generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for each criterion\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (criterion_key, criterion) in enumerate(CRITERIA.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Extract scores\n",
    "    auto_scores = [e['autorater_scores'][criterion_key]['score'] for e in evaluation_results]\n",
    "    human_scores = [e['human_scores'][criterion_key] for e in evaluation_results]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    confusion = np.zeros((5, 5))\n",
    "    for h, a in zip(human_scores, auto_scores):\n",
    "        confusion[h-1, a-1] += 1\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(confusion, annot=True, fmt='g', cmap='Blues', ax=ax,\n",
    "               xticklabels=range(1, 6), yticklabels=range(1, 6),\n",
    "               cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    ax.set_xlabel('Autorater Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Human Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f\"{criterion['name']} Confusion Matrix\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Score Agreement Confusion Matrices', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Confusion matrices generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparing mean scores\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "criteria_names = [CRITERIA[k]['name'] for k in CRITERIA.keys()]\n",
    "autorater_means = [agreement_metrics['by_criterion'][k]['autorater_mean'] for k in CRITERIA.keys()]\n",
    "human_means = [agreement_metrics['by_criterion'][k]['human_mean'] for k in CRITERIA.keys()]\n",
    "\n",
    "# Number of criteria\n",
    "num_criteria = len(criteria_names)\n",
    "angles = np.linspace(0, 2 * np.pi, num_criteria, endpoint=False).tolist()\n",
    "autorater_means += autorater_means[:1]  # Complete the circle\n",
    "human_means += human_means[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "ax.plot(angles, autorater_means, 'o-', linewidth=2, label='Autorater', color='blue')\n",
    "ax.fill(angles, autorater_means, alpha=0.25, color='blue')\n",
    "\n",
    "ax.plot(angles, human_means, 'o-', linewidth=2, label='Human', color='red')\n",
    "ax.fill(angles, human_means, alpha=0.25, color='red')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(criteria_names, fontsize=11)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_yticks([1, 2, 3, 4, 5])\n",
    "ax.set_yticklabels(['1', '2', '3', '4', '5'])\n",
    "ax.set_title('Mean Score Comparison: Autorater vs Human', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Radar chart generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias heatmap by criterion and score level\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Calculate bias for each criterion at each score level\n",
    "bias_matrix = []\n",
    "for criterion_key in CRITERIA.keys():\n",
    "    criterion_bias = []\n",
    "    for score_level in range(1, 6):\n",
    "        # Get cases where human scored this level\n",
    "        auto_scores = [e['autorater_scores'][criterion_key]['score'] \n",
    "                      for e in evaluation_results \n",
    "                      if e['human_scores'][criterion_key] == score_level]\n",
    "        \n",
    "        if auto_scores:\n",
    "            mean_auto = np.mean(auto_scores)\n",
    "            bias = mean_auto - score_level\n",
    "        else:\n",
    "            bias = 0\n",
    "        \n",
    "        criterion_bias.append(bias)\n",
    "    \n",
    "    bias_matrix.append(criterion_bias)\n",
    "\n",
    "bias_matrix = np.array(bias_matrix)\n",
    "\n",
    "sns.heatmap(bias_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "           xticklabels=[f'Score {i}' for i in range(1, 6)],\n",
    "           yticklabels=[CRITERIA[k]['name'] for k in CRITERIA.keys()],\n",
    "           cbar_kws={'label': 'Autorater Bias (positive = over-scores)'},\n",
    "           vmin=-1, vmax=1, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Human Score Level', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Criterion', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Autorater Bias by Criterion and Human Score Level', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Bias heatmap generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Save Results & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for saving\n",
    "import time\n",
    "\n",
    "results_output = {\n",
    "    \"metadata\": {\n",
    "        \"mode\": MODE,\n",
    "        \"num_responses\": len(responses),\n",
    "        \"num_criteria\": len(CRITERIA),\n",
    "        \"total_evaluations\": len(responses) * len(CRITERIA),\n",
    "        \"model\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"total_cost_usd\": round(total_cost, 2),\n",
    "        \"input_tokens\": total_input_tokens,\n",
    "        \"output_tokens\": total_output_tokens\n",
    "    },\n",
    "    \"criteria\": CRITERIA,\n",
    "    \"agreement_metrics\": agreement_metrics,\n",
    "    \"evaluation_results\": evaluation_results,\n",
    "    \"large_disagreements\": large_disagreements\n",
    "}\n",
    "\n",
    "# Save to results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_path = results_dir / \"autorater_calibration_results.json\"\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_output, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_path}\")\n",
    "print(f\"\\nüìä FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mode:                    {MODE}\")\n",
    "print(f\"Responses evaluated:     {len(responses)}\")\n",
    "print(f\"Total evaluations:       {len(responses) * len(CRITERIA)}\")\n",
    "print(f\"Total cost:              ${total_cost:.2f}\")\n",
    "print(f\"\\nAgreement Metrics:\")\n",
    "print(f\"  Pearson correlation:   {agreement_metrics['overall']['pearson_correlation']:.3f}\")\n",
    "print(f\"  Spearman correlation:  {agreement_metrics['overall']['spearman_correlation']:.3f}\")\n",
    "print(f\"  Cohen's kappa:         {agreement_metrics['overall']['cohens_kappa']:.3f}\")\n",
    "print(f\"  Mean absolute error:   {agreement_metrics['overall']['mean_absolute_error']:.3f}\")\n",
    "print(f\"  Mean bias:             {agreement_metrics['overall']['mean_difference']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "print(\"üîç VALIDATION CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checks_passed = 0\n",
    "checks_total = 0\n",
    "\n",
    "# Check 1: Cost within budget\n",
    "checks_total += 1\n",
    "if MODE == \"DEMO\":\n",
    "    cost_limit = 1.00\n",
    "else:\n",
    "    cost_limit = 3.00\n",
    "\n",
    "if total_cost <= cost_limit:\n",
    "    print(f\"‚úÖ Cost check passed: ${total_cost:.2f} <= ${cost_limit:.2f}\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"‚ùå Cost check failed: ${total_cost:.2f} > ${cost_limit:.2f}\")\n",
    "\n",
    "# Check 2: All evaluations completed\n",
    "checks_total += 1\n",
    "expected_evals = len(responses) * len(CRITERIA)\n",
    "actual_evals = len(evaluation_results) * len(CRITERIA)\n",
    "\n",
    "if actual_evals == expected_evals:\n",
    "    print(f\"‚úÖ Evaluation completeness: {actual_evals}/{expected_evals}\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"‚ùå Evaluation completeness: {actual_evals}/{expected_evals}\")\n",
    "\n",
    "# Check 3: Correlation is reasonable (>0.5)\n",
    "checks_total += 1\n",
    "correlation = agreement_metrics['overall']['pearson_correlation']\n",
    "if correlation >= 0.5:\n",
    "    print(f\"‚úÖ Correlation check: {correlation:.3f} >= 0.500\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Correlation check: {correlation:.3f} < 0.500 (autorater may need calibration)\")\n",
    "\n",
    "# Check 4: Results file exists\n",
    "checks_total += 1\n",
    "if results_path.exists():\n",
    "    print(f\"‚úÖ Results file created: {results_path}\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"‚ùå Results file missing: {results_path}\")\n",
    "\n",
    "# Check 5: No NaN values in metrics\n",
    "checks_total += 1\n",
    "has_nan = any(np.isnan(v) if isinstance(v, (int, float)) else False \n",
    "             for v in agreement_metrics['overall'].values())\n",
    "\n",
    "if not has_nan:\n",
    "    print(f\"‚úÖ Data integrity check: No NaN values\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"‚ùå Data integrity check: NaN values detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üéØ Validation: {checks_passed}/{checks_total} checks passed\")\n",
    "\n",
    "if checks_passed == checks_total:\n",
    "    print(\"‚úÖ ALL VALIDATION CHECKS PASSED - Tutorial complete!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {checks_total - checks_passed} validation check(s) failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Autorater Design**: Effective autoraters require clear rubrics, detailed prompts, and multi-dimensional evaluation\n",
    "\n",
    "2. **Calibration Process**: \n",
    "   - Collect human annotations on diverse examples\n",
    "   - Calculate agreement metrics (correlation, kappa, MAE)\n",
    "   - Identify systematic biases\n",
    "   - Iterate on prompts to improve alignment\n",
    "\n",
    "3. **Agreement Metrics Interpretation**:\n",
    "   - **Pearson r > 0.7**: Good correlation\n",
    "   - **Cohen's kappa > 0.6**: Substantial agreement\n",
    "   - **MAE < 0.5**: Scores are close on average\n",
    "\n",
    "4. **Common Biases**:\n",
    "   - Over-scoring or under-scoring specific criteria\n",
    "   - Difficulty with edge cases or ambiguous responses\n",
    "   - Inconsistency across different query types\n",
    "\n",
    "5. **Production Recommendations**:\n",
    "   - Start with DEMO mode for rapid iteration\n",
    "   - Use FULL mode for final calibration\n",
    "   - Re-calibrate periodically with new human annotations\n",
    "   - Monitor autorater performance in production\n",
    "   - Use HITL evaluation for critical decisions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Tutorial**: Complete `benchmark_evaluation.ipynb` (Task 4.12)\n",
    "- **Concept**: Read `human_in_the_loop_evaluation.md` for HITL workflows\n",
    "- **Practice**: Build custom autoraters for your domain\n",
    "- **Integration**: Use `backend/trajectory_evaluation.py` for full agent evaluation pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "1. **Insufficient Human Annotations**: Need at least 50+ examples for reliable calibration\n",
    "2. **Vague Rubrics**: Autoraters perform poorly without clear, detailed scoring criteria\n",
    "3. **Temperature > 0**: Use temperature=0 for deterministic, consistent evaluations\n",
    "4. **Ignoring Bias**: Systematically over/under-scoring can invalidate results\n",
    "5. **No Re-calibration**: Autorater performance drifts over time‚Äîre-calibrate regularly\n",
    "\n",
    "---\n",
    "\n",
    "## FAQ\n",
    "\n",
    "**Q: How many human annotations do I need?**\n",
    "A: Minimum 30 for initial calibration, 100+ for production use. More diverse examples = better calibration.\n",
    "\n",
    "**Q: What if correlation is low (<0.5)?**\n",
    "A: Refine your rubrics, add more detailed examples in prompts, or consider multi-annotator consensus for ground truth.\n",
    "\n",
    "**Q: Should I use autoraters or human evaluation?**\n",
    "A: Use both! Autoraters for rapid iteration and scale, humans for critical decisions and calibration.\n",
    "\n",
    "**Q: How often should I re-calibrate?**\n",
    "A: Monthly for active projects, or whenever you detect performance drift (correlation drops >0.1).\n",
    "\n",
    "**Q: Can I use this for multi-turn agent conversations?**\n",
    "A: Yes! Adapt the rubrics to evaluate full trajectories instead of single responses. See `trajectory_evaluation_tutorial.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
