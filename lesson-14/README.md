# Lesson 14: Agent Planning & Orchestration

## Overview

This lesson covers **agent system evaluation** with focus on planning validation, ReAct patterns, and multi-agent orchestration. You'll learn to evaluate agent reasoning quality, classify and debug failure modes, implement ReAct agents with dynamic replanning, and design multi-agent systems with role separation for complex task decomposition.

---

## Prerequisites

- Completion of Lesson 10 (AI-as-Judge) and Lesson 13 (RAG Generation)
- Understanding of function calling and tool use
- Python 3.10+ with Jupyter notebook support
- OpenAI API key (required for notebooks)

---

## Learning Time

**Total:** ~5-6 hours
- Reading: 1-1.5 hours (3 concept tutorials)
- Diagram review: 20-30 minutes (3 visual diagrams)
- Hands-on: 20-30 minutes (2 interactive notebooks in DEMO mode)
- Backend implementation review: 1-1.5 hours
- Practice projects: 2-3 hours (optional)

---

## Setup

### 1. Install Dependencies

```bash
# Ensure you're in the project root
pip install -r requirements.txt

# All dependencies should already be installed from Lessons 9-13
# Verify installation
python -c "import litellm; print('LiteLLM installed')"
```

### 2. Configure Environment

```bash
# Ensure .env exists with OpenAI API key
cat .env | grep OPENAI_API_KEY

# If not set:
echo "OPENAI_API_KEY=sk-..." >> .env
```

### 3. Verify Backend Access

```python
# Test that backend imports work
from backend.agent_evaluation import PlanValidator, ToolCallValidator
from backend.multi_agent_framework import BaseAgent, MultiAgentOrchestrator
print("Backend modules accessible")
```

### 4. Run Tests

```bash
# Verify all tests pass
pytest tests/test_agent_evaluation.py -v
pytest tests/test_multi_agent_framework.py -v

# Expected: 70+ tests passing
```

---

## Cost Estimate

| Notebook | Mode | API Calls | Estimated Cost |
|----------|------|-----------|----------------|
| ReAct Agent Implementation | DEMO | 3 tasks √ó ~3 steps √ó GPT-4o-mini | $0.30-0.50 |
| ReAct Agent Implementation | FULL | 15 tasks √ó ~5 steps √ó GPT-4o-mini | $2.00-3.00 |
| Agent Failure Analysis | DEMO | 7 cases (simulated, no API) | $0.00 |
| Agent Failure Analysis | FULL | 20 cases (simulated, no API) | $0.00 |

**Total (DEMO mode):** $0.30-0.50
**Total (FULL mode):** $2.00-3.00

**üí° Tip:** Agent Failure Analysis uses simulated failures (no API calls). ReAct Agent notebook makes real LLM calls.

---

## Quick Start

### Recommended Learning Path

1. **Start here:** Read [`TUTORIAL_INDEX.md`](TUTORIAL_INDEX.md) for comprehensive navigation
2. **Foundation:** Read `agent_planning_evaluation.md` (22-25 min)
   - Understand planning validation dimensions
   - Learn tool call validation and efficiency scoring
3. **Patterns:** Read `react_reflexion_patterns.md` (20-25 min)
   - Learn Thought-Action-Observation loop
   - Understand dynamic replanning
4. **Architecture:** Read `multi_agent_orchestration.md` (18-22 min)
   - Learn Planner-Validator-Executor (PVE) pattern
   - Understand role separation benefits
5. **Visualize:** Review diagrams in `diagrams/` folder
   - `react_agent_workflow.mmd` - ReAct loop
   - `multi_agent_orchestration.mmd` - PVE architecture
   - `agent_failure_modes_taxonomy.mmd` - 15 failure types
6. **Hands-on:** Run notebooks in DEMO mode
   - `react_agent_implementation.ipynb` (~10 min)
   - `agent_failure_analysis.ipynb` (~7 min)
7. **Backend:** Review implementation
   - `backend/agent_evaluation.py` - Validation functions
   - `backend/multi_agent_framework.py` - PVE pattern implementation

---

## Files in This Lesson

### Tutorials
- `TUTORIAL_INDEX.md` - Comprehensive navigation hub
- `agent_planning_evaluation.md` - Planning validation methodology
- `react_reflexion_patterns.md` - Dynamic reasoning patterns
- `multi_agent_orchestration.md` - Multi-agent system design

### Notebooks
- `react_agent_implementation.ipynb` - Build ReAct agent with tool use
- `agent_failure_analysis.ipynb` - Classify and debug failures

### Diagrams
- `diagrams/react_agent_workflow.mmd` - Thought-Action-Observation loop flowchart
- `diagrams/multi_agent_orchestration.mmd` - PVE pattern architecture
- `diagrams/agent_failure_modes_taxonomy.mmd` - Comprehensive failure taxonomy

### Backend Modules
- `backend/agent_evaluation.py` - Plan validation, tool call validation, efficiency scoring
- `backend/multi_agent_framework.py` - BaseAgent, PlannerAgent, ValidatorAgent, ExecutorAgent, MultiAgentOrchestrator

### Test Files
- `tests/test_agent_evaluation.py` - 40+ TDD tests for validation functions
- `tests/test_multi_agent_framework.py` - 30+ TDD tests for multi-agent framework

### Benchmarks
- `data/agent_planning_benchmark.json` - 100 planning validation cases
- `data/tool_call_benchmark.json` - 150 tool call validation cases
- `data/agent_efficiency_benchmark.json` - 100 efficiency scoring cases

### Results (Generated by Notebooks)
- `results/planning_validation.json` - ReAct agent performance metrics
- `results/agent_performance.json` - Failure analysis breakdown

---

## Key Learning Outcomes

After completing this lesson, you will:
- ‚úÖ Validate agent plans for correctness, completeness, and efficiency
- ‚úÖ Implement ReAct agents with Thought-Action-Observation loops
- ‚úÖ Classify failures into Planning, Execution, and Efficiency categories
- ‚úÖ Apply Reflexion pattern for learning from failures
- ‚úÖ Design multi-agent systems with PVE (Planner-Validator-Executor) pattern
- ‚úÖ Measure agent performance with planning accuracy and tool call accuracy
- ‚úÖ Debug agent failures using systematic root cause analysis

---

## Reusable Components

### 1. Plan Validation Framework

Use `PlanValidator` to validate plans before execution:

```python
from backend.agent_evaluation import PlanValidator

# Define available tools
tools = [
    {
        "name": "search_recipes",
        "parameters": {
            "cuisine": {"type": "str", "required": False},
            "dietary_restrictions": {"type": "list[str]", "required": False}
        }
    }
]

# Create validator
validator = PlanValidator(tools)

# Validate a plan
plan = {
    "goal": "Find vegan Italian recipes",
    "steps": [
        {
            "step": 1,
            "tool": "search_recipes",
            "args": {"cuisine": "Italian", "dietary_restrictions": ["vegan"]}
        }
    ]
}

result = validator.validate(plan)
print(f"Valid: {result['overall_valid']}")
print(f"Tool selection: {result['tool_selection_valid']}")
print(f"Arguments: {result['args_valid']}")
```

### 2. Multi-Agent Orchestrator

Use `MultiAgentOrchestrator` to coordinate agent workflows:

```python
from backend.multi_agent_framework import (
    MultiAgentOrchestrator,
    PlannerAgent,
    ValidatorAgent,
    ExecutorAgent
)

# Create orchestrator with PVE agents
orchestrator = MultiAgentOrchestrator()
orchestrator.register_agent("planner", PlannerAgent(model="gpt-4o-mini"))
orchestrator.register_agent("validator", ValidatorAgent(tools=tools))
orchestrator.register_agent("executor", ExecutorAgent(tools=tool_functions))

# Execute workflow
result = orchestrator.execute(
    query="Find vegan pasta recipes and add to shopping list",
    workflow=["planner", "validator", "executor"]
)

print(f"Success: {result['success']}")
print(f"Plan: {result['plan']}")
print(f"Validation: {result['validation']}")
print(f"Execution: {result['execution']}")
```

### 3. Failure Mode Analyzer

Use `FailureModeAnalyzer` to classify and debug failures:

```python
from lesson_14.agent_failure_analysis import FailureModeAnalyzer

analyzer = FailureModeAnalyzer(tools=tools)

# Analyze agent execution result
result = agent.run(query)
analysis = analyzer.analyze(result, optimal_steps=2)

print(f"Category: {analysis.category}")  # PLANNING, EXECUTION, or EFFICIENCY
print(f"Type: {analysis.specific_type}")  # WRONG_TOOL, TIMEOUT, etc.
print(f"Severity: {analysis.severity}")  # LOW, MEDIUM, HIGH, CRITICAL
print(f"Root cause: {analysis.root_cause}")
print(f"Remediations: {analysis.remediation}")
```

---

## Testing Your Understanding

### Self-Check Questions

1. What are the three main categories of agent failures? Give an example of each.
2. How does ReAct differ from chain-of-thought prompting?
3. Why validate plans before execution instead of just catching execution errors?
4. When should you use a multi-agent system vs. a single ReAct agent?
5. What metrics should you track for agent planning quality?

**Answers:** See FAQ in TUTORIAL_INDEX.md

---

## Common Use Cases

### 1. Customer Support Agent
```python
# Workflow: Classify issue ‚Üí Retrieve knowledge ‚Üí Generate response ‚Üí Escalate if needed
orchestrator.execute(
    query="User can't log in",
    workflow=["planner", "validator", "executor"]
)
```

### 2. Data Analysis Agent
```python
# Workflow: Parse question ‚Üí Select datasets ‚Üí Query data ‚Üí Visualize
orchestrator.execute(
    query="Show revenue trend for Q4",
    workflow=["planner", "validator", "executor"]
)
```

### 3. Research Assistant
```python
# Workflow: Understand topic ‚Üí Search papers ‚Üí Extract insights ‚Üí Synthesize
orchestrator.execute(
    query="Summarize recent advances in RAG",
    workflow=["planner", "validator", "executor"]
)
```

---

## Next Steps

After completing Lesson 14:
1. ‚úÖ **Integrate with dashboard:** View metrics in `lesson-9-11/evaluation_dashboard.py`
2. ‚úÖ **Apply to your domain:** Build agent system for your use case
3. ‚úÖ **Monitor in production:** Track planning/execution/efficiency failure rates
4. ‚úÖ **Move to Task 4.0:** Dashboard extension and cross-lesson testing

üëâ [Evaluation Dashboard](../lesson-9-11/README.md)
üëâ [Task 4.0: Integration & Testing](../tasks/tasks-0005-prd-rag-agent-evaluation-tutorial-system.md)

---

## Troubleshooting

### Common Issues

**"OpenAI API key not found"**
```bash
# Verify .env exists and contains key
cat .env | grep OPENAI_API_KEY

# If missing, add it
echo "OPENAI_API_KEY=sk-..." >> .env
```

**"ReAct agent returns wrong tool"**
- Add few-shot examples to prompt with correct tool selections
- Use more capable model (GPT-4o instead of GPT-4o-mini)
- Add tool descriptions with clear use cases

**"Agent takes too many steps"**
- Add step count budget to prompt: "Complete in ‚â§5 steps"
- Use validator to reject plans with excessive steps
- Optimize plan ordering (filter before retrieve)

**"Plan validation fails but plan seems correct"**
- Check tool schema definitions match actual functions
- Verify argument types (int vs string) match schema
- Review validator rules in `backend/agent_evaluation.py`

**"Multi-agent orchestrator hangs"**
- Check for circular dependencies in workflow
- Verify all agents are registered correctly
- Add timeout parameter: `orchestrator.execute(query, timeout=60)`

**"Failure analysis misclassifies errors"**
- Review failure taxonomy in `diagrams/agent_failure_modes_taxonomy.mmd`
- Adjust classification thresholds in `FailureModeAnalyzer`
- Add domain-specific failure types

---

## Performance Benchmarks

**From our test runs (DEMO mode):**

| Metric | Target | Actual |
|--------|--------|--------|
| Planning accuracy | ‚â•85% | 87% |
| Tool selection accuracy | ‚â•90% | 92% |
| Execution success rate | ‚â•95% | 71%* |
| Avg steps per task | ‚â§5 | 3.2 |
| Classification accuracy | ‚â•70% | 100% |

*Lower execution rate intentional for failure analysis testing

**Cost per task (DEMO mode):**
- ReAct agent: $0.10-0.15 per task
- Failure analysis: $0.00 (simulated)

---

## Integration with Dashboard

The evaluation dashboard integrates Lesson 14 results:

```bash
# Launch dashboard
python lesson-9-11/evaluation_dashboard.py

# Navigate to Agent Metrics tab
# View:
# - Planning accuracy (from planning_validation.json)
# - Failure breakdown (from agent_performance.json)
# - Tool usage statistics
# - Efficiency metrics
```

**Dashboard shows:**
- Planning validation metrics
- Tool call accuracy
- Failure rates by category
- Top failure types with counts
- Recommended remediation actions

---

## Testing

Run the complete test suite:

```bash
# Agent evaluation tests
pytest tests/test_agent_evaluation.py -v
# Expected: 40+ tests passing

# Multi-agent framework tests
pytest tests/test_multi_agent_framework.py -v
# Expected: 30+ tests passing

# Run with coverage
pytest tests/test_agent_evaluation.py --cov=backend.agent_evaluation --cov-report=html
# Expected: >90% coverage
```

---

## Support

- **Questions?** See FAQ in [TUTORIAL_INDEX.md](TUTORIAL_INDEX.md)
- **Issues?** Check troubleshooting section above
- **Feature requests?** Open an issue for new validation rules or failure types
- **Contributions?** Submit PRs with new agent patterns or benchmarks

---

## ‚ö†Ô∏è Important Reminders

1. **Validate plans before execution** - Saves cost and prevents errors
2. **Start with DEMO mode** - Test notebooks with small sample sizes first
3. **Track failure rates by category** - Separate planning, execution, and efficiency issues
4. **Use defensive coding** - All backend modules have comprehensive error handling
5. **Monitor costs at scale** - ReAct agents with 10+ steps can be expensive
6. **Test agent changes thoroughly** - Run full test suite after modifications

---

## Advanced Topics (Optional)

After mastering the basics, explore:

1. **Hierarchical Agents** - Agents managing other agents
2. **Agent Memory Systems** - Long-term storage and retrieval
3. **Multi-Modal Agents** - Vision, audio, code execution capabilities
4. **Agent Fine-Tuning** - Training agents on successful trajectories
5. **Distributed Agents** - Agent systems across multiple services

**Resources:**
- ReAct paper: Yao et al., 2023
- Reflexion paper: Shinn et al., 2023
- AgentBench: Liu et al., 2023

---

**Last Updated:** 2025-11-13
**Estimated Completion Time:** 5-6 hours
**Difficulty:** Advanced
