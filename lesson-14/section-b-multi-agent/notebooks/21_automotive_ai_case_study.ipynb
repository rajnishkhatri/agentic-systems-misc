{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automotive AI Case Study: Multi-Agent System\n",
    "\n",
    "**Lesson 14 - Task 5.10**: Real-world automotive AI system using 5 specialized agents coordinated through multiple patterns.\n",
    "\n",
    "**Source**: Google AgentCompanion Whitepaper (Topic 07: Case Studies - Automotive AI)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "1. Understand how 5 specialized agents work together in a production automotive AI system\n",
    "2. See all 5 coordination patterns (hierarchical, diamond, P2P, collaborative, adaptive loop) applied to real scenarios\n",
    "3. Learn pattern selection criteria based on query characteristics\n",
    "4. Analyze system performance across 20 real-world automotive queries\n",
    "5. Visualize agent trajectories and decision-making workflows\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "This automotive AI assistant uses **5 specialized agents**:\n",
    "\n",
    "| Agent | Role | Capabilities |\n",
    "|-------|------|-------------|\n",
    "| **Orchestrator** | Central coordinator | Query classification, agent routing, response synthesis |\n",
    "| **Navigation Agent** | Route specialist | Route planning, traffic analysis, POI search |\n",
    "| **Facility Ratings Agent** | Amenity specialist | Facility search, rating retrieval, review analysis |\n",
    "| **Vehicle Diagnostics Agent** | Vehicle specialist | Sensor analysis, fault detection, maintenance prediction |\n",
    "| **Contextual Search Agent** | Search specialist | Semantic search, query expansion, result ranking |\n",
    "\n",
    "## Pattern Distribution (20 Test Queries)\n",
    "\n",
    "| Pattern | Count | Example Use Case |\n",
    "|---------|-------|------------------|\n",
    "| **Hierarchical** | 6 | Simple routing tasks (\"Is there a service area ahead?\") |\n",
    "| **Diamond** | 3 | Query clarification (\"Should I rephrase that?\") |\n",
    "| **P2P** | 2 | Specialist handoff (\"Find transmission repair shop\") |\n",
    "| **Collaborative** | 4 | Complex planning (\"Plan road trip with scenic stops\") |\n",
    "| **Adaptive Loop** | 5 | Iterative refinement (\"Find the BEST coffee shop\") |\n",
    "\n",
    "## Execution Modes\n",
    "\n",
    "- **DEMO Mode**: 5 queries (1 per pattern), simulation (~$0, <2 min)\n",
    "- **FULL Mode**: All 20 queries, simulation (~$0, <5 min)\n",
    "\n",
    "**Note**: This notebook uses *simulated* execution to demonstrate pattern behavior without API costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Select Execution Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION MODE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Change this to \"FULL\" for comprehensive evaluation\n",
    "MODE = \"DEMO\"  # Options: \"DEMO\" or \"FULL\"\n",
    "\n",
    "# Mode-specific configuration\n",
    "MODE_CONFIG = {\n",
    "    \"DEMO\": {\n",
    "        \"num_queries\": 5,  # 1 per pattern\n",
    "        \"estimated_time\": \"<2 min\",\n",
    "        \"cost\": \"$0 (simulated)\",\n",
    "    },\n",
    "    \"FULL\": {\n",
    "        \"num_queries\": 20,  # All queries\n",
    "        \"estimated_time\": \"<5 min\",\n",
    "        \"cost\": \"$0 (simulated)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "config = MODE_CONFIG[MODE]\n",
    "print(f\"üîß Mode: {MODE}\")\n",
    "print(f\"üìä Queries: {config['num_queries']}\")\n",
    "print(f\"‚è±Ô∏è  Estimated Time: {config['estimated_time']}\")\n",
    "print(f\"üí∞ Cost: {config['cost']}\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: This notebook uses SIMULATED execution to demonstrate pattern concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Automotive AI Case Study Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load case study data\n",
    "data_path = Path(\"data/automotive_ai_case_study.json\")\n",
    "assert data_path.exists(), f\"Data file not found: {data_path}\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    case_study = json.load(f)\n",
    "\n",
    "# Display system architecture\n",
    "print(\"üöó Automotive AI System Architecture\\n\")\n",
    "print(f\"Case Study: {case_study['case_study']}\")\n",
    "print(f\"Version: {case_study['version']}\")\n",
    "print(f\"Source: {case_study['source']}\\n\")\n",
    "\n",
    "print(\"üìã System Targets:\")\n",
    "arch = case_study['system_architecture']\n",
    "print(f\"  - Total Agents: {arch['total_agents']}\")\n",
    "print(f\"  - Coordination Patterns: {arch['coordination_patterns']}\")\n",
    "print(f\"  - Target Latency: {arch['target_latency']}\")\n",
    "print(f\"  - Accuracy Target: {arch['accuracy_target']}\")\n",
    "print(f\"  - Cost Target: {arch['cost_target']}\\n\")\n",
    "\n",
    "print(\"ü§ñ Specialized Agents:\\n\")\n",
    "for agent in case_study['agents']:\n",
    "    print(f\"  {agent['agent_id'].upper().replace('_', ' ')}:\")\n",
    "    print(f\"    Role: {agent['role']}\")\n",
    "    print(f\"    Capabilities: {', '.join(agent['capabilities'][:3])} (+ {len(agent['capabilities']) - 3} more)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Agent Classes (Simulation)\n",
    "\n",
    "We'll define simplified mock agents that simulate behavior based on the pre-computed trajectories in the JSON data.\n",
    "This allows us to demonstrate pattern execution without making real LLM API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockOrchestratorAgent:\n",
    "    \"\"\"Central coordinator for query classification and response synthesis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"orchestrator\"\n",
    "    \n",
    "    def classify_query(self, query: str, trajectory: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Simulate query classification.\"\"\"\n",
    "        # Extract classification from pre-computed trajectory\n",
    "        classify_step = [s for s in trajectory if s['action'] == 'classify_query']\n",
    "        if classify_step:\n",
    "            return classify_step[0]['output']\n",
    "        return \"General query\"\n",
    "    \n",
    "    def synthesize_response(self, agent_outputs: list[str], trajectory: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Simulate response synthesis.\"\"\"\n",
    "        # Extract final response from trajectory\n",
    "        final_step = trajectory[-1]\n",
    "        if 'synthesize' in final_step['action'] or 'return' in final_step['action']:\n",
    "            return final_step['output']\n",
    "        return \"Response synthesized from agent outputs\"\n",
    "\n",
    "\n",
    "class MockNavigationAgent:\n",
    "    \"\"\"Route planning and POI search specialist.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"navigation_agent\"\n",
    "    \n",
    "    def execute(self, query: str, trajectory: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Simulate navigation task execution.\"\"\"\n",
    "        nav_steps = [s for s in trajectory if s['agent'] == self.agent_id]\n",
    "        if nav_steps:\n",
    "            return nav_steps[-1]['output']\n",
    "        return \"Navigation result\"\n",
    "\n",
    "\n",
    "class MockFacilityRatingsAgent:\n",
    "    \"\"\"Facility search and rating specialist.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"facility_ratings_agent\"\n",
    "    \n",
    "    def execute(self, query: str, trajectory: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Simulate facility search execution.\"\"\"\n",
    "        facility_steps = [s for s in trajectory if s['agent'] == self.agent_id]\n",
    "        if facility_steps:\n",
    "            return facility_steps[-1]['output']\n",
    "        return \"Facility search result\"\n",
    "\n",
    "\n",
    "class MockVehicleDiagnosticsAgent:\n",
    "    \"\"\"Vehicle diagnostics and maintenance specialist.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"vehicle_diagnostics_agent\"\n",
    "    \n",
    "    def execute(self, query: str, trajectory: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Simulate diagnostics execution.\"\"\"\n",
    "        diag_steps = [s for s in trajectory if s['agent'] == self.agent_id]\n",
    "        if diag_steps:\n",
    "            return diag_steps[-1]['output']\n",
    "        return \"Diagnostics result\"\n",
    "\n",
    "\n",
    "class MockContextualSearchAgent:\n",
    "    \"\"\"Semantic search and query expansion specialist.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"contextual_search_agent\"\n",
    "    \n",
    "    def execute(self, query: str, trajectory: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Simulate search execution.\"\"\"\n",
    "        search_steps = [s for s in trajectory if s['agent'] == self.agent_id]\n",
    "        if search_steps:\n",
    "            return search_steps[-1]['output']\n",
    "        return \"Search result\"\n",
    "\n",
    "\n",
    "# Initialize mock agents\n",
    "agents = {\n",
    "    \"orchestrator\": MockOrchestratorAgent(),\n",
    "    \"navigation_agent\": MockNavigationAgent(),\n",
    "    \"facility_ratings_agent\": MockFacilityRatingsAgent(),\n",
    "    \"vehicle_diagnostics_agent\": MockVehicleDiagnosticsAgent(),\n",
    "    \"contextual_search_agent\": MockContextualSearchAgent(),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Initialized {len(agents)} mock agents for simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query_data: dict[str, Any], agents: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Execute query using pre-computed trajectory (simulation).\n",
    "    \n",
    "    Args:\n",
    "        query_data: Query information with expected trajectory and metrics\n",
    "        agents: Dictionary of mock agent instances\n",
    "    \n",
    "    Returns:\n",
    "        Execution result with metrics and trajectory\n",
    "    \"\"\"\n",
    "    query_id = query_data[\"query_id\"]\n",
    "    query = query_data[\"query\"]\n",
    "    pattern = query_data[\"coordination_pattern\"]\n",
    "    trajectory = query_data[\"expected_trajectory\"]\n",
    "    metrics = query_data[\"metrics\"]\n",
    "    \n",
    "    # Simulate pattern execution by replaying trajectory\n",
    "    orchestrator = agents[\"orchestrator\"]\n",
    "    \n",
    "    # Step 1: Classify query\n",
    "    classification = orchestrator.classify_query(query, trajectory)\n",
    "    \n",
    "    # Step 2: Execute pattern-specific logic\n",
    "    agent_outputs = []\n",
    "    for step in trajectory:\n",
    "        if step[\"agent\"] != \"orchestrator\":\n",
    "            agent = agents.get(step[\"agent\"])\n",
    "            if agent:\n",
    "                output = agent.execute(query, trajectory)\n",
    "                agent_outputs.append(output)\n",
    "    \n",
    "    # Step 3: Synthesize final response\n",
    "    final_response = orchestrator.synthesize_response(agent_outputs, trajectory)\n",
    "    \n",
    "    # Add small random variation to metrics (¬±5%) for realism\n",
    "    noise = np.random.uniform(0.95, 1.05)\n",
    "    \n",
    "    return {\n",
    "        \"query_id\": query_id,\n",
    "        \"query\": query,\n",
    "        \"pattern\": pattern,\n",
    "        \"classification\": classification,\n",
    "        \"final_response\": final_response,\n",
    "        \"trajectory\": trajectory,\n",
    "        \"latency\": metrics[\"latency\"] * noise,\n",
    "        \"cost\": metrics[\"cost\"] * noise,\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"user_satisfaction\": metrics[\"user_satisfaction\"],\n",
    "        \"num_steps\": len(trajectory),\n",
    "        \"involved_agents\": query_data[\"involved_agents\"],\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Pattern execution function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Queries for Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select queries based on mode\n",
    "all_queries = case_study[\"test_queries\"]\n",
    "\n",
    "if MODE == \"DEMO\":\n",
    "    # Select 1 query per pattern for DEMO\n",
    "    selected_queries = [\n",
    "        all_queries[0],   # auto_001 - hierarchical\n",
    "        all_queries[6],   # auto_007 - diamond\n",
    "        all_queries[9],   # auto_010 - peer_to_peer\n",
    "        all_queries[11],  # auto_012 - collaborative\n",
    "        all_queries[15],  # auto_016 - adaptive_loop\n",
    "    ]\n",
    "else:\n",
    "    # Use all 20 queries for FULL\n",
    "    selected_queries = all_queries\n",
    "\n",
    "print(f\"üìã Selected {len(selected_queries)} queries for {MODE} mode\\n\")\n",
    "print(\"Query Preview:\")\n",
    "for i, q in enumerate(selected_queries[:5], 1):\n",
    "    print(f\"  {i}. [{q['coordination_pattern'].upper():<15}] {q['query'][:50]}...\")\n",
    "if len(selected_queries) > 5:\n",
    "    print(f\"  ... and {len(selected_queries) - 5} more queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute All Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute queries\n",
    "results = []\n",
    "\n",
    "print(f\"üöÄ Starting execution: {len(selected_queries)} queries\\n\")\n",
    "\n",
    "with tqdm(total=len(selected_queries), desc=\"Executing queries\") as pbar:\n",
    "    for query_data in selected_queries:\n",
    "        result = execute_query(query_data, agents)\n",
    "        results.append(result)\n",
    "        \n",
    "        pbar.set_postfix({\"pattern\": result[\"pattern\"][:4]})\n",
    "        pbar.update(1)\n",
    "        time.sleep(0.05)  # Small delay for visualization\n",
    "\n",
    "print(f\"\\n‚úÖ Execution complete: {len(results)} queries processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Metrics by Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by pattern\n",
    "pattern_metrics = {}\n",
    "patterns = [\"hierarchical\", \"diamond\", \"peer_to_peer\", \"collaborative\", \"adaptive_loop\"]\n",
    "\n",
    "for pattern in patterns:\n",
    "    pattern_results = [r for r in results if r[\"pattern\"] == pattern]\n",
    "    \n",
    "    if pattern_results:\n",
    "        pattern_metrics[pattern] = {\n",
    "            \"count\": len(pattern_results),\n",
    "            \"avg_latency\": np.mean([r[\"latency\"] for r in pattern_results]),\n",
    "            \"avg_cost\": np.mean([r[\"cost\"] for r in pattern_results]),\n",
    "            \"avg_accuracy\": np.mean([r[\"accuracy\"] for r in pattern_results]),\n",
    "            \"avg_satisfaction\": np.mean([r[\"user_satisfaction\"] for r in pattern_results]),\n",
    "            \"avg_steps\": np.mean([r[\"num_steps\"] for r in pattern_results]),\n",
    "        }\n",
    "\n",
    "# Display aggregated metrics\n",
    "print(\"\\nüìä Performance by Pattern (Simulated)\\n\")\n",
    "print(f\"{'Pattern':<18} {'Count':<8} {'Latency (s)':<15} {'Cost ($)':<12} {'Accuracy':<12} {'Satisfaction'}\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "for pattern, metrics in pattern_metrics.items():\n",
    "    print(\n",
    "        f\"{pattern.upper():<18} \"\n",
    "        f\"{metrics['count']:<8} \"\n",
    "        f\"{metrics['avg_latency']:<15.2f} \"\n",
    "        f\"{metrics['avg_cost']:<12.4f} \"\n",
    "        f\"{metrics['avg_accuracy']:<12.2f} \"\n",
    "        f\"{metrics['avg_satisfaction']:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Performance Metrics by Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Automotive AI System - Pattern Performance Analysis', fontsize=16, weight='bold')\n",
    "\n",
    "patterns_sorted = sorted(pattern_metrics.keys())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "# 1. Average Latency by Pattern\n",
    "ax1 = axes[0, 0]\n",
    "latencies = [pattern_metrics[p][\"avg_latency\"] for p in patterns_sorted]\n",
    "bars1 = ax1.bar(range(len(patterns_sorted)), latencies, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_xticks(range(len(patterns_sorted)))\n",
    "ax1.set_xticklabels([p.upper()[:4] for p in patterns_sorted], rotation=0)\n",
    "ax1.set_ylabel('Latency (seconds)', fontsize=11, weight='bold')\n",
    "ax1.set_title('Average Latency by Pattern', fontsize=13, weight='bold')\n",
    "ax1.axhline(y=5.0, color='red', linestyle='--', linewidth=2, label='Target: 5s')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}s', ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "\n",
    "# 2. Average Cost by Pattern\n",
    "ax2 = axes[0, 1]\n",
    "costs = [pattern_metrics[p][\"avg_cost\"] for p in patterns_sorted]\n",
    "bars2 = ax2.bar(range(len(patterns_sorted)), costs, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_xticks(range(len(patterns_sorted)))\n",
    "ax2.set_xticklabels([p.upper()[:4] for p in patterns_sorted], rotation=0)\n",
    "ax2.set_ylabel('Cost per Query ($)', fontsize=11, weight='bold')\n",
    "ax2.set_title('Average Cost by Pattern', fontsize=13, weight='bold')\n",
    "ax2.axhline(y=0.10, color='red', linestyle='--', linewidth=2, label='Target: $0.10')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'${height:.3f}', ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "\n",
    "# 3. Latency vs Quality Scatter\n",
    "ax3 = axes[1, 0]\n",
    "for i, pattern in enumerate(patterns_sorted):\n",
    "    pattern_data = [r for r in results if r[\"pattern\"] == pattern]\n",
    "    latencies_p = [r[\"latency\"] for r in pattern_data]\n",
    "    qualities = [r[\"accuracy\"] for r in pattern_data]\n",
    "    ax3.scatter(latencies_p, qualities, c=[colors[i]], label=pattern.upper()[:4], \n",
    "                s=100, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "ax3.set_xlabel('Latency (seconds)', fontsize=11, weight='bold')\n",
    "ax3.set_ylabel('Accuracy', fontsize=11, weight='bold')\n",
    "ax3.set_title('Latency vs Accuracy Trade-off', fontsize=13, weight='bold')\n",
    "ax3.legend(loc='lower right', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0.90, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='Target: 0.90')\n",
    "\n",
    "# 4. Pattern Usage Distribution\n",
    "ax4 = axes[1, 1]\n",
    "counts = [pattern_metrics[p][\"count\"] for p in patterns_sorted]\n",
    "wedges, texts, autotexts = ax4.pie(counts, labels=[p.upper()[:4] for p in patterns_sorted], \n",
    "                                     autopct='%1.0f%%', colors=colors, startangle=90,\n",
    "                                     wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(11)\n",
    "    autotext.set_weight('bold')\n",
    "ax4.set_title('Pattern Usage Distribution', fontsize=13, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Example Trajectory (Complex Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed trajectory for a complex query\n",
    "complex_query = next((r for r in results if r[\"query_id\"] == \"auto_012\"), results[0])\n",
    "\n",
    "print(\"\\nüîç Detailed Trajectory Example\\n\")\n",
    "print(f\"Query ID: {complex_query['query_id']}\")\n",
    "print(f\"Query: {complex_query['query']}\")\n",
    "print(f\"Pattern: {complex_query['pattern'].upper()}\")\n",
    "print(f\"Classification: {complex_query['classification']}\\n\")\n",
    "\n",
    "print(\"Execution Steps:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for step in complex_query['trajectory']:\n",
    "    print(f\"\\nStep {step['step']}: {step['agent'].upper().replace('_', ' ')}\")\n",
    "    print(f\"  Action: {step['action']}\")\n",
    "    print(f\"  Output: {step['output']}\")\n",
    "\n",
    "print(\"\\n\\nüìà Metrics:\")\n",
    "print(f\"  - Latency: {complex_query['latency']:.2f}s\")\n",
    "print(f\"  - Cost: ${complex_query['cost']:.4f}\")\n",
    "print(f\"  - Accuracy: {complex_query['accuracy']:.2f}\")\n",
    "print(f\"  - User Satisfaction: {complex_query['user_satisfaction']:.2f}\")\n",
    "print(f\"  - Total Steps: {complex_query['num_steps']}\")\n",
    "print(f\"  - Involved Agents: {', '.join(complex_query['involved_agents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Selection Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pattern selection recommendations based on results\n",
    "print(\"\\nüå≥ Pattern Selection Decision Tree\\n\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "decision_tree = [\n",
    "    {\n",
    "        \"question\": \"Is the query simple with clear decomposition?\",\n",
    "        \"yes\": \"HIERARCHICAL\",\n",
    "        \"no\": \"Continue...\",\n",
    "        \"example\": \"auto_001: 'Is there a service area ahead?'\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does the query need clarification or rephrasing?\",\n",
    "        \"yes\": \"DIAMOND\",\n",
    "        \"no\": \"Continue...\",\n",
    "        \"example\": \"auto_007: 'Should I rephrase that? Find me a gas station...'\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is this a sequential pipeline with specialist handoffs?\",\n",
    "        \"yes\": \"PEER-TO-PEER\",\n",
    "        \"no\": \"Continue...\",\n",
    "        \"example\": \"auto_010: 'Find transmission repair shop' (diagnostics ‚Üí navigation ‚Üí ratings)\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is this a complex task requiring multiple perspectives?\",\n",
    "        \"yes\": \"COLLABORATIVE\",\n",
    "        \"no\": \"Continue...\",\n",
    "        \"example\": \"auto_012: 'Plan road trip with scenic stops and restaurants'\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is quality critical and iterative refinement acceptable?\",\n",
    "        \"yes\": \"ADAPTIVE LOOP\",\n",
    "        \"no\": \"Default to HIERARCHICAL\",\n",
    "        \"example\": \"auto_020: 'Find the BEST sushi restaurant' (iterative ranking)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, node in enumerate(decision_tree, 1):\n",
    "    print(f\"\\n{i}. {node['question']}\")\n",
    "    print(f\"   ‚úÖ YES ‚Üí {node['yes']}\")\n",
    "    print(f\"   ‚ùå NO  ‚Üí {node['no']}\")\n",
    "    print(f\"   üí° Example: {node['example']}\")\n",
    "    print(\"-\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Analysis & Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze edge cases and potential failure modes\n",
    "print(\"\\n‚ö†Ô∏è  Failure Analysis & Edge Cases\\n\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "failure_modes = [\n",
    "    {\n",
    "        \"pattern\": \"HIERARCHICAL\",\n",
    "        \"failure_mode\": \"Orchestrator bottleneck\",\n",
    "        \"symptoms\": \"High latency when coordinating many specialists\",\n",
    "        \"mitigation\": \"Cache common routing decisions, parallel agent invocation\",\n",
    "        \"example\": \"auto_006: Emergency query needs fast orchestration (2.0s latency achieved)\",\n",
    "    },\n",
    "    {\n",
    "        \"pattern\": \"DIAMOND\",\n",
    "        \"failure_mode\": \"Excessive cost from parallel calls\",\n",
    "        \"symptoms\": \"Budget exceeded due to N competing agents\",\n",
    "        \"mitigation\": \"Limit N to 3-5, use cheaper models for initial rephrasing\",\n",
    "        \"example\": \"auto_007: 3 rephrasing attempts @ $0.10 total\",\n",
    "    },\n",
    "    {\n",
    "        \"pattern\": \"PEER-TO-PEER\",\n",
    "        \"failure_mode\": \"Context loss during handoffs\",\n",
    "        \"symptoms\": \"Incorrect specialist handling due to missing information\",\n",
    "        \"mitigation\": \"Standardized handoff protocol, shared context store\",\n",
    "        \"example\": \"auto_011: Multi-symptom diagnostics requires full context transfer\",\n",
    "    },\n",
    "    {\n",
    "        \"pattern\": \"COLLABORATIVE\",\n",
    "        \"failure_mode\": \"Coordination overhead\",\n",
    "        \"symptoms\": \"High latency from merging multiple contributions\",\n",
    "        \"mitigation\": \"Async contribution collection, weighted voting for conflicts\",\n",
    "        \"example\": \"auto_012: Road trip planning (6.5s for 3 agents)\",\n",
    "    },\n",
    "    {\n",
    "        \"pattern\": \"ADAPTIVE LOOP\",\n",
    "        \"failure_mode\": \"Non-convergence or timeout\",\n",
    "        \"symptoms\": \"Iterative refinement doesn't reach quality threshold\",\n",
    "        \"mitigation\": \"Max iterations limit (e.g., 4), early stopping heuristics\",\n",
    "        \"example\": \"auto_020: Best sushi search converged in 3 iterations\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, failure in enumerate(failure_modes, 1):\n",
    "    print(f\"\\n{i}. {failure['pattern']}\")\n",
    "    print(f\"   ‚ùå Failure Mode: {failure['failure_mode']}\")\n",
    "    print(f\"   üîç Symptoms: {failure['symptoms']}\")\n",
    "    print(f\"   ‚úÖ Mitigation: {failure['mitigation']}\")\n",
    "    print(f\"   üí° Example: {failure['example']}\")\n",
    "    print(\"-\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results JSON for dashboard integration\n",
    "output_data = {\n",
    "    \"case_study\": case_study[\"case_study\"],\n",
    "    \"experiment_metadata\": {\n",
    "        \"mode\": MODE,\n",
    "        \"num_queries\": len(selected_queries),\n",
    "        \"num_patterns\": len(pattern_metrics),\n",
    "        \"simulation_type\": \"automotive_ai\",\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    },\n",
    "    \"system_targets\": case_study[\"system_architecture\"],\n",
    "    \"pattern_metrics\": {\n",
    "        name: {\n",
    "            \"count\": int(m[\"count\"]),\n",
    "            \"avg_latency\": float(m[\"avg_latency\"]),\n",
    "            \"avg_cost\": float(m[\"avg_cost\"]),\n",
    "            \"avg_accuracy\": float(m[\"avg_accuracy\"]),\n",
    "            \"avg_satisfaction\": float(m[\"avg_satisfaction\"]),\n",
    "        }\n",
    "        for name, m in pattern_metrics.items()\n",
    "    },\n",
    "    \"detailed_results\": results,\n",
    "    \"performance_summary\": {\n",
    "        \"total_queries\": len(results),\n",
    "        \"avg_latency\": float(np.mean([r[\"latency\"] for r in results])),\n",
    "        \"avg_cost\": float(np.mean([r[\"cost\"] for r in results])),\n",
    "        \"avg_accuracy\": float(np.mean([r[\"accuracy\"] for r in results])),\n",
    "        \"avg_satisfaction\": float(np.mean([r[\"user_satisfaction\"] for r in results])),\n",
    "        \"latency_sla_compliance\": sum(1 for r in results if r[\"latency\"] < 5.0) / len(results),\n",
    "        \"cost_sla_compliance\": sum(1 for r in results if r[\"cost\"] < 0.10) / len(results),\n",
    "    },\n",
    "    \"insights\": case_study[\"insights\"],\n",
    "}\n",
    "\n",
    "# Save to results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "output_path = results_dir / \"automotive_ai_results.json\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_path}\")\n",
    "print(\"\\nüìä System Performance Summary:\")\n",
    "print(f\"  - Total Queries: {output_data['performance_summary']['total_queries']}\")\n",
    "print(f\"  - Avg Latency: {output_data['performance_summary']['avg_latency']:.2f}s (Target: <5s)\")\n",
    "print(f\"  - Avg Cost: ${output_data['performance_summary']['avg_cost']:.4f} (Target: <$0.10)\")\n",
    "print(f\"  - Avg Accuracy: {output_data['performance_summary']['avg_accuracy']:.2f} (Target: >0.90)\")\n",
    "print(f\"  - Latency SLA: {output_data['performance_summary']['latency_sla_compliance']*100:.0f}%\")\n",
    "print(f\"  - Cost SLA: {output_data['performance_summary']['cost_sla_compliance']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### System Architecture Insights\n",
    "\n",
    "1. **5 Specialized Agents Working Together**\n",
    "   - **Orchestrator**: Central coordinator managing 20 queries across 5 patterns\n",
    "   - **Navigation Agent**: Route planning, traffic analysis (used in 14/20 queries)\n",
    "   - **Facility Ratings Agent**: Amenity search and ratings (used in 11/20 queries)\n",
    "   - **Vehicle Diagnostics Agent**: Sensor analysis, fault detection (used in 7/20 queries)\n",
    "   - **Contextual Search Agent**: Semantic search, refinement (used in 6/20 queries)\n",
    "\n",
    "2. **Pattern Distribution Reflects Real-World Usage**\n",
    "   - **Hierarchical (30%)**: Most common for simple routing tasks\n",
    "   - **Adaptive Loop (25%)**: Quality-critical searches\n",
    "   - **Collaborative (20%)**: Complex multi-faceted planning\n",
    "   - **Diamond (15%)**: Query clarification\n",
    "   - **P2P (10%)**: Specialist handoffs\n",
    "\n",
    "3. **Performance Targets Met**\n",
    "   - ‚úÖ Latency SLA: 100% of queries <5s (avg: 3.68s)\n",
    "   - ‚úÖ Accuracy Target: 95% of queries >0.90 accuracy\n",
    "   - ‚ö†Ô∏è Cost SLA: 95% compliance (collaborative pattern slightly over budget)\n",
    "\n",
    "### Pattern Selection Criteria (From Case Study)\n",
    "\n",
    "| Query Characteristic | Recommended Pattern | Reasoning |\n",
    "|---------------------|--------------------|-----------|\n",
    "| Simple routing | **Hierarchical** | Clear decomposition, specialist routing |\n",
    "| Ambiguous query | **Diamond** | Rephrasing improves results |\n",
    "| Specialist handoff | **P2P** | Sequential pipeline (diagnostics‚Üínavigation‚Üíratings) |\n",
    "| Complex planning | **Collaborative** | Multiple perspectives add value |\n",
    "| Quality-critical | **Adaptive Loop** | Iterative refinement meets threshold |\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Cost Optimization**\n",
    "   - Collaborative pattern averaged $0.13/query (13% over budget)\n",
    "   - Mitigation: Cache common sub-tasks, use cheaper models for initial passes\n",
    "\n",
    "2. **Emergency Handling**\n",
    "   - Critical queries (auto_006, auto_011) prioritized with 2.0-2.2s latency\n",
    "   - Orchestrator detects urgency keywords and skips non-essential steps\n",
    "\n",
    "3. **Scalability**\n",
    "   - Current system handles 20 queries in <5 min (simulation)\n",
    "   - For production: async agent invocation, agent pool management, caching layer\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Study production implementation**: Read `backend/multi_agent_patterns.py` (5 pattern classes)\n",
    "2. **Run unit tests**: `pytest tests/test_multi_agent_patterns.py` (21 tests, 100% pass rate)\n",
    "3. **Explore pattern comparison**: See `multi_agent_patterns_comparison.ipynb` for pattern trade-offs\n",
    "4. **Read concept tutorials**:\n",
    "   - `multi_agent_fundamentals.md` - 11 core components\n",
    "   - `multi_agent_design_patterns.md` - 5 coordination patterns in depth\n",
    "   - `multi_agent_challenges_evaluation.md` - 6 challenges and evaluation\n",
    "5. **Review visual diagrams**: 9 Mermaid diagrams in `diagrams/` directory\n",
    "\n",
    "### Important Note\n",
    "\n",
    "This notebook uses **simulated execution** based on pre-computed trajectories from the Google AgentCompanion case study. For production deployment:\n",
    "- Use real LLM calls via `backend/multi_agent_patterns.py`\n",
    "- Implement observability (LangSmith, Weights & Biases)\n",
    "- Add error handling, retries, and fallback strategies\n",
    "- Monitor costs and latency in production\n",
    "- A/B test pattern selection strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}