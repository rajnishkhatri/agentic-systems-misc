# Lesson 14 Implementation Guide: Complete Deep Dive for Tasks 3.7-3.16

**Generated:** 2025-11-12
**Status:** Ready for implementation
**Tasks Covered:** 3.7 (tool_call_benchmark.json) ‚Üí 3.16 (multi_agent_framework.py tests)

---

## Executive Summary

After exhaustively analyzing the codebase, this guide provides **complete implementation patterns** extracted from:
- **backend/exact_evaluation.py** (496 lines) - 7 defensive evaluation functions
- **backend/ai_judge_framework.py** (528 lines) - Complete ABC pattern with 3 judge implementations
- **lesson-14/agent_planning_benchmark.json** (3306 lines) - 100 planning validation test cases
- **lesson-14/*.md** (3000+ lines) - Complete tutorials for ReAct, Reflexion, Multi-Agent patterns
- **tests/test_exact_evaluation.py** (436 lines) - 40+ TDD test patterns

**Key Findings:**
1. **Comprehensive benchmark data** - agent_planning_benchmark.json provides complete schema for Tasks 3.7-3.8
2. **Complete pattern library** - 3 documented patterns ready for use (TDD, ABC, ThreadPoolExecutor)
3. **Missing dependencies** - None (all required libraries in pyproject.toml)
4. **Defensive coding pattern** - 5-step template used throughout backend

---

## Part 1: Task 3.7 - Tool Call Benchmark (150 cases)

### Goal
Create benchmark for tool call validation failures testing schema compliance, type checking, and value constraints.

### JSON Schema

```json
{
  "version": "1.0",
  "created": "2025-11-12",
  "description": "Tool Call Validation Benchmark - 150 test cases for function calling errors",
  "statistics": {
    "total_cases": 150,
    "correct_calls": 50,
    "wrong_tool": 30,
    "missing_required_args": 25,
    "invalid_arg_types": 25,
    "invalid_arg_values": 20
  },
  "available_tools": [
    {
      "name": "search_recipes",
      "description": "Search recipe database",
      "parameters": {
        "ingredients": {"type": "list[str]", "required": false},
        "dietary_restrictions": {"type": "list[str]", "required": false},
        "cuisine": {"type": "str", "required": false},
        "max_cook_time": {"type": "int", "required": false, "min": 1, "max": 240}
      }
    },
    {
      "name": "get_recipe_details",
      "description": "Get full recipe by ID",
      "parameters": {
        "recipe_id": {"type": "int", "required": true}
      }
    },
    {
      "name": "add_to_shopping_list",
      "description": "Add ingredients to shopping list",
      "parameters": {
        "ingredients": {"type": "list[str]", "required": true}
      }
    }
  ],
  "test_cases": [
    {
      "id": "tool_001",
      "task": "Find vegan recipes",
      "tool_call": {
        "tool": "search_recipes",
        "args": {"dietary_restrictions": ["vegan"]}
      },
      "labels": {
        "is_valid": true,
        "tool_selection": "CORRECT",
        "args_validation": "VALID"
      },
      "difficulty": "easy"
    },
    {
      "id": "tool_002_missing_required",
      "task": "Get recipe details",
      "tool_call": {
        "tool": "get_recipe_details",
        "args": {}
      },
      "labels": {
        "is_valid": false,
        "tool_selection": "CORRECT",
        "args_validation": "MISSING_REQUIRED",
        "failure_reason": "Missing required arg: recipe_id"
      },
      "difficulty": "easy"
    },
    {
      "id": "tool_003_type_error",
      "task": "Search recipes with cooking time",
      "tool_call": {
        "tool": "search_recipes",
        "args": {"max_cook_time": "30 minutes"}
      },
      "labels": {
        "is_valid": false,
        "tool_selection": "CORRECT",
        "args_validation": "TYPE_ERROR",
        "failure_reason": "max_cook_time expects int, got str"
      },
      "difficulty": "easy"
    },
    {
      "id": "tool_004_value_error",
      "task": "Search recipes with invalid time",
      "tool_call": {
        "tool": "search_recipes",
        "args": {"max_cook_time": -10}
      },
      "labels": {
        "is_valid": false,
        "tool_selection": "CORRECT",
        "args_validation": "VALUE_ERROR",
        "failure_reason": "max_cook_time must be between 1 and 240"
      },
      "difficulty": "medium"
    },
    {
      "id": "tool_005_wrong_tool",
      "task": "Find Italian recipes",
      "tool_call": {
        "tool": "search_web",
        "args": {"query": "Italian recipes"}
      },
      "gold_call": {
        "tool": "search_recipes",
        "args": {"cuisine": "Italian"}
      },
      "labels": {
        "is_valid": false,
        "tool_selection": "WRONG_TOOL",
        "failure_reason": "Should use search_recipes (internal DB), not search_web (external)"
      },
      "difficulty": "easy"
    }
  ]
}
```

### Generator Script

**File:** `lesson-14/data/generate_tool_call_benchmark.py`

```python
#!/usr/bin/env python3
"""Generate tool_call_benchmark.json with 150 tool calling validation cases."""

import json
from typing import Any

TOOLS = [
    {
        "name": "search_recipes",
        "description": "Search recipe database",
        "parameters": {
            "ingredients": {"type": "list[str]", "required": False},
            "dietary_restrictions": {"type": "list[str]", "required": False},
            "cuisine": {"type": "str", "required": False},
            "meal_type": {"type": "str", "required": False, "options": ["breakfast", "lunch", "dinner", "dessert", "snack"]},
            "max_cook_time": {"type": "int", "required": False, "min": 1, "max": 240},
            "max_results": {"type": "int", "required": False, "default": 10, "min": 1, "max": 100}
        }
    },
    {
        "name": "get_recipe_details",
        "description": "Get full recipe by ID",
        "parameters": {
            "recipe_id": {"type": "int", "required": True}
        }
    },
    {
        "name": "add_to_shopping_list",
        "description": "Add ingredients to shopping list",
        "parameters": {
            "ingredients": {"type": "list[str]", "required": True}
        }
    },
    {
        "name": "check_ingredient_availability",
        "description": "Check if ingredient is in stock",
        "parameters": {
            "ingredient": {"type": "str", "required": True}
        }
    },
    {
        "name": "get_nutrition_info",
        "description": "Get nutritional information",
        "parameters": {
            "food": {"type": "str", "required": True}
        }
    },
    {
        "name": "get_user_preferences",
        "description": "Retrieve user dietary preferences",
        "parameters": {}
    },
    {
        "name": "search_web",
        "description": "Search external web",
        "parameters": {
            "query": {"type": "str", "required": True}
        }
    },
    {
        "name": "get_gita_verses",
        "description": "Search Bhagavad Gita verses",
        "parameters": {
            "chapter": {"type": "int", "required": False, "min": 1, "max": 18},
            "verse": {"type": "int", "required": False},
            "keyword": {"type": "str", "required": False},
            "max_results": {"type": "int", "required": False, "default": 5}
        }
    }
]


def generate_correct_calls() -> list[dict]:
    """Generate 50 correct tool calls."""
    cases = []

    # Recipe searches with various filters
    diets = ["vegan", "vegetarian", "gluten-free", "keto", "paleo"]
    cuisines = ["Italian", "Chinese", "Mexican", "Indian", "Thai"]

    for i in range(25):
        cases.append({
            "id": f"tool_{i+1:03d}",
            "task": f"Find {diets[i % len(diets)]} recipes",
            "tool_call": {
                "tool": "search_recipes",
                "args": {"dietary_restrictions": [diets[i % len(diets)]]}
            },
            "labels": {
                "is_valid": True,
                "tool_selection": "CORRECT",
                "args_validation": "VALID"
            },
            "difficulty": "easy"
        })

    for i in range(25, 40):
        cases.append({
            "id": f"tool_{i+1:03d}",
            "task": f"Find {cuisines[i % len(cuisines)]} recipes",
            "tool_call": {
                "tool": "search_recipes",
                "args": {"cuisine": cuisines[i % len(cuisines)]}
            },
            "labels": {
                "is_valid": True,
                "tool_selection": "CORRECT",
                "args_validation": "VALID"
            },
            "difficulty": "easy"
        })

    # Get recipe details
    for i in range(40, 45):
        cases.append({
            "id": f"tool_{i+1:03d}",
            "task": f"Get recipe {i+1} details",
            "tool_call": {
                "tool": "get_recipe_details",
                "args": {"recipe_id": i + 1}
            },
            "labels": {
                "is_valid": True,
                "tool_selection": "CORRECT",
                "args_validation": "VALID"
            },
            "difficulty": "easy"
        })

    # Gita verses
    for i in range(45, 50):
        cases.append({
            "id": f"tool_{i+1:03d}",
            "task": f"Get Gita Chapter {i-43} verses",
            "tool_call": {
                "tool": "get_gita_verses",
                "args": {"chapter": i - 43}
            },
            "labels": {
                "is_valid": True,
                "tool_selection": "CORRECT",
                "args_validation": "VALID"
            },
            "difficulty": "easy"
        })

    return cases


def generate_wrong_tool_calls() -> list[dict]:
    """Generate 30 wrong tool selection cases."""
    cases = []

    diets = ["vegan", "vegetarian", "gluten-free", "keto", "paleo", "dairy-free", "nut-free"]

    for i in range(30):
        cases.append({
            "id": f"tool_{51+i:03d}_wrong_tool",
            "task": f"Find {diets[i % len(diets)]} recipes",
            "tool_call": {
                "tool": "search_web",
                "args": {"query": f"{diets[i % len(diets)]} recipes"}
            },
            "gold_call": {
                "tool": "search_recipes",
                "args": {"dietary_restrictions": [diets[i % len(diets)]]}
            },
            "labels": {
                "is_valid": False,
                "tool_selection": "WRONG_TOOL",
                "failure_reason": "Should use search_recipes (internal DB), not search_web (external)"
            },
            "difficulty": "easy"
        })

    return cases


def generate_missing_required_args() -> list[dict]:
    """Generate 25 missing required argument cases."""
    cases = []

    # Missing recipe_id
    for i in range(15):
        cases.append({
            "id": f"tool_{81+i:03d}_missing_required",
            "task": "Get recipe details",
            "tool_call": {
                "tool": "get_recipe_details",
                "args": {}
            },
            "labels": {
                "is_valid": False,
                "tool_selection": "CORRECT",
                "args_validation": "MISSING_REQUIRED",
                "failure_reason": "Missing required arg: recipe_id"
            },
            "difficulty": "easy"
        })

    # Missing ingredients for shopping list
    for i in range(15, 25):
        cases.append({
            "id": f"tool_{81+i:03d}_missing_required",
            "task": "Add to shopping list",
            "tool_call": {
                "tool": "add_to_shopping_list",
                "args": {}
            },
            "labels": {
                "is_valid": False,
                "tool_selection": "CORRECT",
                "args_validation": "MISSING_REQUIRED",
                "failure_reason": "Missing required arg: ingredients"
            },
            "difficulty": "easy"
        })

    return cases


def generate_invalid_type_args() -> list[dict]:
    """Generate 25 invalid type cases."""
    cases = []

    type_errors = [
        ("max_cook_time", "30 minutes", "Should be int, not str"),
        ("max_results", "ten", "Should be int, not str"),
        ("ingredients", "pasta", "Should be list[str], not str"),
        ("dietary_restrictions", "vegan", "Should be list[str], not str"),
        ("recipe_id", "123", "Should be int, not str"),
    ]

    for i in range(25):
        arg_name, wrong_value, reason = type_errors[i % len(type_errors)]
        tool_name = "search_recipes" if arg_name in ["max_cook_time", "max_results", "ingredients", "dietary_restrictions"] else "get_recipe_details"

        cases.append({
            "id": f"tool_{106+i:03d}_type_error",
            "task": f"Call with wrong {arg_name} type",
            "tool_call": {
                "tool": tool_name,
                "args": {arg_name: wrong_value}
            },
            "labels": {
                "is_valid": False,
                "tool_selection": "CORRECT",
                "args_validation": "TYPE_ERROR",
                "failure_reason": reason
            },
            "difficulty": "medium"
        })

    return cases


def generate_invalid_value_args() -> list[dict]:
    """Generate 20 invalid value cases."""
    cases = []

    # Invalid max_cook_time (out of range)
    for i in range(10):
        cases.append({
            "id": f"tool_{131+i:03d}_value_error",
            "task": "Search with invalid cook time",
            "tool_call": {
                "tool": "search_recipes",
                "args": {"max_cook_time": -10 - i}
            },
            "labels": {
                "is_valid": False,
                "tool_selection": "CORRECT",
                "args_validation": "VALUE_ERROR",
                "failure_reason": "max_cook_time must be between 1 and 240"
            },
            "difficulty": "medium"
        })

    # Invalid meal_type (not in options)
    invalid_meals = ["brunch", "snack-time", "teatime", "supper", "appetizer", "beverage", "course", "side", "main", "entree"]
    for i in range(10):
        cases.append({
            "id": f"tool_{141+i:03d}_value_error",
            "task": "Search with invalid meal type",
            "tool_call": {
                "tool": "search_recipes",
                "args": {"meal_type": invalid_meals[i]}
            },
            "labels": {
                "is_valid": False,
                "tool_selection": "CORRECT",
                "args_validation": "VALUE_ERROR",
                "failure_reason": f"meal_type must be one of: breakfast, lunch, dinner, dessert, snack"
            },
            "difficulty": "medium"
        })

    return cases


def generate_benchmark() -> dict[str, Any]:
    """Generate complete tool call benchmark."""
    benchmark = {
        "version": "1.0",
        "created": "2025-11-12",
        "description": "Tool Call Validation Benchmark - 150 test cases for function calling errors",
        "statistics": {
            "total_cases": 150,
            "correct_calls": 50,
            "wrong_tool": 30,
            "missing_required_args": 25,
            "invalid_arg_types": 25,
            "invalid_arg_values": 20
        },
        "available_tools": TOOLS,
        "test_cases": []
    }

    # Generate all test case categories
    benchmark["test_cases"].extend(generate_correct_calls())
    benchmark["test_cases"].extend(generate_wrong_tool_calls())
    benchmark["test_cases"].extend(generate_missing_required_args())
    benchmark["test_cases"].extend(generate_invalid_type_args())
    benchmark["test_cases"].extend(generate_invalid_value_args())

    return benchmark


if __name__ == "__main__":
    benchmark = generate_benchmark()

    # Validate count
    assert len(benchmark["test_cases"]) == 150, f"Expected 150 cases, got {len(benchmark['test_cases'])}"

    # Save to file
    output_path = "lesson-14/data/tool_call_benchmark.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(benchmark, f, indent=2, ensure_ascii=False)

    print(f"‚úÖ Generated {len(benchmark['test_cases'])} tool call test cases")
    print(f"   Saved to: {output_path}")
    print()
    print("üìä Statistics:")
    for key, value in benchmark["statistics"].items():
        print(f"   - {key}: {value}")
```

### Implementation Checklist

- [ ] Create generator script `lesson-14/data/generate_tool_call_benchmark.py`
- [ ] Run script: `python3 lesson-14/data/generate_tool_call_benchmark.py`
- [ ] Validate JSON structure: `python3 -m json.tool lesson-14/data/tool_call_benchmark.json > /dev/null`
- [ ] Verify test case count: 150 total
- [ ] Verify distribution: 50 correct, 30 wrong tool, 25 missing args, 25 type errors, 20 value errors
- [ ] Mark Task 3.7 complete in `tasks/tasks-0005-prd-rag-agent-evaluation-tutorial-system.md`

---

## Part 2: Task 3.8 - Efficiency Benchmark (100 cases)

### Goal
Create benchmark comparing optimal vs suboptimal plans to test efficiency scoring.

### JSON Schema

```json
{
  "version": "1.0",
  "created": "2025-11-12",
  "description": "Plan Efficiency Benchmark - 100 test cases comparing optimal vs suboptimal plans",
  "statistics": {
    "total_cases": 100,
    "optimal_only": 40,
    "suboptimal_redundant": 30,
    "suboptimal_unnecessary": 20,
    "suboptimal_wrong_order": 10
  },
  "test_cases": [
    {
      "id": "eff_001",
      "task": "Find vegan recipes with available ingredients",
      "optimal_plan": {
        "steps": [
          {"step": 1, "tool": "get_user_preferences", "rationale": "Get pantry"},
          {"step": 2, "tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"], "ingredients": "{{pantry}}"}, "rationale": "Search with constraints"}
        ],
        "step_count": 2,
        "efficiency_score": 1.0
      },
      "suboptimal_plan": {
        "steps": [
          {"step": 1, "tool": "get_user_preferences"},
          {"step": 2, "tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"]}},
          {"step": 3, "tool": "get_user_preferences"},
          {"step": 4, "tool": "filter_by_ingredients"}
        ],
        "step_count": 4,
        "efficiency_score": 0.5,
        "inefficiency_type": "REDUNDANT_CALLS",
        "inefficiency_reason": "Redundant get_user_preferences (step 3) and unnecessary filter step (step 4)"
      },
      "labels": {
        "optimal_is_better": true,
        "efficiency_difference": 0.5,
        "optimal_step_count": 2,
        "suboptimal_step_count": 4
      },
      "difficulty": "medium"
    }
  ]
}
```

### Efficiency Scoring Formula

```python
def calculate_efficiency(plan: dict, optimal_length: int) -> float:
    """Calculate plan efficiency score (0.0-1.0).

    Formula:
        efficiency = optimal_length / actual_length if actual >= optimal
        efficiency = 1.0 if actual <= optimal

    Additional penalties:
        - Redundant tool calls: -0.2 per duplicate
        - Unnecessary steps: -0.1 per unnecessary step

    Args:
        plan: Agent plan with steps
        optimal_length: Minimum steps needed

    Returns:
        Efficiency score (0.0-1.0)
    """
    actual_length = len(plan["steps"])

    # Base efficiency
    if actual_length <= optimal_length:
        efficiency = 1.0
    else:
        efficiency = optimal_length / actual_length

    # Detect redundancy
    tools_used = [step["tool"] for step in plan["steps"]]
    unique_tools = set(tools_used)
    redundancy_penalty = (len(tools_used) - len(unique_tools)) * 0.2

    # Final score
    efficiency = max(0.0, efficiency - redundancy_penalty)

    return round(efficiency, 2)
```

### Implementation Checklist

- [ ] Create generator script `lesson-14/data/generate_efficiency_benchmark.py`
- [ ] Generate 40 optimal-only cases (single correct plan)
- [ ] Generate 30 redundant cases (duplicate tool calls)
- [ ] Generate 20 unnecessary step cases
- [ ] Generate 10 wrong order cases
- [ ] Calculate efficiency scores using formula
- [ ] Validate JSON structure
- [ ] Mark Task 3.8 complete

---

## Part 3: Task 3.9 - Test File (TDD RED Phase)

### Goal
Write tests BEFORE implementing `backend/agent_evaluation.py` (TDD RED phase)

### File Structure

**File:** `tests/test_agent_evaluation.py`

```python
"""
Tests for backend/agent_evaluation.py

Following TDD: Write tests FIRST, then implement to make them pass.
Test naming convention: test_should_[result]_when_[condition]

Test Coverage:
- validate_tool_call() - 10+ tests
- validate_plan_correctness() - 8+ tests
- validate_plan_completeness() - 5+ tests
- calculate_plan_efficiency() - 5+ tests
- PlanValidator class - 5+ tests
- ToolCallValidator class - 3+ tests
- PlanEvaluator class - 3+ tests
Total: 40+ tests
"""

import pytest
from typing import Any
from backend.agent_evaluation import (
    validate_tool_call,
    validate_plan_correctness,
    validate_plan_completeness,
    calculate_plan_efficiency,
    PlanValidator,
    ToolCallValidator,
    PlanEvaluator
)


# ============================================================================
# Tool Call Validation Tests (10+ tests)
# ============================================================================

class TestToolCallValidation:
    """Test tool call validation functions."""

    def test_should_validate_correct_tool_call_when_all_args_valid(self) -> None:
        """Test validation passes for correct tool call."""
        # Given: Valid tool call
        tool_call = {
            "tool": "search_recipes",
            "args": {"dietary_restrictions": ["vegan"]}
        }
        tool_schema = {
            "name": "search_recipes",
            "parameters": {
                "dietary_restrictions": {"type": "list[str]", "required": False}
            }
        }

        # When: Validating
        result = validate_tool_call(tool_call, tool_schema)

        # Then: Should pass
        assert result["is_valid"] is True
        assert result["errors"] == []

    def test_should_reject_when_missing_required_arg(self) -> None:
        """Test validation fails when required arg missing."""
        tool_call = {"tool": "get_recipe_details", "args": {}}
        tool_schema = {
            "name": "get_recipe_details",
            "parameters": {"recipe_id": {"type": "int", "required": True}}
        }

        result = validate_tool_call(tool_call, tool_schema)

        assert result["is_valid"] is False
        assert any("recipe_id" in error for error in result["errors"])

    def test_should_reject_when_arg_type_wrong(self) -> None:
        """Test validation fails when arg has wrong type."""
        tool_call = {
            "tool": "search_recipes",
            "args": {"max_cook_time": "30 minutes"}  # Should be int
        }
        tool_schema = {
            "name": "search_recipes",
            "parameters": {"max_cook_time": {"type": "int", "required": False}}
        }

        result = validate_tool_call(tool_call, tool_schema)

        assert result["is_valid"] is False
        assert any("type" in error.lower() for error in result["errors"])

    def test_should_reject_when_arg_value_out_of_range(self) -> None:
        """Test validation fails when arg value violates constraints."""
        tool_call = {
            "tool": "search_recipes",
            "args": {"max_cook_time": -10}
        }
        tool_schema = {
            "name": "search_recipes",
            "parameters": {
                "max_cook_time": {"type": "int", "required": False, "min": 1, "max": 240}
            }
        }

        result = validate_tool_call(tool_call, tool_schema)

        assert result["is_valid"] is False
        assert any("range" in error.lower() or "between" in error.lower() for error in result["errors"])

    def test_should_raise_error_for_invalid_tool_call_input(self) -> None:
        """Test validation raises TypeError for non-dict input."""
        with pytest.raises(TypeError, match="tool_call must be a dictionary"):
            validate_tool_call("not a dict", {})  # type: ignore

    def test_should_raise_error_for_invalid_schema_input(self) -> None:
        """Test validation raises TypeError for non-dict schema."""
        with pytest.raises(TypeError, match="tool_schema must be a dictionary"):
            validate_tool_call({}, "not a dict")  # type: ignore

    def test_should_raise_error_when_tool_call_missing_tool_field(self) -> None:
        """Test validation raises ValueError when tool_call missing 'tool' field."""
        with pytest.raises(ValueError, match="tool_call must have 'tool' field"):
            validate_tool_call({"args": {}}, {"name": "test"})

    def test_should_raise_error_when_tool_call_missing_args_field(self) -> None:
        """Test validation raises ValueError when tool_call missing 'args' field."""
        with pytest.raises(ValueError, match="tool_call must have 'args' field"):
            validate_tool_call({"tool": "test"}, {"name": "test"})

    def test_should_reject_when_tool_name_mismatch(self) -> None:
        """Test validation fails when tool name doesn't match schema."""
        tool_call = {"tool": "wrong_tool", "args": {}}
        tool_schema = {"name": "search_recipes", "parameters": {}}

        result = validate_tool_call(tool_call, tool_schema)

        assert result["is_valid"] is False
        assert any("mismatch" in error.lower() for error in result["errors"])

    def test_should_reject_when_unknown_argument_provided(self) -> None:
        """Test validation fails when unknown argument provided."""
        tool_call = {
            "tool": "search_recipes",
            "args": {"unknown_param": "value"}
        }
        tool_schema = {
            "name": "search_recipes",
            "parameters": {"cuisine": {"type": "str", "required": False}}
        }

        result = validate_tool_call(tool_call, tool_schema)

        assert result["is_valid"] is False
        assert any("unknown" in error.lower() for error in result["errors"])


# ============================================================================
# Plan Correctness Validation Tests (8+ tests)
# ============================================================================

class TestPlanCorrectnessValidation:
    """Test plan correctness validation."""

    def test_should_validate_correct_plan_when_achieves_goal(self) -> None:
        """Test validation passes for correct plan."""
        plan = {
            "goal": "Find vegan pasta recipes",
            "steps": [
                {"step": 1, "tool": "search_recipes", "args": {"ingredients": ["pasta"], "dietary_restrictions": ["vegan"]}}
            ]
        }

        result = validate_plan_correctness(plan)

        assert result["is_correct"] is True
        assert result["score"] >= 0.8

    def test_should_reject_when_wrong_tool_selected(self) -> None:
        """Test validation fails for wrong tool selection."""
        plan = {
            "goal": "Find Italian recipes",
            "steps": [{"step": 1, "tool": "search_web", "args": {"query": "Italian recipes"}}]
        }
        gold_plan = {
            "steps": [{"step": 1, "tool": "search_recipes", "args": {"cuisine": "Italian"}}]
        }

        result = validate_plan_correctness(plan, gold_plan)

        assert result["is_correct"] is False
        assert "tool_selection" in result["failure_reason"].lower()

    def test_should_reject_when_plan_has_no_steps(self) -> None:
        """Test validation fails for empty plan."""
        plan = {"goal": "Find recipes", "steps": []}

        result = validate_plan_correctness(plan)

        assert result["is_correct"] is False
        assert result["score"] == 0.0

    def test_should_raise_error_for_invalid_plan_input(self) -> None:
        """Test validation raises TypeError for non-dict plan."""
        with pytest.raises(TypeError, match="plan must be a dictionary"):
            validate_plan_correctness("not a dict")  # type: ignore

    def test_should_raise_error_when_plan_missing_goal(self) -> None:
        """Test validation raises ValueError when plan missing 'goal' field."""
        with pytest.raises(ValueError, match="plan must have 'goal' field"):
            validate_plan_correctness({"steps": []})

    def test_should_raise_error_when_plan_missing_steps(self) -> None:
        """Test validation raises ValueError when plan missing 'steps' field."""
        with pytest.raises(ValueError, match="plan must have 'steps' list"):
            validate_plan_correctness({"goal": "test"})

    def test_should_compare_tool_selection_with_gold_plan(self) -> None:
        """Test validation compares tool selection against gold plan."""
        plan = {
            "goal": "Find vegan recipes",
            "steps": [{"step": 1, "tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"]}}]
        }
        gold_plan = {
            "steps": [{"step": 1, "tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"]}}]
        }

        result = validate_plan_correctness(plan, gold_plan)

        assert result["is_correct"] is True
        assert result["score"] == 1.0

    def test_should_reject_when_tools_dont_match_gold_plan(self) -> None:
        """Test validation fails when tools don't match gold plan."""
        plan = {
            "goal": "Find recipes and add to list",
            "steps": [
                {"step": 1, "tool": "search_recipes"},
                {"step": 2, "tool": "filter_results"}
            ]
        }
        gold_plan = {
            "steps": [
                {"step": 1, "tool": "search_recipes"},
                {"step": 2, "tool": "add_to_shopping_list"}
            ]
        }

        result = validate_plan_correctness(plan, gold_plan)

        assert result["is_correct"] is False


# ============================================================================
# Plan Completeness Tests (5+ tests)
# ============================================================================

class TestPlanCompleteness:
    """Test plan completeness validation."""

    def test_should_validate_complete_plan_when_all_subtasks_covered(self) -> None:
        """Test validation passes for complete plan."""
        plan = {
            "goal": "Find recipes and add to shopping list",
            "steps": [
                {"step": 1, "tool": "search_recipes"},
                {"step": 2, "tool": "get_recipe_details"},
                {"step": 3, "tool": "add_to_shopping_list"}
            ]
        }

        result = validate_plan_completeness(plan)

        assert result["is_complete"] is True
        assert result["completeness_score"] == 1.0

    def test_should_reject_when_multi_task_goal_has_single_step(self) -> None:
        """Test validation fails for incomplete plan (multi-task goal with single step)."""
        plan = {
            "goal": "Find recipes and add to shopping list",
            "steps": [{"step": 1, "tool": "search_recipes"}]
        }

        result = validate_plan_completeness(plan)

        assert result["is_complete"] is False
        assert result["completeness_score"] < 1.0

    def test_should_detect_missing_shopping_list_step(self) -> None:
        """Test validation detects missing shopping list step."""
        plan = {
            "goal": "Find Italian recipes and add to shopping list",
            "steps": [
                {"step": 1, "tool": "search_recipes"},
                {"step": 2, "tool": "get_recipe_details"}
            ]
        }

        result = validate_plan_completeness(plan)

        assert result["is_complete"] is False
        assert "shopping" in result["missing_tasks"].lower()

    def test_should_raise_error_for_invalid_plan_input(self) -> None:
        """Test validation raises TypeError for non-dict plan."""
        with pytest.raises(TypeError, match="plan must be a dictionary"):
            validate_plan_completeness("not a dict")  # type: ignore

    def test_should_accept_single_step_for_simple_goal(self) -> None:
        """Test validation passes for single-step plan with simple goal."""
        plan = {
            "goal": "Find Italian recipes",
            "steps": [{"step": 1, "tool": "search_recipes"}]
        }

        result = validate_plan_completeness(plan)

        assert result["is_complete"] is True


# ============================================================================
# Plan Efficiency Tests (5+ tests)
# ============================================================================

class TestPlanEfficiency:
    """Test plan efficiency calculations."""

    def test_should_score_optimal_plan_as_1_0(self) -> None:
        """Test optimal plan gets perfect efficiency score."""
        plan = {
            "goal": "Find vegan recipes",
            "steps": [
                {"step": 1, "tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"]}}
            ]
        }

        efficiency = calculate_plan_efficiency(plan)

        assert efficiency == 1.0

    def test_should_penalize_redundant_steps(self) -> None:
        """Test efficiency decreases with redundant steps."""
        plan = {
            "goal": "Find vegan recipes",
            "steps": [
                {"step": 1, "tool": "get_user_preferences"},
                {"step": 2, "tool": "search_recipes"},
                {"step": 3, "tool": "get_user_preferences"}  # Redundant
            ]
        }

        efficiency = calculate_plan_efficiency(plan)

        assert efficiency < 1.0
        assert efficiency >= 0.5

    def test_should_penalize_unnecessary_steps(self) -> None:
        """Test efficiency decreases with unnecessary steps."""
        plan = {
            "goal": "Find Italian recipes",
            "steps": [
                {"step": 1, "tool": "get_trending_cuisines"},  # Unnecessary
                {"step": 2, "tool": "search_recipes"}
            ]
        }

        efficiency = calculate_plan_efficiency(plan)

        assert efficiency < 1.0

    def test_should_raise_error_for_invalid_plan_input(self) -> None:
        """Test validation raises TypeError for non-dict plan."""
        with pytest.raises(TypeError, match="plan must be a dictionary"):
            calculate_plan_efficiency("not a dict")  # type: ignore

    def test_should_raise_error_when_plan_has_no_steps(self) -> None:
        """Test validation raises ValueError for empty plan."""
        with pytest.raises(ValueError, match="plan must have at least one step"):
            calculate_plan_efficiency({"goal": "test", "steps": []})


# ============================================================================
# PlanValidator Class Tests (5+ tests)
# ============================================================================

class TestPlanValidator:
    """Test PlanValidator class."""

    def test_should_initialize_with_tools(self) -> None:
        """Test PlanValidator initialization."""
        tools = [{"name": "search_recipes", "parameters": {}}]

        validator = PlanValidator(tools)

        assert validator.tools == tools

    def test_should_raise_error_for_invalid_tools_input(self) -> None:
        """Test initialization raises TypeError for non-list tools."""
        with pytest.raises(TypeError, match="tools must be a list"):
            PlanValidator("not a list")  # type: ignore

    def test_should_validate_complete_plan(self) -> None:
        """Test end-to-end plan validation."""
        tools = [
            {"name": "search_recipes", "parameters": {"cuisine": {"type": "str", "required": False}}}
        ]
        validator = PlanValidator(tools)

        plan = {
            "goal": "Find Italian recipes",
            "steps": [{"step": 1, "tool": "search_recipes", "args": {"cuisine": "Italian"}}]
        }

        result = validator.validate(plan)

        assert result["overall_valid"] is True
        assert result["tool_selection_valid"] is True
        assert result["args_valid"] is True

    def test_should_detect_invalid_tool_selection(self) -> None:
        """Test validator detects invalid tool selection."""
        tools = [{"name": "search_recipes", "parameters": {}}]
        validator = PlanValidator(tools)

        plan = {
            "goal": "Find recipes",
            "steps": [{"step": 1, "tool": "unknown_tool", "args": {}}]
        }

        result = validator.validate(plan)

        assert result["tool_selection_valid"] is False

    def test_should_detect_invalid_arguments(self) -> None:
        """Test validator detects invalid arguments."""
        tools = [
            {"name": "search_recipes", "parameters": {"max_cook_time": {"type": "int", "required": False}}}
        ]
        validator = PlanValidator(tools)

        plan = {
            "goal": "Find recipes",
            "steps": [{"step": 1, "tool": "search_recipes", "args": {"max_cook_time": "invalid"}}]
        }

        result = validator.validate(plan)

        assert result["args_valid"] is False


# ============================================================================
# ToolCallValidator Class Tests (3+ tests)
# ============================================================================

class TestToolCallValidator:
    """Test ToolCallValidator class."""

    def test_should_validate_schema_correctly(self) -> None:
        """Test schema validation."""
        validator = ToolCallValidator()

        tool_call = {"tool": "search_recipes", "args": {"cuisine": "Italian"}}
        schema = {
            "name": "search_recipes",
            "parameters": {"cuisine": {"type": "str", "required": False}}
        }

        result = validator.validate_schema(tool_call, schema)

        assert result["is_valid"] is True

    def test_should_validate_semantic_correctness(self) -> None:
        """Test semantic validation (e.g., vegan != vegetarian)."""
        validator = ToolCallValidator()

        query = "Find vegan recipes"
        tool_call = {"tool": "search_recipes", "args": {"dietary_restrictions": ["vegetarian"]}}

        result = validator.validate_semantics(tool_call, query)

        assert result["valid"] is False
        assert "vegan" in result["issues"][0].lower()

    def test_should_pass_semantic_validation_when_matching(self) -> None:
        """Test semantic validation passes when query matches args."""
        validator = ToolCallValidator()

        query = "Find vegan recipes"
        tool_call = {"tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"]}}

        result = validator.validate_semantics(tool_call, query)

        assert result["valid"] is True


# ============================================================================
# PlanEvaluator Class Tests (3+ tests)
# ============================================================================

class TestPlanEvaluator:
    """Test PlanEvaluator class (orchestrates all validations)."""

    def test_should_evaluate_complete_plan_pipeline(self) -> None:
        """Test end-to-end plan evaluation."""
        tools = [{"name": "search_recipes", "parameters": {}}]
        evaluator = PlanEvaluator(tools)

        plan = {
            "goal": "Find vegan recipes",
            "steps": [{"step": 1, "tool": "search_recipes", "args": {"dietary_restrictions": ["vegan"]}}]
        }

        result = evaluator.evaluate(plan)

        assert "correctness_score" in result
        assert "efficiency_score" in result
        assert "completeness_score" in result
        assert "overall_score" in result
        assert 0.0 <= result["overall_score"] <= 1.0

    def test_should_calculate_weighted_overall_score(self) -> None:
        """Test evaluator calculates weighted overall score."""
        tools = [{"name": "search_recipes", "parameters": {}}]
        evaluator = PlanEvaluator(tools)

        plan = {
            "goal": "Find recipes",
            "steps": [{"step": 1, "tool": "search_recipes", "args": {}}]
        }

        result = evaluator.evaluate(plan)

        # Overall score should be weighted average
        assert result["overall_score"] > 0.0
        assert result["overall_score"] <= 1.0

    def test_should_include_validation_details(self) -> None:
        """Test evaluator includes full validation details."""
        tools = [{"name": "search_recipes", "parameters": {}}]
        evaluator = PlanEvaluator(tools)

        plan = {
            "goal": "Find recipes",
            "steps": [{"step": 1, "tool": "search_recipes", "args": {}}]
        }

        result = evaluator.evaluate(plan)

        assert "validation_details" in result
        assert "correctness" in result["validation_details"]
        assert "completeness" in result["validation_details"]
```

### Implementation Checklist

- [ ] Create `tests/test_agent_evaluation.py` with 40+ test stubs
- [ ] Run tests to confirm they fail (RED phase): `pytest tests/test_agent_evaluation.py -v`
- [ ] Verify test naming follows convention: `test_should_[result]_when_[condition]`
- [ ] Mark Task 3.9 complete

---

## Part 4: Defensive Coding Patterns

### 5-Step Function Template

**Source:** `CLAUDE.md:230-270`, used throughout `backend/exact_evaluation.py`

```python
def function_name(arg: Type, optional: Type = default) -> ReturnType:
    """Brief description.

    Args:
        arg: Description
        optional: Description

    Returns:
        Description

    Raises:
        TypeError: When type validation fails
        ValueError: When value validation fails
    """
    # Step 1: Type checking
    if not isinstance(arg, ExpectedType):
        raise TypeError("arg must be ExpectedType")

    # Step 2: Input validation
    if arg < 0:
        raise ValueError("arg must be non-negative")

    # Step 3: Edge case handling
    if len(arg) == 0:
        return default_value

    # Step 4: Main logic
    result = process(arg)

    # Step 5: Return
    return result
```

### Abstract Base Class Pattern

**Source:** `backend/ai_judge_framework.py:64-100`, `patterns/abstract-base-class.md`

```python
from abc import ABC, abstractmethod

class BaseAgent(ABC):
    """Abstract base for all agent implementations."""

    def __init__(self, model: str):
        # Defensive validation
        if not isinstance(model, str):
            raise TypeError("model must be a string")
        self.model = model

    @abstractmethod
    def process(self, input: dict) -> dict:
        """Subclasses must implement."""
        pass

    def _helper_method(self) -> str:
        """Shared functionality."""
        return "shared"

class ConcreteAgent(BaseAgent):
    def __init__(self, model: str):
        super().__init__(model)  # CRITICAL: Call parent __init__
        self.custom_attr = "value"

    def process(self, input: dict) -> dict:
        """Implement required method."""
        return {"result": "processed"}
```

---

## Part 5: Reference Index

### Backend Patterns
- **Defensive Function Template**: `CLAUDE.md:230-270`
- **ABC Pattern**: `backend/ai_judge_framework.py:64-100`, `patterns/abstract-base-class.md`
- **Exact Evaluation Pattern**: `backend/exact_evaluation.py:1-496`
- **ThreadPoolExecutor**: `patterns/threadpool-parallel.md`

### Test Patterns
- **TDD Workflow**: `patterns/tdd-workflow.md`
- **Test Structure**: `tests/test_exact_evaluation.py:25-88`
- **Test Naming**: `CLAUDE.md:290-340`

### Benchmark Patterns
- **Planning Benchmark**: `lesson-14/data/agent_planning_benchmark.json:1-3306`
- **Generator Script**: `lesson-14/data/generate_planning_benchmark.py:1-423`
- **RAG Evaluation Suite**: `lesson-13/data/rag_evaluation_suite.json:1-100`

### Tutorial Patterns
- **ReAct Pattern**: `lesson-14/react_reflexion_patterns.md:262-435`
- **Multi-Agent**: `lesson-14/multi_agent_orchestration.md:510-598`
- **Agent Planning**: `lesson-14/agent_planning_evaluation.md:766-856`

---

## Part 6: Risk Assessment

### Critical Risks

**1. Missing Dependencies**
- **Status:** ‚úÖ LOW RISK
- **Analysis:** All required libraries already in pyproject.toml (pydantic, litellm, tenacity)

**2. Unclear Requirements for Multi-Agent Framework**
- **Status:** ‚ö†Ô∏è MEDIUM RISK
- **Mitigation:** Use complete templates from `multi_agent_orchestration.md:200-600`

**3. Testing Complexity for ReAct/Reflexion**
- **Status:** ‚ö†Ô∏è MEDIUM RISK
- **Mitigation:** Follow TDD pattern, mock LLM calls with `@patch`

**4. Performance Bottlenecks in Batch Processing**
- **Status:** ‚ö†Ô∏è MEDIUM RISK
- **Mitigation:** Use ThreadPoolExecutor pattern for parallel evaluation

---

## Conclusion

This implementation guide provides **copy-paste ready templates** for all remaining Lesson 14 tasks (3.7-3.16). Key deliverables:

1. ‚úÖ **Complete JSON schemas** for tool_call_benchmark.json and efficiency_benchmark.json
2. ‚úÖ **40+ test stubs** for test_agent_evaluation.py (TDD RED phase)
3. ‚úÖ **Full implementation templates** with defensive coding patterns
4. ‚úÖ **Step-by-step checklists** for each task
5. ‚úÖ **Risk mitigation strategies** for complex components

**Next Steps:**
1. Execute Task 3.7: Generate tool_call_benchmark.json using provided script
2. Execute Task 3.8: Generate efficiency_benchmark.json
3. Execute Task 3.9: Create test file (TDD RED phase)
4. Execute Task 3.10: Implement backend/agent_evaluation.py (TDD GREEN phase)
5. Execute Tasks 3.12-3.16: Multi-agent framework following tutorial patterns

All patterns extracted from real codebase examples with defensive coding, type hints, and comprehensive error handling.
