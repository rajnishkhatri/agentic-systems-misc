{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Systems Implementation - Lesson 14\n",
    "\n",
    "**Tutorial 18: Interactive Implementation of Memory Patterns**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Working memory management (trimming & summarization)\n",
    "- Long-term memory with Chroma vector database\n",
    "- Search-o1 pattern with Reason-in-Documents\n",
    "- Context engineering techniques (MMR, compression, ordering)\n",
    "- Cost/latency optimization strategies\n",
    "\n",
    "**Prerequisites:**\n",
    "- Read `memory_systems_fundamentals.md`\n",
    "- Read `context_engineering_guide.md`\n",
    "- Familiarity with RAG basics (`04_Agentic_RAG.md`)\n",
    "\n",
    "**Execution Modes:**\n",
    "- **DEMO**: Quick demonstration (~8-10 minutes, 10 queries, 100 documents)\n",
    "- **FULL**: Comprehensive analysis (~30-40 minutes, 50 queries, 500 documents)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "### ‚ö†Ô∏è Cost Warning\n",
    "\n",
    "**DEMO mode**: No API costs (uses mock LLM calls)  \n",
    "**FULL mode**: Estimated API costs if LLM enabled:\n",
    "- ~$0.50 - $1.00 for GPT-4 calls\n",
    "- ~$0.10 - $0.25 for GPT-3.5-turbo calls\n",
    "\n",
    "Set `EXECUTION_MODE = 'DEMO'` to avoid costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Cell\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add lesson-14 to path for imports\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Execution mode\n",
    "EXECUTION_MODE = 'DEMO'  # Change to 'FULL' for comprehensive analysis\n",
    "\n",
    "# Optional: Enable real LLM calls (requires API key)\n",
    "USE_LLM = False  # Set to True to use actual LLM API calls\n",
    "\n",
    "# Mode-specific configs\n",
    "if EXECUTION_MODE == 'DEMO':\n",
    "    NUM_QUERIES = 10\n",
    "    NUM_DOCUMENTS = 100\n",
    "    CHUNK_SIZE = 500\n",
    "elif EXECUTION_MODE == 'FULL':\n",
    "    NUM_QUERIES = 50\n",
    "    NUM_DOCUMENTS = 500\n",
    "    CHUNK_SIZE = 1000\n",
    "else:\n",
    "    raise ValueError(f\"EXECUTION_MODE must be 'DEMO' or 'FULL', got '{EXECUTION_MODE}'\")\n",
    "\n",
    "print(f\"‚úÖ Execution Mode: {EXECUTION_MODE}\")\n",
    "print(f\"   - Queries: {NUM_QUERIES}\")\n",
    "print(f\"   - Documents: {NUM_DOCUMENTS}\")\n",
    "print(f\"   - LLM Enabled: {USE_LLM}\")\n",
    "print(f\"   - Estimated Runtime: {'<10 min' if EXECUTION_MODE == 'DEMO' else '30-40 min'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "# ChromaDB for vector storage\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Our helper functions (tested in tests/test_memory_systems_notebook.py)\n",
    "from memory_systems_helpers import (\n",
    "    validate_execution_mode,\n",
    "    calculate_mmr_score,\n",
    "    select_documents_mmr,\n",
    "    count_tokens,\n",
    "    trim_conversation_history,\n",
    "    simulate_summarization,\n",
    "    calculate_search_o1_overhead,\n",
    "    calculate_compression_roi,\n",
    "    export_results_json,\n",
    ")\n",
    "\n",
    "# Validate execution mode\n",
    "validate_execution_mode(EXECUTION_MODE, raise_on_invalid=True)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Working Memory Management\n",
    "\n",
    "Demonstrates short-term memory techniques:\n",
    "1. Token counting and budget tracking\n",
    "2. Conversation trimming (FIFO strategy)\n",
    "3. Conversation summarization\n",
    "4. Cost comparison: trimming vs summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the Bhagavad Gita about?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Bhagavad Gita is a 700-verse Hindu scripture that is part of the epic Mahabharata. It consists of a dialogue between Prince Arjuna and the god Krishna on the battlefield of Kurukshetra.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is Arjuna?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Arjuna is one of the Pandava brothers, a skilled archer and warrior. In the Gita, he is faced with moral dilemmas about fighting his own relatives in battle.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is dharma?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Dharma is a central concept meaning duty, righteousness, or moral order. Krishna advises Arjuna to follow his dharma as a warrior, even in difficult circumstances.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about karma yoga.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Karma Yoga is the path of selfless action. The Gita teaches performing one's duty without attachment to results, dedicating actions to the divine rather than personal gain.\"},\n",
    "]\n",
    "\n",
    "# Count tokens in conversation\n",
    "total_tokens = sum(count_tokens(msg[\"content\"]) for msg in conversation_history)\n",
    "print(f\"\\nüìä Original Conversation:\")\n",
    "print(f\"   - Messages: {len(conversation_history)}\")\n",
    "print(f\"   - Total Tokens: {total_tokens}\")\n",
    "print(f\"   - Cost (@$0.03/1K tokens): ${(total_tokens / 1000) * 0.03:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Trimming (FIFO - keep most recent)\n",
    "max_tokens_budget = 100\n",
    "trimmed_history = trim_conversation_history(conversation_history, max_tokens_budget)\n",
    "\n",
    "trimmed_tokens = sum(count_tokens(msg[\"content\"]) for msg in trimmed_history)\n",
    "print(f\"\\n‚úÇÔ∏è After Trimming (max {max_tokens_budget} tokens):\")\n",
    "print(f\"   - Messages Kept: {len(trimmed_history)} / {len(conversation_history)}\")\n",
    "print(f\"   - Tokens: {trimmed_tokens}\")\n",
    "print(f\"   - Reduction: {((total_tokens - trimmed_tokens) / total_tokens) * 100:.1f}%\")\n",
    "print(f\"   - Cost: ${(trimmed_tokens / 1000) * 0.03:.4f}\")\n",
    "print(f\"\\n   Kept messages (most recent):\")\n",
    "for msg in trimmed_history:\n",
    "    print(f\"     - {msg['role']}: {msg['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Summarization\n",
    "compression_ratio = 0.3  # Target 30% of original tokens\n",
    "summary_msg, orig_tokens, summary_tokens = simulate_summarization(\n",
    "    conversation_history, compression_ratio\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù After Summarization (ratio={compression_ratio}):\")\n",
    "print(f\"   - Original Tokens: {orig_tokens}\")\n",
    "print(f\"   - Summary Tokens: {summary_tokens}\")\n",
    "print(f\"   - Compression: {(summary_tokens / orig_tokens) * 100:.1f}% of original\")\n",
    "print(f\"   - Cost: ${(summary_tokens / 1000) * 0.03:.4f}\")\n",
    "print(f\"\\n   Summary: {summary_msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Token usage comparison\n",
    "strategies = ['Original', 'Trimming', 'Summarization']\n",
    "token_counts = [total_tokens, trimmed_tokens, summary_tokens]\n",
    "costs = [(t / 1000) * 0.03 for t in token_counts]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Tokens comparison\n",
    "ax1.bar(strategies, token_counts, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax1.set_ylabel('Tokens')\n",
    "ax1.set_title('Token Usage by Strategy')\n",
    "ax1.axhline(y=max_tokens_budget, color='r', linestyle='--', label=f'Budget ({max_tokens_budget})')\n",
    "ax1.legend()\n",
    "\n",
    "# Cost comparison\n",
    "ax2.bar(strategies, costs, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax2.set_ylabel('Cost ($)')\n",
    "ax2.set_title('API Cost by Strategy (@$0.03/1K tokens)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insight: Summarization achieves {((total_tokens - summary_tokens) / total_tokens) * 100:.0f}% token reduction\")\n",
    "print(f\"   vs {((total_tokens - trimmed_tokens) / total_tokens) * 100:.0f}% for trimming, but preserves older context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Long-Term Memory with Chroma\n",
    "\n",
    "Set up persistent vector database for episodic/semantic memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma persistent client\n",
    "chroma_path = Path.cwd() / \"data\" / \"chroma_memory_demo\"\n",
    "chroma_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=str(chroma_path),\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create/get collection\n",
    "collection_name = \"lesson14_memory_demo\"\n",
    "try:\n",
    "    client.delete_collection(collection_name)  # Clean slate\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"Memory systems tutorial demo collection\"}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Chroma client initialized at {chroma_path}\")\n",
    "print(f\"   Collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest sample data from lesson-14\n",
    "# Read multi_agent_fundamentals.md and chunk it\n",
    "source_file = Path.cwd() / \"multi_agent_fundamentals.md\"\n",
    "\n",
    "if source_file.exists():\n",
    "    with open(source_file) as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Simple chunking by paragraphs\n",
    "    paragraphs = [p.strip() for p in content.split('\\n\\n') if len(p.strip()) > 50]\n",
    "    \n",
    "    # Limit to NUM_DOCUMENTS\n",
    "    chunks = paragraphs[:NUM_DOCUMENTS]\n",
    "    \n",
    "    # Create mock embeddings (in real use, use sentence-transformers or OpenAI)\n",
    "    # For demo, use random embeddings with some semantic structure\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.rand(len(chunks), 384).tolist()  # 384-dim like sentence-transformers\n",
    "    \n",
    "    # Add to collection with metadata\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"source\": \"multi_agent_fundamentals.md\", \"chunk_id\": i} for i in range(len(chunks))],\n",
    "        ids=[f\"doc_{i}\" for i in range(len(chunks))]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Ingested {len(chunks)} chunks into Chroma\")\n",
    "    print(f\"   Source: multi_agent_fundamentals.md\")\n",
    "    print(f\"   Embedding Dimension: 384\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Source file not found: {source_file}\")\n",
    "    print(f\"   Using synthetic data instead...\")\n",
    "    \n",
    "    # Fallback: create synthetic chunks\n",
    "    synthetic_chunks = [\n",
    "        f\"This is synthetic document {i} about multi-agent systems and memory patterns.\"\n",
    "        for i in range(NUM_DOCUMENTS)\n",
    "    ]\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.rand(len(synthetic_chunks), 384).tolist()\n",
    "    \n",
    "    collection.add(\n",
    "        documents=synthetic_chunks,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"source\": \"synthetic\", \"chunk_id\": i} for i in range(len(synthetic_chunks))],\n",
    "        ids=[f\"doc_{i}\" for i in range(len(synthetic_chunks))]\n",
    "    )\n",
    "    print(f\"‚úÖ Ingested {len(synthetic_chunks)} synthetic chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic retrieval example\n",
    "query_text = \"What are the key components of multi-agent systems?\"\n",
    "np.random.seed(123)\n",
    "query_embedding = np.random.rand(384).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Basic Retrieval Example:\")\n",
    "print(f\"   Query: {query_text}\")\n",
    "print(f\"   Top 5 Results:\")\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"\\n   {i+1}. (Source: {meta['source']}, Chunk {meta['chunk_id']})\")\n",
    "    print(f\"      {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Context Engineering - MMR Selection\n",
    "\n",
    "Compare different Œª values for relevance vs diversity trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: MMR with different lambda values\n",
    "# Retrieve more documents than needed, then select with MMR\n",
    "candidate_results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=20  # Over-retrieve\n",
    ")\n",
    "\n",
    "candidate_embeddings = np.array(candidate_results['embeddings'][0])\n",
    "k = 5  # Select top 5 with MMR\n",
    "\n",
    "lambda_values = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "mmr_results = {}\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "    selected_indices = select_documents_mmr(\n",
    "        query_embedding,\n",
    "        candidate_embeddings.tolist(),\n",
    "        k=k,\n",
    "        lambda_param=lambda_val\n",
    "    )\n",
    "    mmr_results[lambda_val] = selected_indices\n",
    "\n",
    "print(f\"\\nüéØ MMR Selection Results (k={k} from 20 candidates):\")\n",
    "print(f\"\\nŒª | Selected Document Indices\")\n",
    "print(\"-\" * 40)\n",
    "for lambda_val, indices in mmr_results.items():\n",
    "    print(f\"{lambda_val:.1f} | {indices}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   Œª=0.0 (pure relevance): Selects most similar documents (may be redundant)\")\n",
    "print(f\"   Œª=1.0 (pure diversity): Maximizes diversity (may sacrifice relevance)\")\n",
    "print(f\"   Œª=0.3-0.7 (balanced): Good trade-off for most use cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare overlap between Œª=0 and Œª=0.7\n",
    "lambda_0_set = set(mmr_results[0.0])\n",
    "lambda_07_set = set(mmr_results[0.7])\n",
    "\n",
    "overlap = len(lambda_0_set & lambda_07_set)\n",
    "unique_to_0 = len(lambda_0_set - lambda_07_set)\n",
    "unique_to_07 = len(lambda_07_set - lambda_0_set)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "categories = ['Overlap', 'Unique to Œª=0.0\\n(pure relevance)', 'Unique to Œª=0.7\\n(balanced)']\n",
    "counts = [overlap, unique_to_0, unique_to_07]\n",
    "colors = ['#9467bd', '#1f77b4', '#2ca02c']\n",
    "\n",
    "ax.bar(categories, counts, color=colors)\n",
    "ax.set_ylabel('Number of Documents')\n",
    "ax.set_title('MMR Selection Comparison: Œª=0.0 vs Œª=0.7')\n",
    "ax.set_ylim(0, k)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Diversity Impact:\")\n",
    "print(f\"   Overlap: {overlap}/{k} documents\")\n",
    "print(f\"   Diversity brings in {unique_to_07} new documents vs pure relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Compression ROI Calculator\n",
    "\n",
    "Exercise 2: Calculate cost savings from context compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: 5 retrieved documents, each ~500 tokens\n",
    "original_doc_tokens = 500\n",
    "num_docs = 5\n",
    "original_total = original_doc_tokens * num_docs\n",
    "\n",
    "# Test different compression strategies\n",
    "compression_strategies = {\n",
    "    \"No compression\": 1.0,\n",
    "    \"Light (remove duplicates)\": 0.8,\n",
    "    \"Medium (extractive summary)\": 0.5,\n",
    "    \"Aggressive (LLMLingua)\": 0.2,\n",
    "}\n",
    "\n",
    "cost_per_1k = 0.03  # GPT-4 input pricing\n",
    "\n",
    "print(f\"\\nüí∞ Compression ROI Analysis:\")\n",
    "print(f\"   Original: {num_docs} docs √ó {original_doc_tokens} tokens = {original_total} tokens\")\n",
    "print(f\"   Pricing: ${cost_per_1k}/1K tokens\\n\")\n",
    "print(f\"Strategy                    | Tokens  | Cost      | Savings   | Ratio\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "roi_data = []\n",
    "for strategy, ratio in compression_strategies.items():\n",
    "    compressed_tokens = int(original_total * ratio)\n",
    "    roi = calculate_compression_roi(original_total, compressed_tokens, cost_per_1k)\n",
    "    \n",
    "    roi_data.append(roi)\n",
    "    print(f\"{strategy:27} | {compressed_tokens:6} | ${roi['compressed_cost']:.4f} | ${roi['cost_savings']:.4f}  | {ratio:.1f}x\")\n",
    "\n",
    "print(f\"\\nüí° Real-world Example (from COMPASS_ARTIFACT_ANALYSIS.md):\")\n",
    "print(f\"   Context compression: $24 ‚Üí $12 ‚Üí $4.80 (5x ROI)\")\n",
    "print(f\"   Strategy: Deduplication + extractive summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Cost savings\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cost comparison\n",
    "strategies_list = list(compression_strategies.keys())\n",
    "costs = [roi['compressed_cost'] for roi in roi_data]\n",
    "ax1.barh(strategies_list, costs, color='#ff7f0e')\n",
    "ax1.axvline(x=roi_data[0]['original_cost'], color='r', linestyle='--', \n",
    "            label=f\"Original (${roi_data[0]['original_cost']:.4f})\")\n",
    "ax1.set_xlabel('Cost ($)')\n",
    "ax1.set_title('API Cost by Compression Strategy')\n",
    "ax1.legend()\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Savings\n",
    "savings = [roi['cost_savings'] for roi in roi_data]\n",
    "ax2.barh(strategies_list, savings, color='#2ca02c')\n",
    "ax2.set_xlabel('Savings ($)')\n",
    "ax2.set_title('Cost Savings vs Original')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Search-o1 Pattern Implementation\n",
    "\n",
    "Demonstrate the Search-o1 reasoning pattern with Reason-in-Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search-o1 Pattern: Multi-branch search with reasoning\n",
    "def simulate_search_o1(query: str, num_branches: int = 3) -> dict[str, Any]:\n",
    "    \"\"\"Simulate Search-o1 pattern with token tracking.\"\"\"\n",
    "    \n",
    "    # Step 1: Generate search queries for multiple branches\n",
    "    search_queries = [\n",
    "        f\"Branch {i+1}: {query} (aspect {i+1})\" for i in range(num_branches)\n",
    "    ]\n",
    "    search_tokens = sum(count_tokens(q) for q in search_queries)\n",
    "    \n",
    "    # Step 2: Retrieve documents for each branch\n",
    "    np.random.seed(42)\n",
    "    retrieval_tokens = 0\n",
    "    all_documents = []\n",
    "    \n",
    "    for search_query in search_queries:\n",
    "        # Mock retrieval (in real use, query vector DB)\n",
    "        query_emb = np.random.rand(384).tolist()\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_emb],\n",
    "            n_results=3\n",
    "        )\n",
    "        docs = results['documents'][0]\n",
    "        all_documents.extend(docs)\n",
    "        retrieval_tokens += sum(count_tokens(doc) for doc in docs)\n",
    "    \n",
    "    # Step 3: Reason-in-Documents (condensation)\n",
    "    # Simulate LLM reasoning over all retrieved docs\n",
    "    condensation_input = f\"Reasoning over {len(all_documents)} documents for query: {query}\"\n",
    "    condensation_input_tokens = count_tokens(condensation_input) + retrieval_tokens\n",
    "    \n",
    "    # Condensation reduces to ~30% of input\n",
    "    condensation_output_tokens = int(condensation_input_tokens * 0.3)\n",
    "    condensation_tokens = condensation_input_tokens + condensation_output_tokens\n",
    "    \n",
    "    # Step 4: Calculate overhead\n",
    "    baseline_tokens = count_tokens(query) + retrieval_tokens  # Without Search-o1\n",
    "    overhead = calculate_search_o1_overhead(\n",
    "        search_tokens, retrieval_tokens, condensation_tokens, baseline_tokens\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"num_branches\": num_branches,\n",
    "        \"search_tokens\": search_tokens,\n",
    "        \"retrieval_tokens\": retrieval_tokens,\n",
    "        \"condensation_tokens\": condensation_tokens,\n",
    "        \"total_overhead\": overhead[\"total_overhead\"],\n",
    "        \"overhead_percentage\": overhead.get(\"overhead_percentage\", 0),\n",
    "        \"documents_retrieved\": len(all_documents),\n",
    "    }\n",
    "\n",
    "# Run Search-o1 simulation\n",
    "test_query = \"Explain multi-agent memory coordination patterns\"\n",
    "search_o1_result = simulate_search_o1(test_query, num_branches=3)\n",
    "\n",
    "print(f\"\\nüî¨ Search-o1 Pattern Simulation:\")\n",
    "print(f\"   Query: {test_query}\")\n",
    "print(f\"   Branches: {search_o1_result['num_branches']}\")\n",
    "print(f\"\\n   Token Breakdown:\")\n",
    "print(f\"     - Search queries: {search_o1_result['search_tokens']}\")\n",
    "print(f\"     - Retrieved docs: {search_o1_result['retrieval_tokens']}\")\n",
    "print(f\"     - Condensation: {search_o1_result['condensation_tokens']}\")\n",
    "print(f\"     - Total overhead: {search_o1_result['total_overhead']}\")\n",
    "print(f\"     - Overhead %: {search_o1_result['overhead_percentage']:.1f}%\")\n",
    "print(f\"\\n   Documents retrieved: {search_o1_result['documents_retrieved']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overhead for different branch counts\n",
    "branch_counts = [1, 2, 3, 4, 5]\n",
    "overhead_comparison = []\n",
    "\n",
    "for num_branches in branch_counts:\n",
    "    result = simulate_search_o1(test_query, num_branches=num_branches)\n",
    "    overhead_comparison.append(result)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Token breakdown by branch count\n",
    "search_tokens_list = [r['search_tokens'] for r in overhead_comparison]\n",
    "retrieval_tokens_list = [r['retrieval_tokens'] for r in overhead_comparison]\n",
    "condensation_tokens_list = [r['condensation_tokens'] for r in overhead_comparison]\n",
    "\n",
    "x = np.arange(len(branch_counts))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, search_tokens_list, width, label='Search', color='#1f77b4')\n",
    "ax1.bar(x, retrieval_tokens_list, width, label='Retrieval', color='#ff7f0e')\n",
    "ax1.bar(x + width, condensation_tokens_list, width, label='Condensation', color='#2ca02c')\n",
    "\n",
    "ax1.set_xlabel('Number of Branches')\n",
    "ax1.set_ylabel('Tokens')\n",
    "ax1.set_title('Search-o1 Token Breakdown by Branch Count')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(branch_counts)\n",
    "ax1.legend()\n",
    "\n",
    "# Overhead percentage\n",
    "overhead_pcts = [r['overhead_percentage'] for r in overhead_comparison]\n",
    "ax2.plot(branch_counts, overhead_pcts, marker='o', linewidth=2, markersize=8, color='#d62728')\n",
    "ax2.set_xlabel('Number of Branches')\n",
    "ax2.set_ylabel('Overhead (%)')\n",
    "ax2.set_title('Search-o1 Overhead vs Baseline')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Trade-off Analysis:\")\n",
    "print(f\"   More branches ‚Üí More comprehensive search\")\n",
    "print(f\"   But: Overhead increases linearly with branches\")\n",
    "print(f\"   Recommendation: 2-3 branches for most use cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Multi-Agent Memory Coordination\n",
    "\n",
    "Exercise 3: Demonstrate shared vs private memory in multi-agent systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: 3 agents with different memory configs\n",
    "agents = {\n",
    "    \"Agent A (Shared Only)\": {\n",
    "        \"shared_memory\": True,\n",
    "        \"private_memory\": False,\n",
    "        \"memory_size_tokens\": 0,  # Only uses shared\n",
    "    },\n",
    "    \"Agent B (Private Only)\": {\n",
    "        \"shared_memory\": False,\n",
    "        \"private_memory\": True,\n",
    "        \"memory_size_tokens\": 500,\n",
    "    },\n",
    "    \"Agent C (Hybrid)\": {\n",
    "        \"shared_memory\": True,\n",
    "        \"private_memory\": True,\n",
    "        \"memory_size_tokens\": 300,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Shared memory (accessible to all agents)\n",
    "shared_memory_tokens = 1000\n",
    "\n",
    "# Calculate total memory footprint\n",
    "print(f\"\\nü§ù Multi-Agent Memory Coordination:\")\n",
    "print(f\"\\n   Shared Memory: {shared_memory_tokens} tokens (accessible to all)\\n\")\n",
    "\n",
    "for agent_name, config in agents.items():\n",
    "    total_access = 0\n",
    "    if config[\"shared_memory\"]:\n",
    "        total_access += shared_memory_tokens\n",
    "    if config[\"private_memory\"]:\n",
    "        total_access += config[\"memory_size_tokens\"]\n",
    "    \n",
    "    print(f\"   {agent_name}:\")\n",
    "    print(f\"     - Shared: {'‚úì' if config['shared_memory'] else '‚úó'}\")\n",
    "    print(f\"     - Private: {'‚úì' if config['private_memory'] else '‚úó'} ({config['memory_size_tokens']} tokens)\")\n",
    "    print(f\"     - Total Access: {total_access} tokens\\n\")\n",
    "\n",
    "# Calculate system-wide memory cost\n",
    "total_private = sum(a[\"memory_size_tokens\"] for a in agents.values())\n",
    "total_system = shared_memory_tokens + total_private\n",
    "cost_per_1k = 0.03\n",
    "total_cost = (total_system / 1000) * cost_per_1k\n",
    "\n",
    "print(f\"   System-Wide Memory:\")\n",
    "print(f\"     - Shared: {shared_memory_tokens} tokens\")\n",
    "print(f\"     - Private (total): {total_private} tokens\")\n",
    "print(f\"     - Total: {total_system} tokens\")\n",
    "print(f\"     - Cost: ${total_cost:.4f} per query\")\n",
    "\n",
    "print(f\"\\nüí° Design Principle:\")\n",
    "print(f\"   Use shared memory for common knowledge (ontologies, facts)\")\n",
    "print(f\"   Use private memory for agent-specific state/history\")\n",
    "print(f\"   Hybrid approach balances coordination and specialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Results Export & Metrics\n",
    "\n",
    "Generate summary statistics and export to JSON for dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics from all exercises\n",
    "summary_statistics = {\n",
    "    \"trimming_reduction\": {\n",
    "        \"mean\": ((total_tokens - trimmed_tokens) / total_tokens),\n",
    "        \"std\": 0.05,  # Mock std\n",
    "    },\n",
    "    \"summarization_reduction\": {\n",
    "        \"mean\": ((total_tokens - summary_tokens) / total_tokens),\n",
    "        \"std\": 0.08,\n",
    "    },\n",
    "    \"mmr_diversity_impact\": {\n",
    "        \"mean\": unique_to_07 / k,\n",
    "        \"std\": 0.12,\n",
    "    },\n",
    "    \"compression_roi\": {\n",
    "        \"mean\": np.mean([r[\"cost_savings\"] for r in roi_data[1:]]),  # Exclude no compression\n",
    "        \"std\": np.std([r[\"cost_savings\"] for r in roi_data[1:]]),\n",
    "    },\n",
    "    \"search_o1_overhead_pct\": {\n",
    "        \"mean\": np.mean(overhead_pcts),\n",
    "        \"std\": np.std(overhead_pcts),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Radar chart data\n",
    "radar_chart_data = {\n",
    "    \"labels\": list(summary_statistics.keys()),\n",
    "    \"values\": [summary_statistics[k][\"mean\"] for k in summary_statistics.keys()],\n",
    "}\n",
    "\n",
    "# Detailed results (per-exercise breakdown)\n",
    "detailed_results = [\n",
    "    {\n",
    "        \"exercise\": \"Working Memory - Trimming\",\n",
    "        \"metric\": \"token_reduction\",\n",
    "        \"value\": summary_statistics[\"trimming_reduction\"][\"mean\"],\n",
    "    },\n",
    "    {\n",
    "        \"exercise\": \"Working Memory - Summarization\",\n",
    "        \"metric\": \"token_reduction\",\n",
    "        \"value\": summary_statistics[\"summarization_reduction\"][\"mean\"],\n",
    "    },\n",
    "    {\n",
    "        \"exercise\": \"Context Engineering - MMR\",\n",
    "        \"metric\": \"diversity_impact\",\n",
    "        \"value\": summary_statistics[\"mmr_diversity_impact\"][\"mean\"],\n",
    "    },\n",
    "    {\n",
    "        \"exercise\": \"Compression ROI\",\n",
    "        \"metric\": \"cost_savings_usd\",\n",
    "        \"value\": summary_statistics[\"compression_roi\"][\"mean\"],\n",
    "    },\n",
    "    {\n",
    "        \"exercise\": \"Search-o1 Pattern\",\n",
    "        \"metric\": \"overhead_percentage\",\n",
    "        \"value\": summary_statistics[\"search_o1_overhead_pct\"][\"mean\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Summary Statistics:\")\n",
    "for metric_name, stats in summary_statistics.items():\n",
    "    print(f\"   {metric_name}: {stats['mean']:.3f} (¬±{stats['std']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output_path = Path.cwd() / \"results\" / \"memory_systems_demo_results.json\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "export_results_json(\n",
    "    output_path=output_path,\n",
    "    summary_statistics=summary_statistics,\n",
    "    radar_chart_data=radar_chart_data,\n",
    "    detailed_results=detailed_results,\n",
    "    execution_mode=EXECUTION_MODE,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Results exported to: {output_path}\")\n",
    "print(f\"   - Execution Mode: {EXECUTION_MODE}\")\n",
    "print(f\"   - Metrics: {len(summary_statistics)}\")\n",
    "print(f\"   - Detailed Results: {len(detailed_results)} exercises\")\n",
    "print(f\"\\nüí° Load this JSON in lesson-9-11/evaluation_dashboard.py to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Demonstrated:\n",
    "\n",
    "1. **Working Memory Management**\n",
    "   - Trimming (FIFO): Simple, preserves recent context\n",
    "   - Summarization: Better compression, preserves older context\n",
    "   - Trade-off: Cost vs completeness\n",
    "\n",
    "2. **Long-Term Memory (Chroma)**\n",
    "   - Persistent vector storage\n",
    "   - Metadata-filtered retrieval\n",
    "   - Local deployment (no external API costs)\n",
    "\n",
    "3. **Context Engineering**\n",
    "   - **MMR Selection**: Œª parameter balances relevance vs diversity\n",
    "   - **Compression**: 5x cost savings with aggressive strategies\n",
    "   - **ROI Math**: Quantify cost/latency benefits\n",
    "\n",
    "4. **Search-o1 Pattern**\n",
    "   - Multi-branch search for comprehensive coverage\n",
    "   - Reason-in-Documents condensation\n",
    "   - Overhead grows linearly with branches (2-3 optimal)\n",
    "\n",
    "5. **Multi-Agent Memory**\n",
    "   - Shared memory: Common knowledge, reduces duplication\n",
    "   - Private memory: Agent-specific state\n",
    "   - Hybrid: Best of both worlds\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Explore Diagrams:**\n",
    "   - `diagrams/memory_types_taxonomy.png` - Memory type hierarchy\n",
    "   - `diagrams/context_engineering_workflow.png` - Selection ‚Üí Compression ‚Üí Ordering pipeline\n",
    "   - `diagrams/search_o1_architecture.png` - Branching reasoning workflow\n",
    "\n",
    "2. **Read Companion Tutorials:**\n",
    "   - `memory_systems_fundamentals.md` - Theory and patterns\n",
    "   - `context_engineering_guide.md` - Deep dive into optimization\n",
    "\n",
    "3. **Apply to Your Projects:**\n",
    "   - Start with DEMO mode to prototype\n",
    "   - Use FULL mode for production evaluation\n",
    "   - Track costs with compression ROI calculator\n",
    "\n",
    "4. **Experiment:**\n",
    "   - Try different Œª values for your use case\n",
    "   - Test compression strategies with real data\n",
    "   - Benchmark Search-o1 vs standard RAG\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Tutorial Complete!**\n",
    "\n",
    "You've successfully implemented and evaluated 5 memory system patterns. Check the exported JSON for detailed metrics, and visualize in the evaluation dashboard.\n",
    "\n",
    "**Estimated Reading Time:** 45-60 minutes (with code execution)  \n",
    "**Execution Time:** ~{runtime} minutes ({EXECUTION_MODE} mode)\n",
    "\n",
    "ü§ñ Generated with [Claude Code](https://claude.com/claude-code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}