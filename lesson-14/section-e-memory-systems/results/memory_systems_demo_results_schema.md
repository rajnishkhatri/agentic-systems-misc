# Memory Systems Demo Results JSON Schema

**Version:** 1.0
**Last Updated:** 2025-11-15
**File:** `memory_systems_demo_results.json`

## Overview

This document defines the JSON schema for memory systems tutorial evaluation results. The schema is designed to be compatible with the evaluation dashboard (`lesson-9-11/evaluation_dashboard.py`) and follows the pattern established by other lesson results files (e.g., `trajectory_eval_results.json`).

## Purpose

The `memory_systems_demo_results.json` file stores metrics generated by the `memory_systems_implementation.ipynb` notebook, capturing:
- Working memory management performance (trimming, summarization)
- Context engineering effectiveness (MMR diversity, compression ROI)
- Search-o1 pattern overhead measurements

This data can be visualized in dashboards, used for tutorial validation, and serves as a reference for students comparing their implementation results.

## Top-Level Schema

```json
{
  "version": "string",
  "created": "string (ISO date)",
  "execution_mode": "string (DEMO|FULL)",
  "num_trajectories": "integer",
  "summary_statistics": { /* object */ },
  "radar_chart_data": { /* object */ },
  "detailed_results": [ /* array */ ]
}
```

## Field Definitions

### 1. `version` (string, required)

**Purpose:** Schema version for backward compatibility tracking.

**Format:** Semantic versioning (e.g., `"1.0"`, `"1.1"`)

**Example:**
```json
"version": "1.0"
```

**Validation:**
- Must be a string matching pattern `^\d+\.\d+$`
- Current version: `1.0`

---

### 2. `created` (string, required)

**Purpose:** Timestamp indicating when results were generated.

**Format:** ISO 8601 date format (`YYYY-MM-DD`)

**Example:**
```json
"created": "2025-11-15"
```

**Validation:**
- Must be a valid ISO date string
- Typically set to current date when notebook is executed

---

### 3. `execution_mode` (string, required)

**Purpose:** Indicates which execution mode was used (affects dataset size and execution time).

**Format:** Enum: `"DEMO"` or `"FULL"`

**Examples:**
```json
"execution_mode": "DEMO"  // <10 min execution, smaller datasets
"execution_mode": "FULL"  // 30-40 min execution, full datasets
```

**Validation:**
- Must be one of: `["DEMO", "FULL"]`
- DEMO mode: ~10 queries, ~100 documents, <10 min execution
- FULL mode: ~50 queries, ~500 documents, 30-40 min execution

**Usage Notes:**
- DEMO mode is for quick validation and tutorial walkthroughs
- FULL mode is for comprehensive analysis and benchmarking

---

### 4. `num_trajectories` (integer, required)

**Purpose:** Number of test trajectories/exercises executed (for validation and result interpretation).

**Format:** Positive integer

**Example:**
```json
"num_trajectories": 5
```

**Validation:**
- Must be a positive integer (> 0)
- Typical values: 5-10 for memory systems exercises

**Interpretation:**
- Corresponds to the number of exercises in the notebook:
  1. Trimming
  2. Summarization
  3. MMR diversity
  4. Compression ROI
  5. Search-o1 overhead

---

### 5. `summary_statistics` (object, required)

**Purpose:** Aggregate statistics (mean, std) for each metric across all exercises.

**Format:**
```json
{
  "<metric_name>": {
    "mean": float,
    "std": float
  },
  ...
}
```

**Example:**
```json
"summary_statistics": {
  "trimming_reduction": {
    "mean": 0.5628415300546448,
    "std": 0.05
  },
  "summarization_reduction": {
    "mean": 0.7049180327868853,
    "std": 0.08
  },
  "mmr_diversity_impact": {
    "mean": 0.8,
    "std": 0.12
  },
  "compression_roi": {
    "mean": 0.0375,
    "std": 0.018371173070873836
  },
  "search_o1_overhead_pct": {
    "mean": 242.42742512423675,
    "std": 1.0131315253859678
  }
}
```

**Metric Definitions:**

| Metric | Description | Range | Interpretation |
|--------|-------------|-------|----------------|
| `trimming_reduction` | Token reduction achieved by trimming (FIFO/sliding window) | 0.0 - 1.0 | Higher = more aggressive trimming (e.g., 0.56 = 56% reduction) |
| `summarization_reduction` | Token reduction achieved by summarization | 0.0 - 1.0 | Higher = better compression (e.g., 0.70 = 70% reduction) |
| `mmr_diversity_impact` | Diversity improvement using MMR vs pure relevance | 0.0 - 1.0 | Higher = more diverse results (λ parameter effect) |
| `compression_roi` | Cost savings in USD from context compression | 0.0 - ∞ | Dollar value (e.g., 0.0375 = $0.0375 saved per query) |
| `search_o1_overhead_pct` | Percentage overhead of Search-o1 pattern | 0.0 - ∞ | Overhead % (e.g., 242.4 = 242% more tokens than baseline) |

**Validation:**
- Each metric must have `mean` and `std` fields (both floats)
- `mean` values should be non-negative for all metrics
- `std` (standard deviation) should be non-negative
- If only one sample, `std` may be 0.0

---

### 6. `radar_chart_data` (object, required)

**Purpose:** Data formatted for radar chart visualization (dashboard or notebook plots).

**Format:**
```json
{
  "labels": ["string", ...],
  "values": [float, ...]
}
```

**Example:**
```json
"radar_chart_data": {
  "labels": [
    "trimming_reduction",
    "summarization_reduction",
    "mmr_diversity_impact",
    "compression_roi",
    "search_o1_overhead_pct"
  ],
  "values": [
    0.5628415300546448,
    0.7049180327868853,
    0.8,
    0.0375,
    242.42742512423675
  ]
}
```

**Validation:**
- `labels` and `values` arrays must have the same length
- `labels` must match keys in `summary_statistics`
- `values` must match `mean` values from `summary_statistics`
- Minimum 3 metrics required for meaningful radar chart

**Usage Notes:**
- Dashboard uses this for quick visual comparison across metrics
- Order of labels/values determines radar chart shape
- Consider normalizing values for better visualization (especially `search_o1_overhead_pct` which has a different scale)

---

### 7. `detailed_results` (array, required)

**Purpose:** Granular per-exercise results for drill-down analysis and debugging.

**Format:**
```json
[
  {
    "exercise": "string",
    "metric": "string",
    "value": float
  },
  ...
]
```

**Example:**
```json
"detailed_results": [
  {
    "exercise": "Working Memory - Trimming",
    "metric": "token_reduction",
    "value": 0.5628415300546448
  },
  {
    "exercise": "Working Memory - Summarization",
    "metric": "token_reduction",
    "value": 0.7049180327868853
  },
  {
    "exercise": "Context Engineering - MMR",
    "metric": "diversity_impact",
    "value": 0.8
  },
  {
    "exercise": "Compression ROI",
    "metric": "cost_savings_usd",
    "value": 0.0375
  },
  {
    "exercise": "Search-o1 Pattern",
    "metric": "overhead_percentage",
    "value": 242.42742512423675
  }
]
```

**Entry Schema:**

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `exercise` | string | Yes | Human-readable exercise name (e.g., "Working Memory - Trimming") |
| `metric` | string | Yes | Metric identifier (e.g., "token_reduction", "diversity_impact") |
| `value` | float | Yes | Measured value for this exercise/metric combination |

**Exercise Types:**
1. **Working Memory - Trimming**: Tests FIFO/sliding window trimming strategies
2. **Working Memory - Summarization**: Tests LLM-based summarization for conversation history
3. **Context Engineering - MMR**: Tests Maximum Marginal Relevance for diversity
4. **Compression ROI**: Calculates cost savings from context compression
5. **Search-o1 Pattern**: Measures overhead of search-based reasoning workflow

**Validation:**
- Must have at least `num_trajectories` entries (typically 5)
- Each entry must have non-null `exercise`, `metric`, and `value` fields
- `value` should be a finite number (no NaN, Infinity)

---

## Optional Fields

While not present in the current schema, the following optional fields may be added in future versions for enhanced analysis:

### `complexity_breakdown` (object, optional)

**Purpose:** Metrics segmented by complexity level (simple, medium, complex).

**Format:**
```json
"complexity_breakdown": {
  "<metric_name>": {
    "simple": float,
    "medium": float,
    "complex": float
  },
  ...
}
```

**Example:** (from `trajectory_eval_results.json`)
```json
"complexity_breakdown": {
  "exact_match": {
    "complex": 1.0,
    "medium": 1.0,
    "simple": 0.833
  }
}
```

**Usage:** Useful for understanding how metrics vary by query/task complexity.

---

### `total_cost` (float, optional)

**Purpose:** Total API cost incurred during evaluation (if LLM calls were made).

**Format:** Dollar amount (USD) as float

**Example:**
```json
"total_cost": 0.45
```

**Usage:** Cost tracking and ROI validation (especially for FULL mode).

---

## Schema Comparison

### vs. `trajectory_eval_results.json` (Lesson 14)

**Similarities:**
- Both use `version`, `created`, `execution_mode`, `num_trajectories`
- Both have `summary_statistics` (mean/std), `radar_chart_data`, `detailed_results`
- Both follow dashboard-compatible schema

**Differences:**
- `trajectory_eval_results.json` includes `complexity_breakdown` (optional)
- `memory_systems_demo_results.json` has memory-specific metrics (trimming, summarization, MMR, compression ROI, Search-o1 overhead)
- `trajectory_eval_results.json` detailed_results include `test_id`, `complexity`, `domain` fields

### vs. `evaluation_metrics.json` (Lesson 9)

**Similarities:**
- Both store evaluation metrics for tutorial validation

**Differences:**
- `evaluation_metrics.json` uses flat structure (no `summary_statistics` nesting)
- `evaluation_metrics.json` includes `total_cost` and `evaluation_timestamp` at top level
- `memory_systems_demo_results.json` follows more comprehensive schema pattern

---

## Dashboard Compatibility

### Current Dashboard Support

The `lesson-9-11/evaluation_dashboard.py` does **not** currently have a `load_lesson14_metrics()` function. However, the schema is fully compatible with dashboard patterns:

**Compatible Fields:**
- ✅ `status` field pattern (should wrap data in `{"status": "ok", "data": {...}}`)
- ✅ `summary_statistics` structure (mean/std per metric)
- ✅ `radar_chart_data` format (labels and values arrays)
- ✅ `detailed_results` array (for drill-down tables)

**Status Codes:** (from dashboard loading functions)
- `"ok"` - Data loaded successfully
- `"no_data"` - File not found (run notebook first)
- `"invalid"` - Missing required fields or malformed JSON
- `"error"` - JSON decode error or file read error

### Future Dashboard Integration

To integrate with dashboard, create a `load_lesson14_metrics()` function:

```python
def load_lesson14_metrics() -> dict[str, Any]:
    """Load Lesson 14 metrics (memory systems).

    Returns:
        Dictionary with working memory, context engineering, Search-o1 metrics

    Raises:
        FileNotFoundError: If metrics file doesn't exist
        json.JSONDecodeError: If file is not valid JSON
    """
    metrics_path = PROJECT_ROOT / "lesson-14" / "results" / "memory_systems_demo_results.json"

    if not metrics_path.exists():
        return {
            "status": "no_data",
            "message": "No Lesson 14 metrics found. Run lesson-14 notebooks first.",
        }

    try:
        with open(metrics_path) as f:
            data = json.load(f)

        # Validate required fields
        required_fields = ["version", "created", "summary_statistics", "radar_chart_data"]
        if not all(field in data for field in required_fields):
            return {
                "status": "invalid",
                "message": "Missing required fields in Lesson 14 metrics",
            }

        return {"status": "ok", "data": data}
    except (json.JSONDecodeError, OSError) as e:
        return {"status": "error", "message": f"Error loading Lesson 14 metrics: {e}"}
```

---

## Validation Rules

### Required Field Validation

```python
def validate_memory_systems_schema(data: dict) -> tuple[bool, str]:
    """Validate memory_systems_demo_results.json schema.

    Args:
        data: Loaded JSON data

    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check top-level required fields
    required_top_level = ["version", "created", "execution_mode", "num_trajectories",
                          "summary_statistics", "radar_chart_data", "detailed_results"]

    for field in required_top_level:
        if field not in data:
            return False, f"Missing required field: {field}"

    # Validate execution_mode
    if data["execution_mode"] not in ["DEMO", "FULL"]:
        return False, f"Invalid execution_mode: {data['execution_mode']} (must be DEMO or FULL)"

    # Validate num_trajectories
    if not isinstance(data["num_trajectories"], int) or data["num_trajectories"] <= 0:
        return False, f"num_trajectories must be positive integer, got: {data['num_trajectories']}"

    # Validate summary_statistics structure
    for metric, stats in data["summary_statistics"].items():
        if "mean" not in stats or "std" not in stats:
            return False, f"Metric {metric} missing mean or std"
        if not isinstance(stats["mean"], (int, float)) or not isinstance(stats["std"], (int, float)):
            return False, f"Metric {metric} mean/std must be numeric"

    # Validate radar_chart_data
    radar = data["radar_chart_data"]
    if "labels" not in radar or "values" not in radar:
        return False, "radar_chart_data missing labels or values"
    if len(radar["labels"]) != len(radar["values"]):
        return False, "radar_chart_data labels and values must have same length"

    # Validate detailed_results
    if not isinstance(data["detailed_results"], list):
        return False, "detailed_results must be array"
    if len(data["detailed_results"]) < data["num_trajectories"]:
        return False, f"detailed_results has {len(data['detailed_results'])} entries, expected at least {data['num_trajectories']}"

    for i, result in enumerate(data["detailed_results"]):
        required_result_fields = ["exercise", "metric", "value"]
        for field in required_result_fields:
            if field not in result:
                return False, f"detailed_results[{i}] missing {field}"

    return True, "Valid"
```

---

## Example Usage

### Loading and Validating

```python
import json
from pathlib import Path

# Load results
results_path = Path("lesson-14/results/memory_systems_demo_results.json")
with open(results_path) as f:
    data = json.load(f)

# Validate schema
is_valid, message = validate_memory_systems_schema(data)
if not is_valid:
    raise ValueError(f"Invalid schema: {message}")

# Access metrics
print(f"Execution Mode: {data['execution_mode']}")
print(f"Trimming Reduction: {data['summary_statistics']['trimming_reduction']['mean']:.2%}")
print(f"Summarization Reduction: {data['summary_statistics']['summarization_reduction']['mean']:.2%}")
print(f"Compression ROI: ${data['summary_statistics']['compression_roi']['mean']:.4f}")
```

### Generating Radar Chart

```python
import matplotlib.pyplot as plt
import numpy as np

# Extract radar chart data
labels = data["radar_chart_data"]["labels"]
values = data["radar_chart_data"]["values"]

# Normalize values for better visualization
# (search_o1_overhead_pct is on different scale)
normalized_values = []
for i, (label, value) in enumerate(zip(labels, values)):
    if label == "search_o1_overhead_pct":
        # Normalize 0-300% to 0-1 scale
        normalized_values.append(min(value / 300, 1.0))
    elif label == "compression_roi":
        # Normalize 0-0.1 to 0-1 scale
        normalized_values.append(min(value / 0.1, 1.0))
    else:
        # Already 0-1 scale
        normalized_values.append(value)

# Create radar chart
angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
values_plot = normalized_values + normalized_values[:1]
angles += angles[:1]

fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
ax.plot(angles, values_plot, 'o-', linewidth=2)
ax.fill(angles, values_plot, alpha=0.25)
ax.set_thetagrids(np.degrees(angles[:-1]), labels)
ax.set_ylim(0, 1)
ax.set_title("Memory Systems Performance", y=1.08)
plt.show()
```

---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2025-11-15 | Initial schema definition for memory systems tutorial |

---

## References

- **Notebook:** `lesson-14/memory_systems_implementation.ipynb`
- **Dashboard:** `lesson-9-11/evaluation_dashboard.py`
- **Similar Schemas:**
  - `lesson-14/results/trajectory_eval_results.json` (comprehensive schema)
  - `lesson-9/results/evaluation_metrics.json` (flat schema)
- **Tutorial Index:** `lesson-14/TUTORIAL_INDEX.md`

---

## Support

For questions about this schema:
1. Review the notebook implementation: `lesson-14/memory_systems_implementation.ipynb`
2. Check similar schemas in other lessons (9, 10, 11, 14)
3. Refer to dashboard loading functions: `lesson-9-11/evaluation_dashboard.py`
4. Consult `CLAUDE.md` for defensive coding and TDD patterns
