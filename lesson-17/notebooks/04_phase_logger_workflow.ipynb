{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase Logger Workflow Demo\n",
        "\n",
        "This notebook demonstrates the **PhaseLogger** - an AgentRxiv-inspired logging system for multi-phase AI agent workflows that tracks decisions and outcomes through each phase.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Initializing PhaseLogger for workflow tracking\n",
        "2. Phase lifecycle: `start_phase` ‚Üí `log_decision` ‚Üí `log_artifact` ‚Üí `end_phase`\n",
        "3. Working with different workflow phases (Planning, Data Collection, Execution, Validation)\n",
        "4. Decision logging with reasoning, alternatives considered, and selection rationale\n",
        "5. Artifact tracking with metadata (file type, path, phase association)\n",
        "6. Error handling: recoverable vs fatal errors\n",
        "7. Generating Mermaid workflow diagrams with `visualize_workflow()`\n",
        "8. Summarizing workflow statistics with `get_phase_summary()`\n",
        "9. Exporting complete workflow logs for compliance and debugging\n",
        "\n",
        "## AgentRxiv Background\n",
        "The PhaseLogger is inspired by AgentRxiv (agentrxiv.github.io), which introduced phase-based research workflow logging for autonomous research agents. This approach is particularly valuable for:\n",
        "- **Compliance auditing**: Track every decision made during multi-step processes\n",
        "- **Debugging**: Understand why an agent made specific choices\n",
        "- **Reproducibility**: Document the reasoning behind workflow outcomes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime, UTC\n",
        "import json\n",
        "\n",
        "# Add lesson-17 to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from backend.explainability.phase_logger import (\n",
        "    PhaseLogger,\n",
        "    WorkflowPhase,\n",
        "    Decision,\n",
        "    Artifact,\n",
        "    PhaseOutcome,\n",
        "    PhaseSummary,\n",
        ")\n",
        "\n",
        "# Initialize PhaseLogger for a research workflow\n",
        "storage_path = Path.cwd().parent / \"cache\"\n",
        "logger = PhaseLogger(\n",
        "    workflow_id=\"research-workflow-001\",\n",
        "    storage_path=storage_path\n",
        ")\n",
        "\n",
        "print(\"=== PhaseLogger Initialized ===\")\n",
        "print(f\"Workflow ID: {logger.workflow_id}\")\n",
        "print(f\"Storage Path: {logger.storage_path}\")\n",
        "print(f\"Logs Directory: {logger._logs_path}\")\n",
        "print()\n",
        "print(\"Available Workflow Phases:\")\n",
        "for phase in WorkflowPhase:\n",
        "    phase_type = \"terminal\" if phase in [WorkflowPhase.COMPLETED, WorkflowPhase.FAILED] else \"active\"\n",
        "    print(f\"  ‚Ä¢ {phase.value:20} ({phase_type})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Phase Lifecycle\n",
        "\n",
        "Each workflow phase follows a consistent lifecycle:\n",
        "\n",
        "```\n",
        "start_phase() ‚Üí [log_decision() | log_artifact() | log_error()]* ‚Üí end_phase()\n",
        "```\n",
        "\n",
        "**Rules:**\n",
        "- Only one phase can be active at a time\n",
        "- You cannot start a new phase while another is in progress\n",
        "- Decisions, artifacts, and errors can only be logged during an active phase\n",
        "- Each phase ends with a status: `success`, `failure`, `partial`, or `skipped`\n",
        "\n",
        "Let's demonstrate this with a **PLANNING** phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 1: PLANNING - Demonstrates the complete lifecycle\n",
        "\n",
        "# Start the planning phase\n",
        "logger.start_phase(WorkflowPhase.PLANNING, metadata={\"objective\": \"research_analysis\"})\n",
        "print(f\"‚úì Started phase: {logger.get_current_phase().value}\")\n",
        "\n",
        "# Log a decision with full reasoning details\n",
        "decision1 = logger.log_decision(\n",
        "    decision=\"Use GPT-4 as primary extraction model\",\n",
        "    reasoning=\"Need high accuracy for financial document analysis\",\n",
        "    alternatives=[\"GPT-3.5-turbo\", \"Claude-3-Sonnet\", \"Llama-3-70B\"],\n",
        "    selected_because=\"GPT-4 shows 15% higher accuracy on financial benchmarks and supports structured output\",\n",
        "    confidence=0.92,\n",
        "    agent_id=\"planning-agent\",\n",
        "    reversible=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Decision Logged:\")\n",
        "print(f\"   ID: {decision1.decision_id}\")\n",
        "print(f\"   Decision: {decision1.decision}\")\n",
        "print(f\"   Alternatives: {decision1.alternatives_considered}\")\n",
        "print(f\"   Selected Because: {decision1.selected_because}\")\n",
        "print(f\"   Confidence: {decision1.confidence:.0%}\")\n",
        "print(f\"   Reversible: {decision1.reversible}\")\n",
        "\n",
        "# Log a second decision \n",
        "decision2 = logger.log_decision(\n",
        "    decision=\"Set batch processing limit to 100 documents\",\n",
        "    reasoning=\"Balance throughput with memory constraints\",\n",
        "    alternatives=[\"50 docs\", \"200 docs\", \"500 docs\"],\n",
        "    selected_because=\"100 docs provides optimal memory usage while maintaining 2-hour processing deadline\",\n",
        "    confidence=0.85,\n",
        "    agent_id=\"planning-agent\",\n",
        ")\n",
        "\n",
        "# Log a planning artifact\n",
        "plan_artifact = logger.log_artifact(\n",
        "    artifact_name=\"research_plan\",\n",
        "    artifact_path=Path(\"outputs/research_plan_v1.json\"),\n",
        "    artifact_type=\"plan\",\n",
        "    metadata={\"version\": 1, \"steps\": 4, \"estimated_duration_hours\": 3},\n",
        ")\n",
        "\n",
        "print(f\"\\nüìÅ Artifact Logged:\")\n",
        "print(f\"   ID: {plan_artifact.artifact_id}\")\n",
        "print(f\"   Name: {plan_artifact.name}\")\n",
        "print(f\"   Type: {plan_artifact.artifact_type}\")\n",
        "print(f\"   Metadata: {plan_artifact.metadata}\")\n",
        "\n",
        "# End the phase\n",
        "outcome = logger.end_phase(status=\"success\")\n",
        "\n",
        "print(f\"\\n‚úì Phase Completed:\")\n",
        "print(f\"   Status: {outcome.status}\")\n",
        "print(f\"   Duration: {outcome.duration_ms}ms\")\n",
        "print(f\"   Decisions Made: {outcome.decisions_made}\")\n",
        "print(f\"   Artifacts Produced: {outcome.artifacts_produced}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DATA_COLLECTION Phase\n",
        "\n",
        "The **DATA_COLLECTION** phase is where agents gather inputs for processing. This phase typically involves:\n",
        "- Source selection decisions\n",
        "- Data quality assessments  \n",
        "- Artifact tracking for downloaded/retrieved data\n",
        "\n",
        "Let's demonstrate data collection with multiple artifact types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 2: DATA_COLLECTION - Multiple artifacts and decisions\n",
        "\n",
        "logger.start_phase(WorkflowPhase.DATA_COLLECTION)\n",
        "print(f\"‚úì Started phase: {logger.get_current_phase().value}\")\n",
        "\n",
        "# Decision: Choose data sources\n",
        "source_decision = logger.log_decision(\n",
        "    decision=\"Use S3 bucket + PostgreSQL as data sources\",\n",
        "    reasoning=\"Comprehensive coverage of historical and real-time data\",\n",
        "    alternatives=[\"S3 only\", \"PostgreSQL only\", \"API endpoints\", \"Local filesystem\"],\n",
        "    selected_because=\"Combination provides historical archives (S3) + latest transactions (PostgreSQL)\",\n",
        "    confidence=0.88,\n",
        "    agent_id=\"data-collector\",\n",
        ")\n",
        "\n",
        "# Log multiple artifacts representing collected data\n",
        "artifacts_info = [\n",
        "    (\"invoice_archive_2024\", \"data/invoices_2024.parquet\", \"dataset\", {\"records\": 15420, \"size_mb\": 128}),\n",
        "    (\"vendor_master_data\", \"data/vendors.csv\", \"dataset\", {\"records\": 892, \"columns\": 15}),\n",
        "    (\"transaction_log\", \"data/transactions_q4.jsonl\", \"log\", {\"records\": 48230, \"date_range\": \"2024-10-01 to 2024-12-31\"}),\n",
        "]\n",
        "\n",
        "print(f\"\\nüìÅ Artifacts Collected:\")\n",
        "for name, path, art_type, meta in artifacts_info:\n",
        "    artifact = logger.log_artifact(\n",
        "        artifact_name=name,\n",
        "        artifact_path=Path(path),\n",
        "        artifact_type=art_type,\n",
        "        metadata=meta,\n",
        "    )\n",
        "    print(f\"   ‚Ä¢ {artifact.name}: {meta.get('records', 'N/A')} records ({art_type})\")\n",
        "\n",
        "# Log data quality decision\n",
        "quality_decision = logger.log_decision(\n",
        "    decision=\"Accept data with 2.3% null rate\",\n",
        "    reasoning=\"Null rate within acceptable threshold for analysis\",\n",
        "    alternatives=[\"Reject and re-collect\", \"Impute missing values\", \"Filter incomplete records\"],\n",
        "    selected_because=\"2.3% null rate is below 5% threshold; imputation would introduce bias\",\n",
        "    confidence=0.95,\n",
        "    agent_id=\"quality-checker\",\n",
        ")\n",
        "\n",
        "outcome = logger.end_phase(status=\"success\")\n",
        "print(f\"\\n‚úì Phase Completed: {outcome.decisions_made} decisions, {len(outcome.artifacts_produced)} artifacts\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. EXECUTION Phase\n",
        "\n",
        "The **EXECUTION** phase is where the main processing happens. This is typically the longest phase and may involve:\n",
        "- Model inference decisions\n",
        "- Processing strategy choices\n",
        "- Runtime parameter adjustments\n",
        "- Intermediate result artifacts\n",
        "\n",
        "This phase often has the most decisions logged because it's where agents actively problem-solve.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 3: EXECUTION - Main processing with multiple decisions\n",
        "\n",
        "logger.start_phase(WorkflowPhase.EXECUTION)\n",
        "print(f\"‚úì Started phase: {logger.get_current_phase().value}\")\n",
        "\n",
        "# Decision 1: Processing strategy\n",
        "strategy_decision = logger.log_decision(\n",
        "    decision=\"Use parallel processing with 4 workers\",\n",
        "    reasoning=\"Optimize throughput for large dataset\",\n",
        "    alternatives=[\"Sequential processing\", \"2 workers\", \"8 workers\", \"Async/await pattern\"],\n",
        "    selected_because=\"4 workers maximizes CPU utilization without memory pressure; 8 workers caused OOM in testing\",\n",
        "    confidence=0.90,\n",
        "    agent_id=\"executor\",\n",
        "    reversible=True,  # Can restart with different config\n",
        ")\n",
        "\n",
        "# Decision 2: Model temperature setting (irreversible during run)\n",
        "temp_decision = logger.log_decision(\n",
        "    decision=\"Set temperature to 0.1 for deterministic extraction\",\n",
        "    reasoning=\"Financial data requires consistent, reproducible outputs\",\n",
        "    alternatives=[\"temperature=0.0\", \"temperature=0.3\", \"temperature=0.7\"],\n",
        "    selected_because=\"0.1 provides slight variation for edge cases while maintaining determinism; 0.0 too rigid\",\n",
        "    confidence=0.87,\n",
        "    agent_id=\"model-controller\",\n",
        "    reversible=False,  # Applied to current batch\n",
        ")\n",
        "\n",
        "# Decision 3: Error handling strategy\n",
        "error_decision = logger.log_decision(\n",
        "    decision=\"Continue on soft errors, halt on schema violations\",\n",
        "    reasoning=\"Maximize data recovery while ensuring output integrity\",\n",
        "    alternatives=[\"Halt on any error\", \"Continue on all errors\", \"Queue for manual review\"],\n",
        "    selected_because=\"Schema violations indicate fundamental extraction failures requiring re-processing\",\n",
        "    confidence=0.93,\n",
        "    agent_id=\"executor\",\n",
        ")\n",
        "\n",
        "# Log intermediate results\n",
        "extraction_artifact = logger.log_artifact(\n",
        "    artifact_name=\"extraction_results\",\n",
        "    artifact_path=Path(\"outputs/extraction_batch_001.json\"),\n",
        "    artifact_type=\"intermediate_result\",\n",
        "    metadata={\n",
        "        \"records_processed\": 15420,\n",
        "        \"success_rate\": 0.982,\n",
        "        \"soft_errors\": 278,\n",
        "        \"schema_violations\": 0,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Execution Progress:\")\n",
        "print(f\"   Records Processed: {extraction_artifact.metadata['records_processed']}\")\n",
        "print(f\"   Success Rate: {extraction_artifact.metadata['success_rate']:.1%}\")\n",
        "print(f\"   Soft Errors: {extraction_artifact.metadata['soft_errors']}\")\n",
        "print(f\"   Schema Violations: {extraction_artifact.metadata['schema_violations']}\")\n",
        "\n",
        "outcome = logger.end_phase(status=\"success\")\n",
        "print(f\"\\n‚úì Phase Completed: {outcome.decisions_made} decisions in {outcome.duration_ms}ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. VALIDATION Phase with Error Handling\n",
        "\n",
        "The **VALIDATION** phase verifies outputs meet quality standards. This phase demonstrates:\n",
        "- **Recoverable errors**: Issues that can be worked around (logged but don't stop the workflow)\n",
        "- **Fatal errors**: Critical failures that require immediate attention\n",
        "\n",
        "The `log_error()` method accepts a `recoverable` flag that determines how the error is categorized:\n",
        "- `recoverable=True`: Logged as `[recoverable]` - workflow continues\n",
        "- `recoverable=False`: Logged as `[fatal]` - typically triggers workflow halt or escalation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 4: VALIDATION - Demonstrates error handling\n",
        "\n",
        "logger.start_phase(WorkflowPhase.VALIDATION)\n",
        "print(f\"‚úì Started phase: {logger.get_current_phase().value}\")\n",
        "\n",
        "# Decision: Choose validation strategy\n",
        "validation_decision = logger.log_decision(\n",
        "    decision=\"Use schema validation + business rule checks\",\n",
        "    reasoning=\"Ensure both structural and semantic correctness\",\n",
        "    alternatives=[\"Schema only\", \"Business rules only\", \"ML-based anomaly detection\"],\n",
        "    selected_because=\"Combined approach catches format errors AND logical inconsistencies\",\n",
        "    confidence=0.91,\n",
        "    agent_id=\"validator\",\n",
        ")\n",
        "\n",
        "# Simulate validation checks with some errors\n",
        "print(\"\\nüîç Running Validation Checks...\")\n",
        "validation_results = {\n",
        "    \"schema_valid\": True,\n",
        "    \"amount_in_range\": True,\n",
        "    \"vendor_exists\": True,\n",
        "    \"duplicate_check\": False,  # Found 3 potential duplicates\n",
        "    \"date_sequence\": True,\n",
        "    \"currency_match\": False,  # 12 records with mismatched currencies\n",
        "}\n",
        "\n",
        "# Log recoverable errors (issues that don't stop processing)\n",
        "logger.log_error(\n",
        "    \"Found 3 potential duplicate invoices (INV-2024-4521, INV-2024-4522, INV-2024-4523)\",\n",
        "    recoverable=True,  # Can continue, flag for review\n",
        ")\n",
        "print(\"   ‚ö†Ô∏è  [recoverable] Duplicate invoice warning logged\")\n",
        "\n",
        "logger.log_error(\n",
        "    \"12 records have source/target currency mismatch - auto-converted using daily rates\",\n",
        "    recoverable=True,  # Applied automatic fix\n",
        ")\n",
        "print(\"   ‚ö†Ô∏è  [recoverable] Currency mismatch warning logged\")\n",
        "\n",
        "# In a real scenario, you might also encounter fatal errors:\n",
        "# logger.log_error(\"Database connection lost during validation\", recoverable=False)\n",
        "\n",
        "# Decision to proceed despite warnings\n",
        "proceed_decision = logger.log_decision(\n",
        "    decision=\"Proceed with flagged records marked for manual review\",\n",
        "    reasoning=\"Overall data quality (99.9%) exceeds threshold\",\n",
        "    alternatives=[\"Halt and investigate\", \"Remove flagged records\", \"Re-run extraction\"],\n",
        "    selected_because=\"15 flagged records out of 15420 (0.1%) is within acceptable error margin\",\n",
        "    confidence=0.88,\n",
        "    agent_id=\"validator\",\n",
        ")\n",
        "\n",
        "# Log validation report artifact\n",
        "validation_artifact = logger.log_artifact(\n",
        "    artifact_name=\"validation_report\",\n",
        "    artifact_path=Path(\"outputs/validation_report.json\"),\n",
        "    artifact_type=\"report\",\n",
        "    metadata={\n",
        "        \"total_records\": 15420,\n",
        "        \"passed\": 15405,\n",
        "        \"flagged\": 15,\n",
        "        \"error_rate\": \"0.1%\",\n",
        "        \"checks_performed\": list(validation_results.keys()),\n",
        "    },\n",
        ")\n",
        "\n",
        "# End with partial status due to warnings\n",
        "outcome = logger.end_phase(status=\"partial\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Phase Completed with Warnings:\")\n",
        "print(f\"   Status: {outcome.status}\")\n",
        "print(f\"   Errors Logged: {len(outcome.errors)}\")\n",
        "for error in outcome.errors:\n",
        "    print(f\"      {error[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Workflow Visualization with Mermaid\n",
        "\n",
        "The `visualize_workflow()` method generates a **Mermaid diagram** showing:\n",
        "- All completed phases with their status (success / failure)\n",
        "- Phase connections (flow order)\n",
        "- Decision counts per phase\n",
        "- Color-coded status: green=success, pink=failure, yellow=partial\n",
        "\n",
        "This visualization is invaluable for workflow status overview and debugging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Mermaid visualization of workflow phases\n",
        "\n",
        "mermaid_diagram = logger.visualize_workflow()\n",
        "\n",
        "print(\"=== Workflow Mermaid Diagram ===\")\n",
        "print()\n",
        "print(mermaid_diagram)\n",
        "print()\n",
        "print(\"Copy the above diagram to mermaid.live or a Mermaid-enabled markdown viewer to render it.\")\n",
        "print()\n",
        "print(\"=== Diagram Explanation ===\")\n",
        "print(\"- Each box represents a completed phase\")\n",
        "print(\"- Arrows show phase execution order\")\n",
        "print(\"- Decision counts are shown as connected notes\")\n",
        "print(\"- Colors indicate status:\")\n",
        "print(\"    #90EE90 (light green) = success\")\n",
        "print(\"    #FFB6C1 (light pink) = failure\")\n",
        "print(\"    #FFE4B5 (moccasin/yellow) = partial\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Workflow Summary Statistics\n",
        "\n",
        "The `get_phase_summary()` method provides aggregated statistics across all phases:\n",
        "- Total and completed phase counts\n",
        "- Total decisions made\n",
        "- Total duration\n",
        "- Per-phase outcomes with details\n",
        "- Overall workflow status\n",
        "\n",
        "This is essential for monitoring and compliance reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get comprehensive workflow summary\n",
        "\n",
        "summary = logger.get_phase_summary()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"WORKFLOW SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(f\"Workflow ID:      {summary.workflow_id}\")\n",
        "print(f\"Overall Status:   {summary.overall_status.upper()}\")\n",
        "print(f\"Total Phases:     {summary.total_phases}\")\n",
        "print(f\"Completed:        {summary.completed_phases}\")\n",
        "print(f\"Total Decisions:  {summary.total_decisions}\")\n",
        "print(f\"Total Duration:   {summary.total_duration_ms}ms\")\n",
        "print()\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"PHASE-BY-PHASE BREAKDOWN\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for outcome in summary.phase_outcomes:\n",
        "    status_icon = {\"success\": \"‚úì\", \"failure\": \"‚úó\", \"partial\": \"‚ö†\"}.get(outcome.status, \"?\")\n",
        "    print(f\"\\n{status_icon} {outcome.phase.value.upper()}\")\n",
        "    print(f\"    Status:     {outcome.status}\")\n",
        "    print(f\"    Duration:   {outcome.duration_ms}ms\")\n",
        "    print(f\"    Decisions:  {outcome.decisions_made}\")\n",
        "    print(f\"    Artifacts:  {outcome.artifacts_produced}\")\n",
        "    if outcome.errors:\n",
        "        print(f\"    Errors:     {len(outcome.errors)}\")\n",
        "        for err in outcome.errors[:2]:  # Show first 2 errors\n",
        "            print(f\"                {err[:50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Querying Decisions and Artifacts by Phase\n",
        "\n",
        "You can retrieve all decisions and artifacts for a specific phase using:\n",
        "- `get_phase_decisions(phase)` - Returns list of Decision objects\n",
        "- `get_phase_artifacts(phase)` - Returns list of Artifact objects\n",
        "\n",
        "This is useful for auditing specific phases or investigating issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query decisions from the EXECUTION phase\n",
        "\n",
        "execution_decisions = logger.get_phase_decisions(WorkflowPhase.EXECUTION)\n",
        "\n",
        "print(\"=== EXECUTION Phase Decisions ===\")\n",
        "print()\n",
        "for dec in execution_decisions:\n",
        "    print(f\"üìã {dec.decision_id}\")\n",
        "    print(f\"   Decision:   {dec.decision}\")\n",
        "    print(f\"   Reasoning:  {dec.reasoning}\")\n",
        "    print(f\"   Confidence: {dec.confidence:.0%}\")\n",
        "    print(f\"   Reversible: {dec.reversible}\")\n",
        "    print()\n",
        "\n",
        "# Query artifacts from DATA_COLLECTION phase\n",
        "collection_artifacts = logger.get_phase_artifacts(WorkflowPhase.DATA_COLLECTION)\n",
        "\n",
        "print(\"=== DATA_COLLECTION Phase Artifacts ===\")\n",
        "print()\n",
        "for art in collection_artifacts:\n",
        "    print(f\"üìÅ {art.artifact_id}\")\n",
        "    print(f\"   Name: {art.name}\")\n",
        "    print(f\"   Type: {art.artifact_type}\")\n",
        "    print(f\"   Path: {art.path}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Exporting Workflow Logs\n",
        "\n",
        "The `export_workflow_log()` method creates a comprehensive JSON export containing:\n",
        "- Workflow summary\n",
        "- All decisions organized by phase\n",
        "- All artifacts organized by phase\n",
        "- All errors organized by phase\n",
        "\n",
        "This export is suitable for:\n",
        "- **Compliance auditing** (HIPAA, SOX, GDPR)\n",
        "- **Post-incident analysis**\n",
        "- **Long-term archival**\n",
        "- **Integration with external monitoring systems**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export complete workflow log to JSON\n",
        "\n",
        "export_path = Path.cwd().parent / \"cache\" / \"exports\" / \"research_workflow_log.json\"\n",
        "logger.export_workflow_log(export_path)\n",
        "\n",
        "print(f\"‚úì Exported workflow log to: {export_path}\")\n",
        "print()\n",
        "\n",
        "# Show the structure of the exported file\n",
        "with open(export_path) as f:\n",
        "    export_data = json.load(f)\n",
        "\n",
        "print(\"=== Export Structure ===\")\n",
        "print()\n",
        "print(f\"workflow_id: {export_data['workflow_id']}\")\n",
        "print(f\"exported_at: {export_data['exported_at']}\")\n",
        "print()\n",
        "print(\"summary:\")\n",
        "print(f\"  - total_phases: {export_data['summary']['total_phases']}\")\n",
        "print(f\"  - completed_phases: {export_data['summary']['completed_phases']}\")\n",
        "print(f\"  - total_decisions: {export_data['summary']['total_decisions']}\")\n",
        "print(f\"  - overall_status: {export_data['summary']['overall_status']}\")\n",
        "print()\n",
        "print(\"decisions: (organized by phase)\")\n",
        "for phase, decisions in export_data['decisions'].items():\n",
        "    print(f\"  - {phase}: {len(decisions)} decisions\")\n",
        "print()\n",
        "print(\"artifacts: (organized by phase)\")\n",
        "for phase, artifacts in export_data['artifacts'].items():\n",
        "    print(f\"  - {phase}: {len(artifacts)} artifacts\")\n",
        "print()\n",
        "print(\"errors: (organized by phase)\")\n",
        "for phase, errors in export_data['errors'].items():\n",
        "    if errors:\n",
        "        print(f\"  - {phase}: {len(errors)} errors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "### What We Covered\n",
        "\n",
        "| Method | Purpose |\n",
        "|--------|---------|\n",
        "| `PhaseLogger(workflow_id, storage_path)` | Initialize logger for a workflow |\n",
        "| `start_phase(phase, metadata)` | Begin a new workflow phase |\n",
        "| `log_decision(decision, reasoning, alternatives, ...)` | Record a decision with full context |\n",
        "| `log_artifact(name, path, type, metadata)` | Track produced artifacts |\n",
        "| `log_error(error, recoverable)` | Log recoverable or fatal errors |\n",
        "| `end_phase(status)` | Complete current phase with status |\n",
        "| `get_current_phase()` | Get currently active phase |\n",
        "| `get_phase_decisions(phase)` | Query decisions for a phase |\n",
        "| `get_phase_artifacts(phase)` | Query artifacts for a phase |\n",
        "| `get_phase_summary()` | Get aggregated workflow statistics |\n",
        "| `visualize_workflow()` | Generate Mermaid diagram |\n",
        "| `export_workflow_log(path)` | Export complete log to JSON |\n",
        "\n",
        "### Workflow Phases Demonstrated\n",
        "\n",
        "1. **PLANNING** - Initial decisions, strategy selection\n",
        "2. **DATA_COLLECTION** - Source selection, data quality assessment\n",
        "3. **EXECUTION** - Main processing, runtime decisions\n",
        "4. **VALIDATION** - Quality checks, error handling\n",
        "\n",
        "### Key Use Cases\n",
        "\n",
        "- **Compliance Auditing**: Track every decision for regulatory requirements (HIPAA, SOX)\n",
        "- **Debugging**: Understand why agents made specific choices\n",
        "- **Reproducibility**: Document reasoning for workflow outcomes\n",
        "- **Monitoring**: Real-time visibility into multi-phase workflows\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore [Tutorial 01: Explainability Fundamentals](../tutorials/01_explainability_fundamentals.md) for the conceptual foundation\n",
        "- See [BlackBoxRecorder Demo](01_black_box_recording_demo.ipynb) for event-level recording\n",
        "- Check [AgentFacts Demo](02_agent_facts_verification.ipynb) for agent identity verification\n",
        "- Review [GuardRails Demo](03_guardrails_validation_traces.ipynb) for validation patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
