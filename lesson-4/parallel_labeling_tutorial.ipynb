{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Labeling Tutorial: ThreadPoolExecutor for LLM Calls\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- ‚úÖ Use ThreadPoolExecutor for parallel LLM API calls\n",
    "- ‚úÖ Implement incremental labeling with resume capability\n",
    "- ‚úÖ Calculate and optimize labeling costs\n",
    "- ‚úÖ Track progress with tqdm\n",
    "- ‚úÖ Handle API failures gracefully\n",
    "- ‚úÖ Use Pydantic for structured LLM outputs\n",
    "\n",
    "## Estimated Time\n",
    "\n",
    "**Execution:** 20-30 minutes (depends on API rate limits)\n",
    "\n",
    "**‚ö†Ô∏è API Cost Warning:** Running on full dataset (~200 conversations) costs ~$0.50-1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# LLM and utilities\n",
    "import litellm\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ========================================\n# CONFIGURATION: Demo vs Full Mode\n# ========================================\n\n# Set DEMO_MODE = False to label full dataset\nDEMO_MODE = True  # Default: Quick demo for tutorial\n\nif DEMO_MODE:\n    DEMO_SIZE = 5  # Label 5 traces for demo\n    MAX_WORKERS = 5\n    print(\"üöÄ DEMO MODE: Labeling small sample\")\n    print(f\"   Traces: {DEMO_SIZE} | Workers: {MAX_WORKERS}\")\n    print(f\"   Estimated cost: $0.05-0.10 | Time: ~10-15 seconds\")\nelse:\n    DEMO_SIZE = None  # Use full dataset (~200 traces)\n    MAX_WORKERS = 20\n    print(\"üìä FULL MODE: Labeling complete dataset\")\n    print(f\"   Traces: ~200 | Workers: {MAX_WORKERS}\")\n    print(f\"   Estimated cost: $0.50-1.00 | Time: ~5-8 minutes\")\n\nprint(\"\\nüí° To switch modes, change DEMO_MODE in this cell and re-run notebook\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Structured Output Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubstantiationLabel(BaseModel):\n",
    "    \"\"\"Pydantic model for LLM output.\n",
    "    \n",
    "    Ensures consistent JSON structure from GPT-4o.\n",
    "    \"\"\"\n",
    "    all_responses_substantiated: bool\n",
    "    rationale: str\n",
    "\n",
    "# Test instantiation\n",
    "example = SubstantiationLabel(\n",
    "    all_responses_substantiated=True,\n",
    "    rationale=\"All claims verified by tool outputs\"\n",
    ")\n",
    "print(f\"‚úì Schema: {example.model_dump()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = Path(\"nurtureboss_traces.json\")\n",
    "\n",
    "if not DATA_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Run clean_logs.py first to generate {DATA_FILE}\"\n",
    "    )\n",
    "\n",
    "with open(DATA_FILE) as f:\n",
    "    traces = json.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded {len(traces)} conversations\")\n",
    "print(f\"‚úì Sample ID: {traces[0]['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(messages: List[Dict], metadata: Dict) -> str:\n",
    "    \"\"\"Construct substantiation evaluation prompt.\"\"\"\n",
    "    \n",
    "    # Format conversation\n",
    "    convo = \"\\n\".join(\n",
    "        f\"{msg['role'].upper()}: {msg['content']}\"\n",
    "        for msg in messages\n",
    "    )\n",
    "    \n",
    "    # Format metadata\n",
    "    meta_str = json.dumps(metadata, indent=2) if metadata else \"<none>\"\n",
    "    \n",
    "    return f\"\"\"\n",
    "You are evaluating substantiation in AI conversations.\n",
    "\n",
    "TASK: Determine if every factual claim can be verified by:\n",
    "1. User-provided information\n",
    "2. Tool outputs in metadata\n",
    "3. Tool capabilities in metadata\n",
    "\n",
    "PASS = All claims substantiated\n",
    "FAIL = At least one unsubstantiated claim\n",
    "\n",
    "Ignore courtesy statements (\"How can I help?\").\n",
    "\n",
    "=== CONVERSATION ===\n",
    "{convo}\n",
    "\n",
    "=== METADATA ===\n",
    "{meta_str}\n",
    "\n",
    "Return JSON: {{\"all_responses_substantiated\": bool, \"rationale\": str}}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Test on first trace\n",
    "sample_prompt = build_prompt(\n",
    "    traces[0].get('messages', []),\n",
    "    {k:v for k,v in traces[0].items() if k not in ['id', 'messages']}\n",
    ")\n",
    "print(f\"Prompt length: {len(sample_prompt)} chars\")\n",
    "print(f\"\\nFirst 500 chars:\\n{sample_prompt[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Label Single Conversation (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_one_conversation(\n",
    "    trace: Dict,\n",
    "    model: str = \"gpt-4o\"\n",
    ") -> SubstantiationLabel:\n",
    "    \"\"\"Call LLM to label one conversation.\"\"\"\n",
    "    \n",
    "    messages = trace.get('messages', [])\n",
    "    metadata = {k:v for k,v in trace.items() if k not in ['id', 'messages']}\n",
    "    \n",
    "    prompt = build_prompt(messages, metadata)\n",
    "    \n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=SubstantiationLabel,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return SubstantiationLabel(**result)\n",
    "\n",
    "# Test on one conversation\n",
    "print(\"Testing on first trace (may take 3-5 seconds)...\")\n",
    "test_label = label_one_conversation(traces[0])\n",
    "print(f\"\\n‚úì Result: {test_label.all_responses_substantiated}\")\n",
    "print(f\"  Rationale: {test_label.rationale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Demo on subset using configuration\ndemo_traces = traces[:DEMO_SIZE] if DEMO_SIZE else traces\n\nprint(f\"Labeling {len(demo_traces)} traces (parallel, ~10-15 seconds)...\\n\")\ndemo_labeled = label_parallel(demo_traces, max_workers=MAX_WORKERS)\n\nprint(f\"\\n‚úì Labeled {len(demo_labeled)} conversations\")\nprint(f\"\\nSample result:\")\nprint(f\"  ID: {demo_labeled[0]['id']}\")\nprint(f\"  Substantiated: {demo_labeled[0].get('all_responses_substantiated')}\")\nprint(f\"  Rationale: {demo_labeled[0].get('substantiation_rationale')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_parallel(\n",
    "    traces_to_label: List[Dict],\n",
    "    max_workers: int = 10,\n",
    "    model: str = \"gpt-4o\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Label conversations in parallel using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        traces_to_label: Conversations to label\n",
    "        max_workers: Parallel threads (adjust for API rate limits)\n",
    "        model: LLM model name\n",
    "    \n",
    "    Returns:\n",
    "        Traces with added 'all_responses_substantiated' and 'substantiation_rationale'\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_trace = {\n",
    "            executor.submit(label_one_conversation, trace, model): trace\n",
    "            for trace in traces_to_label\n",
    "        }\n",
    "        \n",
    "        # Collect results with progress bar\n",
    "        for future in tqdm(\n",
    "            as_completed(future_to_trace),\n",
    "            total=len(traces_to_label),\n",
    "            desc=\"Labeling\"\n",
    "        ):\n",
    "            trace = future_to_trace[future]\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                trace['all_responses_substantiated'] = result.all_responses_substantiated\n",
    "                trace['substantiation_rationale'] = result.rationale\n",
    "                labeled.append(trace)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è  Error on {trace['id']}: {e}\")\n",
    "                labeled.append(trace)  # Keep unlabeled\n",
    "    \n",
    "    return labeled\n",
    "\n",
    "# Demo on small subset (5 traces)\n",
    "DEMO_SIZE = 5\n",
    "demo_traces = traces[:DEMO_SIZE]\n",
    "\n",
    "print(f\"Labeling {DEMO_SIZE} traces (parallel, ~10-15 seconds)...\\n\")\n",
    "demo_labeled = label_parallel(demo_traces, max_workers=5)\n",
    "\n",
    "print(f\"\\n‚úì Labeled {len(demo_labeled)} conversations\")\n",
    "print(f\"\\nSample result:\")\n",
    "print(f\"  ID: {demo_labeled[0]['id']}\")\n",
    "print(f\"  Substantiated: {demo_labeled[0].get('all_responses_substantiated')}\")\n",
    "print(f\"  Rationale: {demo_labeled[0].get('substantiation_rationale')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(\n",
    "    num_conversations: int,\n",
    "    avg_tokens_per_call: int = 800,\n",
    "    model: str = \"gpt-4o\"\n",
    ") -> dict:\n",
    "    \"\"\"Estimate labeling cost.\n",
    "    \n",
    "    Pricing (as of 2024):\n",
    "    - GPT-4o: ~$2.50 / 1M input tokens, ~$10 / 1M output tokens\n",
    "    - GPT-4o-mini: ~$0.15 / 1M input tokens, ~$0.60 / 1M output tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rough estimates\n",
    "    pricing = {\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.0},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60}\n",
    "    }\n",
    "    \n",
    "    if model not in pricing:\n",
    "        return {\"error\": f\"Unknown model: {model}\"}\n",
    "    \n",
    "    input_tokens = num_conversations * avg_tokens_per_call\n",
    "    output_tokens = num_conversations * 50  # ~50 tokens for JSON response\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * pricing[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"conversations\": num_conversations,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_cost_usd\": round(total_cost, 2),\n",
    "        \"cost_per_label\": round(total_cost / num_conversations, 4)\n",
    "    }\n",
    "\n",
    "# Estimate for full dataset\n",
    "full_estimate = estimate_cost(len(traces), model=\"gpt-4o\")\n",
    "mini_estimate = estimate_cost(len(traces), model=\"gpt-4o-mini\")\n",
    "\n",
    "print(\"Cost Estimates:\\n\")\n",
    "print(f\"GPT-4o ({len(traces)} conversations):\")\n",
    "print(f\"  Total: ${full_estimate['total_cost_usd']}\")\n",
    "print(f\"  Per label: ${full_estimate['cost_per_label']}\")\n",
    "print()\n",
    "print(f\"GPT-4o-mini ({len(traces)} conversations):\")\n",
    "print(f\"  Total: ${mini_estimate['total_cost_usd']}\")\n",
    "print(f\"  Per label: ${mini_estimate['cost_per_label']}\")\n",
    "print(f\"  Savings: ${full_estimate['total_cost_usd'] - mini_estimate['total_cost_usd']:.2f} (97% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Incremental Labeling (Resume Capability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_with_resume(\n",
    "    all_traces: List[Dict],\n",
    "    output_file: Path,\n",
    "    max_workers: int = 10,\n",
    "    save_every: int = 50\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Label traces with checkpoint saving.\n",
    "    \n",
    "    Skips traces that already have 'all_responses_substantiated' field.\n",
    "    Saves progress every `save_every` labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load existing labels if file exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file) as f:\n",
    "            all_traces = json.load(f)\n",
    "        print(f\"‚úì Loaded existing file: {output_file}\")\n",
    "    \n",
    "    # Filter unlabeled\n",
    "    unlabeled = [\n",
    "        t for t in all_traces\n",
    "        if 'all_responses_substantiated' not in t\n",
    "    ]\n",
    "    \n",
    "    labeled_count = len(all_traces) - len(unlabeled)\n",
    "    print(f\"Already labeled: {labeled_count}\")\n",
    "    print(f\"To label: {len(unlabeled)}\")\n",
    "    \n",
    "    if not unlabeled:\n",
    "        print(\"‚úì All traces already labeled!\")\n",
    "        return all_traces\n",
    "    \n",
    "    # Label in batches\n",
    "    for i in range(0, len(unlabeled), save_every):\n",
    "        batch = unlabeled[i:i+save_every]\n",
    "        print(f\"\\nBatch {i//save_every + 1}: Labeling {len(batch)} traces...\")\n",
    "        \n",
    "        labeled_batch = label_parallel(batch, max_workers=max_workers)\n",
    "        \n",
    "        # Update in-place\n",
    "        for j, trace in enumerate(batch):\n",
    "            trace.update(labeled_batch[j])\n",
    "        \n",
    "        # Save checkpoint\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(all_traces, f, indent=2)\n",
    "        print(f\"‚úì Saved checkpoint to {output_file}\")\n",
    "    \n",
    "    return all_traces\n",
    "\n",
    "# Demo: Label with checkpointing (small batch)\n",
    "OUTPUT_FILE = Path(\"nurtureboss_traces_labeled_demo.json\")\n",
    "\n",
    "print(\"Demo: Incremental labeling with resume capability\\n\")\n",
    "demo_result = label_with_resume(\n",
    "    traces[:10],  # Only 10 for demo\n",
    "    OUTPUT_FILE,\n",
    "    max_workers=5,\n",
    "    save_every=5\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Final count: {len(demo_result)} labeled traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Full Labeling (Optional - Costs Money!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è  UNCOMMENT TO RUN ON FULL DATASET\n",
    "# This will cost ~$0.50-1.00 depending on model\n",
    "\n",
    "# FULL_OUTPUT = Path(\"nurtureboss_traces_labeled.json\")\n",
    "#\n",
    "# final_traces = label_with_resume(\n",
    "#     traces,\n",
    "#     FULL_OUTPUT,\n",
    "#     max_workers=20,  # Adjust based on API rate limits\n",
    "#     save_every=50\n",
    "# )\n",
    "#\n",
    "# print(f\"\\n‚úì Labeled {len(final_traces)} total conversations\")\n",
    "# print(f\"  Saved to: {FULL_OUTPUT}\")\n",
    "\n",
    "print(\"‚è≠Ô∏è  Skipped full labeling (uncomment code above to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Parallel processing with ThreadPoolExecutor** - 10-64x speedup vs sequential\n",
    "2. **Structured outputs with Pydantic** - Type-safe LLM responses\n",
    "3. **Incremental labeling** - Resume after interruptions, save checkpoints\n",
    "4. **Cost optimization** - Estimate before running, choose model wisely\n",
    "5. **Progress tracking** - tqdm for user feedback\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Approach | Time for 200 traces | Cost (GPT-4o) |\n",
    "|----------|---------------------|---------------|\n",
    "| Sequential | ~40 min | $1.00 |\n",
    "| Parallel (10 workers) | ~5-8 min | $1.00 |\n",
    "| Parallel (64 workers) | ~2-3 min | $1.00 |\n",
    "\n",
    "**Key Insight:** Parallelization saves time, not money. Same API calls = same cost.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Run on full dataset (costs ~$1)\n",
    "- Proceed to [Judge Evaluation Pipeline](judge_evaluation_pipeline_tutorial.ipynb)\n",
    "- Apply bias correction with judgy library\n",
    "\n",
    "---\n",
    "\n",
    "**Tutorial Status:** ‚úÖ Complete  \n",
    "**Last Updated:** 2025-10-30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipe Chatbot (.venv)",
   "language": "python",
   "name": "recipe-chatbot"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}