{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge Evaluation Pipeline Tutorial\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- ✅ Split datasets deterministically with hash-based partitioning\n",
    "- ✅ Select balanced few-shot examples from training set\n",
    "- ✅ Build judge prompts with few-shot calibration\n",
    "- ✅ Calculate TPR (True Positive Rate) and TNR (True Negative Rate)\n",
    "- ✅ Interpret judge bias and correction strategies\n",
    "- ✅ Apply judgy library for statistical bias correction\n",
    "\n",
    "## Estimated Time\n",
    "\n",
    "**Execution:** 15-20 minutes (depends on API)\n",
    "\n",
    "**Prerequisites:** Completed [Parallel Labeling Tutorial](parallel_labeling_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ⚠️ API Cost Warning\n\n**Estimated cost:** $0.10-0.30 for demo (20 traces with `gpt-4o-mini`)  \n**Full dataset cost:** $1.00-2.00 for 150 traces  \n**Estimated time:** 30-60 seconds for demo | 5-10 minutes for full dataset\n\nFor this tutorial, we **limit evaluation to 20 traces** to keep costs under $0.30. Remove `limit=20` in cell-11 to run on full test set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import hashlib\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import litellm\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = Path(\"nurtureboss_traces_labeled.json\")\n",
    "\n",
    "with open(DATA_FILE) as f:\n",
    "    labeled_traces = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(labeled_traces)} labeled conversations\")\n",
    "\n",
    "# Count label distribution\n",
    "pass_count = sum(1 for t in labeled_traces if t.get('all_responses_substantiated') == True)\n",
    "fail_count = len(labeled_traces) - pass_count\n",
    "\n",
    "print(f\"  PASS: {pass_count} ({pass_count/len(labeled_traces)*100:.1f}%)\")\n",
    "print(f\"  FAIL: {fail_count} ({fail_count/len(labeled_traces)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deterministic Dataset Splitting (Hash-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    records: List[Dict],\n",
    "    train_size: int = 20,\n",
    "    dev_size: int = 30\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Split dataset deterministically using ID hash.\n",
    "    \n",
    "    Same IDs always go to same split (reproducible).\n",
    "    \"\"\"\n",
    "    # Sort by hash of ID for deterministic order\n",
    "    sorted_records = sorted(\n",
    "        records,\n",
    "        key=lambda r: hashlib.sha256(r['id'].encode()).hexdigest()\n",
    "    )\n",
    "    \n",
    "    train = sorted_records[:train_size]\n",
    "    dev = sorted_records[train_size:train_size + dev_size]\n",
    "    test = sorted_records[train_size + dev_size:]\n",
    "    \n",
    "    return {'train': train, 'dev': dev, 'test': test}\n",
    "\n",
    "# Split data\n",
    "splits = split_dataset(labeled_traces)\n",
    "\n",
    "print(f\"Dataset Splits:\")\n",
    "for name, data in splits.items():\n",
    "    pass_pct = sum(1 for r in data if r.get('all_responses_substantiated'))/len(data)*100\n",
    "    print(f\"  {name.upper()}: {len(data)} traces ({pass_pct:.1f}% PASS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Few-Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_few_shot(\n",
    "    train_data: List[Dict],\n",
    "    num_pass: int = 1,\n",
    "    num_fail: int = 1\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"Select balanced few-shot examples from training set.\"\"\"\n",
    "    \n",
    "    pass_examples = [r for r in train_data if r.get('all_responses_substantiated')]\n",
    "    fail_examples = [r for r in train_data if not r.get('all_responses_substantiated')]\n",
    "    \n",
    "    return {\n",
    "        'pass': pass_examples[0] if pass_examples else None,\n",
    "        'fail': fail_examples[0] if fail_examples else None\n",
    "    }\n",
    "\n",
    "# Get few-shot examples\n",
    "few_shot = select_few_shot(splits['train'])\n",
    "\n",
    "print(\"Few-Shot Examples Selected:\")\n",
    "print(f\"  PASS example: {few_shot['pass']['id'] if few_shot['pass'] else 'None'}\")\n",
    "print(f\"  FAIL example: {few_shot['fail']['id'] if few_shot['fail'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Judge Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeResult(BaseModel):\n",
    "    all_responses_substantiated: bool\n",
    "    reason: str\n",
    "\n",
    "def build_judge_prompt(\n",
    "    messages: List[Dict],\n",
    "    metadata: Dict,\n",
    "    example_pass_conv: str,\n",
    "    example_fail_conv: str\n",
    ") -> str:\n",
    "    \"\"\"Construct judge prompt with few-shot examples.\"\"\"\n",
    "    \n",
    "    conv = \"\\n\".join(f\"{m['role'].upper()}: {m['content']}\" for m in messages)\n",
    "    meta_str = json.dumps(metadata, indent=2) if metadata else \"<none>\"\n",
    "    \n",
    "    return f\"\"\"\n",
    "You are evaluating substantiation in AI conversations.\n",
    "\n",
    "PASS = All claims verified by user input, tool outputs, or metadata\n",
    "FAIL = At least one unsubstantiated claim\n",
    "\n",
    "Rules:\n",
    "1. Courtesy statements don't need evidence (\"How can I help?\")\n",
    "2. Paraphrases of tool outputs count as substantiated\n",
    "3. Specific claims need specific evidence (e.g., \"balcony\" must appear in tools)\n",
    "4. When uncertain, default to PASS\n",
    "\n",
    "Few-Shot Examples:\n",
    "--- PASS Example ---\n",
    "{example_pass_conv}\n",
    "\n",
    "--- FAIL Example ---\n",
    "{example_fail_conv}\n",
    "--- End Examples ---\n",
    "\n",
    "Evaluate this conversation:\n",
    "\n",
    "=== CONVERSATION ===\n",
    "{conv}\n",
    "\n",
    "=== METADATA ===\n",
    "{meta_str}\n",
    "\n",
    "Return JSON: {{\"all_responses_substantiated\": bool, \"reason\": str}}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Test prompt construction\n",
    "sample = splits['dev'][0]\n",
    "test_prompt = build_judge_prompt(\n",
    "    sample['messages'],\n",
    "    {k:v for k,v in sample.items() if k not in ['id', 'messages', 'all_responses_substantiated', 'substantiation_rationale']},\n",
    "    \"USER: Find recipes\\nAGENT: Searching database...\",\n",
    "    \"USER: Tell me about A11\\nAGENT: It has a balcony\"\n",
    ")\n",
    "print(f\"Prompt constructed ({len(test_prompt)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Judge on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_judge(\n",
    "    test_data: List[Dict],\n",
    "    few_shot_examples: Dict,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    limit: int = 20  # Limit for demo\n",
    ") -> Dict:\n",
    "    \"\"\"Run judge on test set and calculate metrics.\"\"\"\n",
    "    \n",
    "    # Format few-shot examples\n",
    "    pass_conv = \"\\n\".join(\n",
    "        f\"{m['role'].upper()}: {m['content']}\"\n",
    "        for m in few_shot_examples['pass']['messages']\n",
    "    ) if few_shot_examples['pass'] else \"<no example>\"\n",
    "    \n",
    "    fail_conv = \"\\n\".join(\n",
    "        f\"{m['role'].upper()}: {m['content']}\"\n",
    "        for m in few_shot_examples['fail']['messages']\n",
    "    ) if few_shot_examples['fail'] else \"<no example>\"\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Limit for demo to save API costs\n",
    "    test_subset = test_data[:limit]\n",
    "    \n",
    "    for trace in test_subset:\n",
    "        # Ground truth\n",
    "        truth = trace.get('all_responses_substantiated')\n",
    "        if truth is None:\n",
    "            continue\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = build_judge_prompt(\n",
    "            trace['messages'],\n",
    "            {k:v for k,v in trace.items() if k not in ['id', 'messages', 'all_responses_substantiated', 'substantiation_rationale']},\n",
    "            pass_conv,\n",
    "            fail_conv\n",
    "        )\n",
    "        \n",
    "        # Call judge\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=JudgeResult,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        result = JudgeResult(**json.loads(response.choices[0].message.content))\n",
    "        \n",
    "        y_true.append(truth)\n",
    "        y_pred.append(result.all_responses_substantiated)\n",
    "        \n",
    "        print(f\".\" if truth == result.all_responses_substantiated else \"X\", end=\"\")\n",
    "    \n",
    "    print(f\"\\n\\n✓ Evaluated {len(y_true)} traces\")\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[True, False])\n",
    "    tn, fp, fn, tp = cm[1,1], cm[1,0], cm[0,1], cm[0,0]  # Adjusted for [True, False] labels\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / len(y_true) if y_true else 0\n",
    "    \n",
    "    return {\n",
    "        'tpr': tpr,\n",
    "        'tnr': tnr,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "# Run evaluation (limited to 20 for demo)\n",
    "print(\"Evaluating judge (20 traces, ~30-60 seconds)...\\n\")\n",
    "metrics = evaluate_judge(splits['test'], few_shot, limit=20)\n",
    "\n",
    "print(f\"\\nJudge Performance:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']*100:.1f}%\")\n",
    "print(f\"  TPR (Sensitivity): {metrics['tpr']*100:.1f}%\")\n",
    "print(f\"  TNR (Specificity): {metrics['tnr']*100:.1f}%\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {metrics['tp']}  FN: {metrics['fn']}\")\n",
    "print(f\"  FP: {metrics['fp']}  TN: {metrics['tn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ========================================\n# VALIDATION: Verify judge metrics\n# ========================================\n\n# Assert metrics were calculated\nassert 'tpr' in metrics, \"TPR not calculated\"\nassert 'tnr' in metrics, \"TNR not calculated\"\nassert 'accuracy' in metrics, \"Accuracy not calculated\"\n\n# Assert metric ranges (probabilities: 0-1)\nassert 0 <= metrics['tpr'] <= 1, f\"Invalid TPR: {metrics['tpr']}\"\nassert 0 <= metrics['tnr'] <= 1, f\"Invalid TNR: {metrics['tnr']}\"\nassert 0 <= metrics['accuracy'] <= 1, f\"Invalid accuracy: {metrics['accuracy']}\"\n\n# Assert confusion matrix values are non-negative\nfor key in ['tp', 'tn', 'fp', 'fn']:\n    assert metrics[key] >= 0, f\"Invalid {key}: {metrics[key]}\"\n\n# Assert confusion matrix sums correctly\ntotal_predictions = metrics['tp'] + metrics['tn'] + metrics['fp'] + metrics['fn']\nassert total_predictions > 0, \"No predictions made\"\nassert total_predictions <= 20, f\"Too many predictions: {total_predictions} (expected ≤20 for demo)\"\n\n# Warn if judge performance is poor\nif metrics['tpr'] < 0.75 or metrics['tnr'] < 0.75:\n    print(f\"⚠️  WARNING: Judge performance below 75% threshold\")\n    print(f\"   TPR: {metrics['tpr']:.1%} | TNR: {metrics['tnr']:.1%}\")\n    print(f\"   Consider refining prompt or using better model\")\n\nprint(f\"✅ VALIDATION PASSED:\")\nprint(f\"   - All metrics in valid range [0, 1]\")\nprint(f\"   - Confusion matrix: {total_predictions} predictions\")\nprint(f\"   - TPR: {metrics['tpr']:.1%} | TNR: {metrics['tnr']:.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpretation:\\n\")\n",
    "\n",
    "if metrics['tpr'] >= 0.85 and metrics['tnr'] >= 0.85:\n",
    "    print(\"✅ GOOD: Judge is reliable (TPR & TNR ≥ 85%)\")\n",
    "    print(\"   Can use for automated evaluation with statistical correction\")\n",
    "elif metrics['tpr'] >= 0.75 and metrics['tnr'] >= 0.75:\n",
    "    print(\"⚠️  ACCEPTABLE: Judge is usable but needs improvement\")\n",
    "    print(\"   Consider refining prompt or using better model\")\n",
    "else:\n",
    "    print(\"❌ POOR: Judge is unreliable (TPR or TNR < 75%)\")\n",
    "    print(\"   Options: Better model, clearer criteria, or manual evaluation\")\n",
    "\n",
    "print(f\"\\nBias Analysis:\")\n",
    "if metrics['tpr'] < metrics['tnr'] - 0.1:\n",
    "    print(\"  Judge is TOO STRICT (misses valid PASSes)\")\n",
    "    print(\"  → Add lenient examples to few-shot\")\n",
    "elif metrics['tnr'] < metrics['tpr'] - 0.1:\n",
    "    print(\"  Judge is TOO LENIENT (misses FAILs)\")\n",
    "    print(\"  → Add strict examples to few-shot\")\n",
    "else:\n",
    "    print(\"  Judge is BALANCED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bias Correction with judgy Library (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires: pip install judgy\n",
    "# from judgy import correct_rate\n",
    "#\n",
    "# # Observed pass rate on large dataset\n",
    "# p_obs = 0.75  # 75% of production traces judged as PASS\n",
    "#\n",
    "# # Correct using measured TPR/TNR\n",
    "# corrected = correct_rate(\n",
    "#     p_obs,\n",
    "#     tpr=metrics['tpr'],\n",
    "#     tnr=metrics['tnr'],\n",
    "#     n=1000  # sample size\n",
    "# )\n",
    "#\n",
    "# print(f\"Observed rate: {p_obs*100:.1f}%\")\n",
    "# print(f\"Corrected rate: {corrected['theta_hat']*100:.1f}%\")\n",
    "# print(f\"95% CI: [{corrected['ci_lower']*100:.1f}%, {corrected['ci_upper']*100:.1f}%]\")\n",
    "\n",
    "print(\"⏭️  Install judgy to run bias correction: pip install judgy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Hash-based splitting** - Deterministic, reproducible train/dev/test\n",
    "2. **Few-shot calibration** - Use training examples to guide judge\n",
    "3. **TPR/TNR metrics** - Measure judge bias quantitatively\n",
    "4. **Bias correction** - Adjust observed rates with judgy library\n",
    "\n",
    "### Judge Quality Thresholds\n",
    "\n",
    "| TPR/TNR | Quality | Action |\n",
    "|---------|---------|--------|\n",
    "| ≥ 0.90 | Excellent | Deploy confidently |\n",
    "| 0.85-0.90 | Good | Use with correction |\n",
    "| 0.75-0.85 | Acceptable | Refine prompt |\n",
    "| < 0.75 | Poor | Better model or manual eval |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Run on full test set (remove `limit=20`)\n",
    "- Iterate on prompt if TPR/TNR < 0.85\n",
    "- Apply to production logs with judgy correction\n",
    "- See [Bias Correction Tutorial](../homeworks/hw3/bias_correction_tutorial.md)\n",
    "\n",
    "---\n",
    "\n",
    "**Tutorial Status:** ✅ Complete  \n",
    "**Last Updated:** 2025-10-30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}